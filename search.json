[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Making shareable documents with Quarto",
    "section": "",
    "text": "It’s possible to create beautiful documentation to share online with Quarto that auto-updates with GitHub. This is very new and incredibly cool. This tutorial is an example of a quarto website — it is a really powerful way to create and share your work. You can communicate about science using the same reproducible workflow you and/or your colleagues use for analyses, whether or not you write code.\nCreating websites with Quarto can be done without knowing R, Python or HTML, CSS, etc, and that’s where we’ll start. However, Quarto integrates with these tools so you can make your websites as complex and beautiful as you like as you see examples and reuse and remix from others in the open community. This tutorial borrows heavily from a lot of great tutorials and resources you should check out too – there are links throughout.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Making shareable documents with Quarto",
    "section": "",
    "text": "It’s possible to create beautiful documentation to share online with Quarto that auto-updates with GitHub. This is very new and incredibly cool. This tutorial is an example of a quarto website — it is a really powerful way to create and share your work. You can communicate about science using the same reproducible workflow you and/or your colleagues use for analyses, whether or not you write code.\nCreating websites with Quarto can be done without knowing R, Python or HTML, CSS, etc, and that’s where we’ll start. However, Quarto integrates with these tools so you can make your websites as complex and beautiful as you like as you see examples and reuse and remix from others in the open community. This tutorial borrows heavily from a lot of great tutorials and resources you should check out too – there are links throughout.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-is-quarto",
    "href": "index.html#what-is-quarto",
    "title": "Making shareable documents with Quarto",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto helps you have your ideas and your code in one place, and present it in a beautiful way.\nQuarto unifies and extends the RMarkdown ecosystem - it unifies by combining the functionality of R Markdown, bookdown, distill, xaringian, etc into a single consistent system. And it extends in several ways: all features are possible beyond R too, including Python and Javascript. It also has more “guardrails”: accessibility and inclusion are centered in the design. Quarto is for people who love RMarkdown, and it’s for people who have never used RMarkdown.\nThe ability for Quarto to streamline collaboration has been so cool and important for our NASA Openscapes project. Quarto has been a common place for us to collaborate - across R and Python languages and coding expertise.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-is-this-tutorial",
    "href": "index.html#what-is-this-tutorial",
    "title": "Making shareable documents with Quarto",
    "section": "What is this tutorial?",
    "text": "What is this tutorial?\nThis is a 1-hour tutorial that can be used to teach or as self-paced learning.\nWe introduce Quarto by exploring this tutorial website, and practicing the basic Quarto workflow using different tools (GitHub browser, RStudio, and Jupyter) for editing your website.\nWe’ll start off from the browser so you don’t need to install any additional software, however this approach is very limited and you will soon outgrow its capabilities. If you don’t already have a workflow to edit files and sync to GitHub from your computer, I recommend RStudio. You don’t need to know R to use RStudio, and it has powerful editor features that make for happy workflows.\nQuarto.org is the go-to place for full documentation and more tutorials!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#example-quarto-sites",
    "href": "index.html#example-quarto-sites",
    "title": "Making shareable documents with Quarto",
    "section": "Example Quarto sites",
    "text": "Example Quarto sites\nA few Quarto websites from Openscapes - so far we have been using Quarto for documentation using Quarto and Markdown files and Jupyter Notebooks.\n\nChampions Lessons Series\nOpenscapes Approach Guide\n\n2021 NASA Cloud Hackathon\nFaylab Lab Manual\nA Quarto tip a day, by Mine Çetinkaya-Rundel",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Making shareable documents with Quarto",
    "section": "About",
    "text": "About\nOpenscapes is about better science for future us. We help researchers reimagine data analysis, develop modern skills that are of immediate value to them, and cultivate collaborative and inclusive research teams as part of the broader global open movement.\nWe’re developing this tutorial to help folks with different levels of technical skills use Quarto for documentation and tutorial building. This tutorial was originally created for several different audiences: NASA-Openscapes researcher support engineers using Python, communications directors at organizations promoting open science who do not identify as coders, and fisheries scientists curious about transitioning from RMarkdown. We’re hoping it’s useful to folks with backgrounds as wide as these; if you find it useful or have suggestions for improvement, please let us know by clicking “Edit this page” or “Report an issue” at the upper right side of any page.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "learning-more.html",
    "href": "learning-more.html",
    "title": "Learning more",
    "section": "",
    "text": "An excellent overview: Reproducible authoring with Quarto - Mine Çetinkaya-Rundel, Feb 2022 - slides, youtube\nA Quarto tip a day in June 2022, from Mine Çetinkaya-Rundel.\n\n\n\nOpenscapes Champions Lessons Series\nOpenscapes Approach Guide\n\nNASA Earthdata Cloud Cookbook\n\nSee many more examples at the quarto gallery!\n\n\n\nAre you making onboarding documentation? Check out The Fay Lab Manual (now in Quarto!) for inspiration on structure - you could also start there and make it your own.",
    "crumbs": [
      "Learning more"
    ]
  },
  {
    "objectID": "learning-more.html#learn-more",
    "href": "learning-more.html#learn-more",
    "title": "Learning more",
    "section": "",
    "text": "An excellent overview: Reproducible authoring with Quarto - Mine Çetinkaya-Rundel, Feb 2022 - slides, youtube\nA Quarto tip a day in June 2022, from Mine Çetinkaya-Rundel.\n\n\n\nOpenscapes Champions Lessons Series\nOpenscapes Approach Guide\n\nNASA Earthdata Cloud Cookbook\n\nSee many more examples at the quarto gallery!\n\n\n\nAre you making onboarding documentation? Check out The Fay Lab Manual (now in Quarto!) for inspiration on structure - you could also start there and make it your own.",
    "crumbs": [
      "Learning more"
    ]
  },
  {
    "objectID": "statistic/ordinal_regression.html",
    "href": "statistic/ordinal_regression.html",
    "title": "Mô Hình Hồi Quy Logistic Thứ Tự",
    "section": "",
    "text": "1. Mô Hình Hồi Quy Logistic Thứ Tự\n\nKhái niệm:\n\nMô hình hồi quy logistic thứ tự là gì?: Hồi quy logistic thứ tự là một phương pháp phân tích thống kê được sử dụng khi biến phụ thuộc có thứ tự, nghĩa là các giá trị của biến phụ thuộc có một thứ tự tự nhiên nhưng không nhất thiết có khoảng cách đều nhau giữa các giá trị. Ví dụ về biến phụ thuộc có thứ tự bao gồm mức độ hài lòng của khách hàng (rất không hài lòng, không hài lòng, trung lập, hài lòng, rất hài lòng) hoặc trình độ học vấn (tiểu học, trung học, đại học).\nVí dụ: Giả sử chúng ta muốn nghiên cứu các yếu tố ảnh hưởng đến mức độ hài lòng của khách hàng sử dụng dịch vụ của một công ty. Biến phụ thuộc là mức độ hài lòng của khách hàng với 5 mức độ: “Rất không hài lòng”, “Không hài lòng”, “Trung lập”, “Hài lòng”, “Rất hài lòng”.\n\n\n\n\n2. Mô Hình Hồi Quy Logistic Thứ Tự\n\nCông thức mô hình:\n\nTrình bày công thức toán học của mô hình hồi quy logistic thứ tự:\n\\[ \\log\\left(\\frac{\\pi_{ij}}{1 - \\pi_{ij}}\\right) = \\alpha_j + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_p x_{ip} \\]\nTrong đó:\n\n\\(\\pi_{ij} = P(Y_i \\leq j)\\) là xác suất tích lũy của mức độ \\(j\\) hoặc thấp hơn.\n\\(\\alpha_j\\) là các ngưỡng (cutpoints) cho các mức độ khác nhau.\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) là các hệ số hồi quy.\n\n\n\n\n\nSo sánh Mô hình Ordinal Logit, Multinomial Logit và Binary Logit\n\n1. Mô hình Binary Logit\n\nĐặc điểm:\n\nSử dụng khi biến phụ thuộc chỉ có hai nhóm phân loại.\nMô hình tính toán xác suất thành công (1) so với thất bại (0).\nCông thức mô hình: \\[ \\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p \\]\nXác suất thành công được tính bằng: \\[ P(Y=1|X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}} \\]\n\n\n\n\n2. Mô hình Multinomial Logit\n\nĐặc điểm:\n\nSử dụng khi biến phụ thuộc có ba hoặc nhiều hơn các nhóm phân loại không có thứ tự.\nMô hình tính toán xác suất cho từng nhóm so với một nhóm tham chiếu.\nCông thức mô hình: \\[ \\log\\left(\\frac{P(Y=k)}{P(Y=K)}\\right) = \\beta_{k0} + \\beta_{k1} X_1 + \\ldots + \\beta_{kp} X_p \\]\nXác suất cho mỗi nhóm được tính bằng: \\[ P(Y=k|X) = \\frac{e^{\\beta_{k0} + \\beta_{k1} X_1 + \\ldots + \\beta_{kp} X_p}}{1 + \\sum_{j=1}^{K-1} e^{\\beta_{j0} + \\beta_{j1} X_1 + \\ldots + \\beta_{jp} X_p}} \\]\n\n\n\n\n3. Mô hình Ordinal Logit\n\nĐặc điểm:\n\nSử dụng khi biến phụ thuộc có nhiều nhóm phân loại có thứ tự.\nMô hình tính toán xác suất cho mỗi nhóm phân loại dựa trên thứ tự.\nCông thức mô hình: \\[ \\log\\left(\\frac{P(Y \\leq j)}{P(Y &gt; j)}\\right) = \\alpha_j + \\beta_1 X_1 + \\ldots + \\beta_p X_p \\]\nXác suất cho mỗi nhóm phân loại được tính bằng: \\[ P(Y = j|X) = P(Y \\leq j|X) - P(Y \\leq j-1|X) \\]\n\n\n\n\nSo sánh:\n\nSố lượng nhóm phân loại:\n\nBinary Logit: 2 nhóm.\nMultinomial Logit: 3 hoặc nhiều nhóm không có thứ tự.\nOrdinal Logit: 3 hoặc nhiều nhóm có thứ tự.\n\nCách tính xác suất:\n\nBinary Logit tính toán xác suất thành công so với thất bại.\nMultinomial Logit tính toán xác suất cho từng nhóm so với một nhóm tham chiếu.\nOrdinal Logit tính toán xác suất cho mỗi nhóm phân loại dựa trên thứ tự.\n\nĐặc điểm dữ liệu:\n\nBinary Logit thường được sử dụng cho các biến phụ thuộc có hai trạng thái như có hoặc không.\nMultinomial Logit được sử dụng khi biến phụ thuộc có ba hoặc nhiều hơn các nhóm không có thứ tự.\nOrdinal Logit thích hợp khi biến phụ thuộc có nhiều nhóm có thứ tự.\n\n\n\n\n\nPrediction\n\nMô hình Logit\nXác suất của một quan sát cho $ y_j = i $ là: \\[ p_{ij} = \\Pr(y_j = i) = \\Pr(\\kappa_{i-1} &lt; x_j \\beta + u \\leq \\kappa_i)  = \\frac{1}{1 + \\exp(-\\kappa_i + x_j \\beta)} - \\frac{1}{1 + \\exp(-\\kappa_{i-1} + x_j \\beta)} \\]\nTrong đó: - $ _0 $ được định nghĩa là $ -$ và $ _k $ là $ +$.\n\n\nMô hình Probit\nXác suất của một quan sát cho $ y_j = i $ là: \\[ p_{ij} = \\Pr(y_j = i) = \\Pr(\\kappa_{i-1} &lt; x_j \\beta + u \\leq \\kappa_i) = = \\Phi(\\kappa_i - x_j \\beta) - \\Phi(\\kappa_{i-1} - x_j \\beta) \\]\nTrong đó: - $ () $ là hàm phân phối tích lũy chuẩn (standard normal cumulative distribution function).\n\n\nHàm Log-Likelihood\nHàm log-likelihood là: \\[ \\ln L = \\sum_{j=1}^N Wj \\sum_{i=1}^k l_i(y_j) \\ln p_{ij} \\]\nTrong đó: - $ w_j $ là một trọng số tùy chọn. - $ l_i(y_j) $ : \\[\n  l_i(y_j) =\n  \\begin{cases}\n  1 & \\text{nếu } y_j = i \\\\\n  0 & \\text{ngược lại}\n  \\end{cases}\n  \\]\nNguồn: rologit\n\n\n\nCách giải thích kết quả của mô hình ordinal logistic\nĐể minh họa cách giải thích kết quả của mô hình ordinal logistic, hãy xem xét một ví dụ cụ thể. Giả sử chúng ta đã chạy một mô hình ordinal logistic để dự đoán mức độ hài lòng của khách hàng (1: Rất không hài lòng, 2: Không hài lòng, 3: Bình thường, 4: Hài lòng, 5: Rất hài lòng) dựa trên hai biến độc lập: thời gian chờ đợi (WaitTime) và chất lượng dịch vụ (ServiceQuality).\nDưới đây là kết quả từ mô hình ordinal logistic:\n                            Coefficient    Std. Error     Z-value    P-value\nIntercept 1 (cutpoint 1)        -2.50          0.30       -8.33      &lt;0.001\nIntercept 2 (cutpoint 2)        -1.50          0.25       -6.00      &lt;0.001\nIntercept 3 (cutpoint 3)        -0.50          0.20       -2.50       0.012\nIntercept 4 (cutpoint 4)         1.00          0.15        6.67      &lt;0.001\nWaitTime                        -0.75          0.10       -7.50      &lt;0.001\nServiceQuality                   1.20          0.08       15.00      &lt;0.001\n\n\nGiải thích Kết quả\n\n1. Các Hệ số Cắt (Cutpoints)\n\nIntercept 1 (cutpoint 1) = -2.50:\n\nĐây là điểm cắt giữa mức độ hài lòng “Rất không hài lòng” (1) và “Không hài lòng hoặc cao hơn” (2, 3, 4, 5).\nGiá trị âm cho thấy rằng với giá trị trung bình của các biến độc lập (WaitTime và ServiceQuality), khả năng là khách hàng sẽ không hài lòng hơn là rất không hài lòng.\n\nIntercept 2 (cutpoint 2) = -1.50:\n\nĐây là điểm cắt giữa mức độ hài lòng “Không hài lòng” (2) và “Bình thường hoặc cao hơn” (3, 4, 5).\nGiá trị âm nhưng lớn hơn -2.50, cho thấy rằng với giá trị trung bình của các biến độc lập, khả năng là khách hàng sẽ bình thường hoặc cao hơn là không hài lòng.\n\nIntercept 3 (cutpoint 3) = -0.50:\n\nĐây là điểm cắt giữa mức độ hài lòng “Bình thường” (3) và “Hài lòng hoặc cao hơn” (4, 5).\nGiá trị gần bằng 0 cho thấy rằng với giá trị trung bình của các biến độc lập, khả năng là khách hàng sẽ hài lòng hoặc cao hơn là bình thường.\n\nIntercept 4 (cutpoint 4) = 1.00:\n\nĐây là điểm cắt giữa mức độ hài lòng “Hài lòng” (4) và “Rất hài lòng” (5).\nGiá trị dương cho thấy rằng với giá trị trung bình của các biến độc lập, khả năng là khách hàng sẽ rất hài lòng hơn là hài lòng.\n\n\n\n\n2. Các Hệ số Hồi quy\n\nWaitTime = -0.75:\n\nHệ số này âm, cho thấy rằng thời gian chờ đợi dài hơn sẽ làm giảm mức độ hài lòng của khách hàng.\nCụ thể, mỗi đơn vị tăng thêm trong thời gian chờ đợi sẽ làm giảm log-odds của việc khách hàng đạt mức độ hài lòng cao hơn so với mức độ hài lòng hiện tại.\nHệ số này có ý nghĩa thống kê (P-value &lt; 0.001), cho thấy rằng thời gian chờ đợi có ảnh hưởng đáng kể đến mức độ hài lòng của khách hàng.\n\nServiceQuality = 1.20:\n\nHệ số này dương, cho thấy rằng chất lượng dịch vụ tốt hơn sẽ tăng mức độ hài lòng của khách hàng.\nCụ thể, mỗi đơn vị tăng thêm trong chất lượng dịch vụ sẽ tăng log-odds của việc khách hàng đạt mức độ hài lòng cao hơn so với mức độ hài lòng hiện tại.\nHệ số này cũng có ý nghĩa thống kê (P-value &lt; 0.001), cho thấy rằng chất lượng dịch vụ có ảnh hưởng đáng kể đến mức độ hài lòng của khách hàng.\n\n\n\n\n\nCách Tính Xác Suất\nGiả sử chúng ta muốn tính xác suất rằng một khách hàng với thời gian chờ đợi là 3 đơn vị và chất lượng dịch vụ là 4 đơn vị sẽ thuộc vào từng mức độ hài lòng.\n\nĐầu tiên, chúng ta tính giá trị tiềm ẩn \\(Y^*\\): \\[\nY^* = -0.75 \\times 3 + 1.20 \\times 4 = -2.25 + 4.8 = 2.55\n\\]\nSau đó, chúng ta tính xác suất cho từng mức độ hài lòng:\n\\[\nP(Y \\leq 1) = \\frac{1}{1 + \\exp(-(-2.50 - 2.55))} = \\frac{1}{1 + \\exp(5.05)} \\approx 0.006\n\\]\n\\[\nP(Y \\leq 2) = \\frac{1}{1 + \\exp(-(-1.50 - 2.55))} = \\frac{1}{1 + \\exp(4.05)} \\approx 0.017\n\\]\n\\[\nP(Y \\leq 3) = \\frac{1}{1 + \\exp(-(-0.50 - 2.55))} = \\frac{1}{1 + \\exp(3.05)} \\approx 0.045\n\\]\n\\[\nP(Y \\leq 4) = \\frac{1}{1 + \\exp(-(1.00 - 2.55))} = \\frac{1}{1 + \\exp(1.55)} \\approx 0.175\n\\]\n\\[\nP(Y = 5) = 1 - P(Y \\leq 4) = 1 - 0.175 = 0.825\n\\]\n\n\n\nTóm lại\n\nXác suất rằng khách hàng rất không hài lòng: 0.6%\nXác suất rằng khách hàng không hài lòng: 1.7% - 0.6%\nXác suất rằng khách hàng bình thường: 4.5% - 1.7%\nXác suất rằng khách hàng hài lòng: 17.5% - 4.5%\nXác suất rằng khách hàng rất hài lòng: 82.5%\n\n\n\nĐánh giá mô hình\n\nCohen’s Kappa:\n\nSử dụng: Với mô hình có biến phụ thuộc là một biến định danh (nominal variable) và muốn đánh giá mức độ chính xác của mô hình dự báo.\nVí dụ: Đánh giá phân loại giữa hai bác sĩ về chẩn đoán bệnh.\n\n⭐Weighted Cohen’s Kappa hoặc Kendall’s Tau:\n\nSử dụng: Với mô hình có biến phụ thuộc là một biến thứ bậc (ordinal variable).\n⭐Weighted Cohen’s Kappa: Điều chỉnh cho trọng số khác nhau của sự không chính xác của kết quả dự báo để phản ánh mức độ nghiêm trọng của các sai lệch dự đoán.\nKendall’s Tau: Được sử dụng để đánh giá mức độ tương quan và độ chính xác của dự báo.\nVí dụ: Đánh giá mức độ hài lòng của khách hàng trên thang điểm từ 1 đến 5.\n\nPearson’s Correlation:\n\nSử dụng: Với mô hình có biến phụ thuộc là một biến định lượng và muốn biết mức độ tương quan giữa kết quả thực tế và dự báo.\nVí dụ: So sánh kết quả của hai phương pháp đo lường huyết áp.\n\nFleiss’ Kappa:\n\nSử dụng: Khi bạn có nhiều hơn hai mẫu phụ thuộc danh nghĩa (nominal dependent samples) và muốn đánh giá mức độ đồng thuận giữa nhiều mô hình dự báo.\nVí dụ: Đánh giá sự đồng thuận giữa nhiều bác sĩ về chẩn đoán bệnh trong một tập hợp các trường hợp.\n\n\n\n\n\nImage Description\n\n\n\n\nQuadratic Weighted Kappa (QWK).\n\n1. Cohen’s Kappa\n\\[ \\kappa = \\frac{p_0 - p_e}{1 - p_e} \\]\n\n\\(p_0\\): Xác suất quan sát đồng thuận (tỷ lệ các giá trị dự đoán trùng với giá trị thực tế).\n\\(p_e\\): Xác suất đồng thuận ngẫu nhiên (tính dựa trên ma trận đồng thuận và xác suất của từng nhãn).\n\n\n\n2. Quadratic Weighted Kappa (QWK)\n\\[ QWK = 1 - \\frac{\\sum_{i,j} W_{ij} O_{ij}}{\\sum_{i,j} W_{ij} E_{ij}} \\]\n\n\\(O_{ij}\\): Ma trận quan sát (confusion matrix), với \\(O_{ij}\\) là số lượng trường hợp mà giá trị thực tế là \\(i\\) và giá trị dự đoán là \\(j\\).\n\\(E_{ij}\\): Ma trận kỳ vọng, được tính dựa trên xác suất của các nhãn.\n\\(W_{ij}\\): Ma trận trọng số, tính dựa trên khoảng cách giữa các nhãn \\(i\\) và \\(j\\), thường được tính như sau:\n\n\\[\nW_{ij} = \\frac{(i - j)^2}{(N - 1)^2}\n\\]\ntrong đó \\(N\\) là số lượng nhãn.\n\n\n3. Khoảng Giá Trị:\n\nQWK có giá trị dao động từ -1 đến 1:\n\n1: Đồng thuận hoàn hảo giữa dự báo và thực tế.\n0: Không có sự đồng thuận nào ngoài mức ngẫu nhiên.\n&lt; 0: Sự đồng thuận kém hơn mức ngẫu nhiên (phản đồng thuận).\n\n\n\n\n4. Ý Nghĩa:\n\n1: Mô hình dự báo hoàn toàn khớp với thực tế.\n0.81 - 1.00: Đồng thuận gần như hoàn hảo giữa dự báo và thực tế.\n0.61 - 0.80: Đồng thuận đáng kể.\n0.41 - 0.60: Đồng thuận khá.\n0.21 - 0.40: Đồng thuận vừa phải.\n0.01 - 0.20: Đồng thuận nhẹ.\n0: Không có sự đồng thuận.\n&lt; 0: Đồng thuận kém hơn ngẫu nhiên, mô hình hoặc đánh giá có vấn đề nghiêm trọng.\n\n\n\n\nVí dụ Minh Họa:\nGiả sử chúng ta có dự báo và giá trị thực tế trên thang điểm từ 1 đến 5 như sau:\n\n\n\nThực tế\nDự báo\n\n\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5\n5\n\n\n\nTrong trường hợp này, QWK sẽ bằng 1, biểu thị sự đồng thuận hoàn hảo giữa dự báo và thực tế.\nTuy nhiên, nếu có một số sự khác biệt như:\n\n\n\nThực tế\nDự báo\n\n\n\n\n1\n2\n\n\n2\n2\n\n\n3\n3\n\n\n4\n5\n\n\n5\n4\n\n\n\nQWK sẽ phản ánh mức độ đồng thuận kém hơn giữa dự báo và thực tế.\nQWK là một chỉ số quan trọng trong việc đánh giá mô hình dự báo trong các bài toán phân loại thứ bậc, đặc biệt khi các hạng mục có thứ tự. Nó giúp chúng ta hiểu rõ hơn về mức độ tương đồng giữa dự báo của mô hình và giá trị thực tế, qua đó cải thiện chất lượng mô hình.\n\n\nVí dụ 2:\n\n\n\nThực tế (Actual)\nDự đoán (Predicted)\n\n\n\n\nRất không hài lòng (1)\nRất không hài lòng (1)\n\n\nKhông hài lòng (2)\nKhông hài lòng (2)\n\n\nTrung lập (3)\nTrung lập (3)\n\n\nHài lòng (4)\nTrung lập (3)\n\n\nRất hài lòng (5)\nRất hài lòng (5)\n\n\nRất không hài lòng (1)\nKhông hài lòng (2)\n\n\nKhông hài lòng (2)\nRất không hài lòng (1)\n\n\nTrung lập (3)\nHài lòng (4)\n\n\nHài lòng (4)\nRất hài lòng (5)\n\n\nRất hài lòng (5)\nHài lòng (4)\n\n\n\n\n\n1. Cohen’s Kappa\nBước 1: Tạo ma trận đồng thuận (confusion matrix).\n\\[\n\\begin{array}{c|ccccc}\n& 1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n1 & 1 & 1 & 0 & 0 & 0 \\\\\n2 & 1 & 1 & 0 & 0 & 0 \\\\\n3 & 0 & 0 & 1 & 1 & 0 \\\\\n4 & 0 & 0 & 1 & 0 & 1 \\\\\n5 & 0 & 0 & 0 & 1 & 1 \\\\\n\\end{array}\n\\]\nBước 2: Tính xác suất quan sát đồng thuận \\(p_0\\) và xác suất đồng thuận ngẫu nhiên \\(p_e\\).\n\n\\(p_0 = \\frac{1 + 1 + 1 + 0 + 1}{10} = \\frac{4}{10} = 0.4\\)\nTính xác suất của từng nhãn:\n\n\\(P(\\text{Actual} = 1) = \\frac{2}{10}, P(\\text{Predicted} = 1) = \\frac{1}{10}\\)\n\\(P(\\text{Actual} = 2) = \\frac{2}{10}, P(\\text{Predicted} = 2) = \\frac{1}{10}\\)\n\\(P(\\text{Actual} = 3) = \\frac{2}{10}, P(\\text{Predicted} = 3) = \\frac{2}{10}\\)\n\\(P(\\text{Actual} = 4) = \\frac{2}{10}, P(\\text{Predicted} = 4) = \\frac{2}{10}\\)\n\\(P(\\text{Actual} = 5) = \\frac{2}{10}, P(\\text{Predicted} = 5) = \\frac{2}{10}\\)\n\n\\(p_e = (0.2 \\times 0.1) + (0.2 \\times 0.1) + (0.2 \\times 0.2) + (0.2 \\times 0.2) + (0.2 \\times 0.2) = 0.02 + 0.02 + 0.04 + 0.04 + 0.04 = 0.16\\)\n\nBước 3: Tính Cohen’s Kappa.\n\\[\n\\kappa = \\frac{p_0 - p_e}{1 - p_e} = \\frac{0.4 - 0.16}{1 - 0.16} = \\frac{0.24}{0.84} \\approx 0.286\n\\]\n\n\n3. Quadratic Weighted Kappa (QWK)\nBước 1: Tạo ma trận đồng thuận (confusion matrix).\n\\[\n\\begin{array}{c|ccccc}\n& 1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n1 & 1 & 1 & 0 & 0 & 0 \\\\\n2 & 1 & 1 & 0 & 0 & 0 \\\\\n3 & 0 & 0 & 1 & 1 & 0 \\\\\n4 & 0 & 0 & 1 & 0 & 1 \\\\\n5 & 0 & 0 & 0 & 1 & 1 \\\\\n\\end{array}\n\\]\nBước 2: Tạo ma trận trọng số dựa trên khoảng cách giữa các nhãn.\n$$\n\\[\\begin{array}{c|ccccc}\n& 1 &\n\n2 & 3 & 4 & 5 \\\\\n\\hline\n1 & 0 & 0.0625 & 0.25 & 0.5625 & 1 \\\\\n2 & 0.0625 & 0 & 0.0625 & 0.25 & 0.5625 \\\\\n3 & 0.25 & 0.0625 & 0 & 0.0625 & 0.25 \\\\\n4 & 0.5625 & 0.25 & 0.0625 & 0 & 0.0625 \\\\\n5 & 1 & 0.5625 & 0.25 & 0.0625 & 0 \\\\\n\\end{array}\\]\n$$\nBước 3: Tạo ma trận kỳ vọng dựa trên xác suất của các nhãn.\n\\[\n\\begin{array}{c|ccccc}\n& 1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\\\\n2 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\\\\n3 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\\\\n4 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\\\\n5 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\\\\n\\end{array}\n\\]\nBước 4: Tính QWK.\n\\[ QWK = 1 - \\frac{\\sum_{i,j} W_{ij} O_{ij}}{\\sum_{i,j} W_{ij} E_{ij}} \\]\n\\[ \\sum_{i,j} W_{ij} O_{ij} = 1 \\times 0 + 1 \\times 0.0625 + 1 \\times 0 + 0 \\times 0.0625 + 1 \\times 0 + 0 \\times 0.0625 + 1 \\times 0.25 + 1 \\times 0.0625 + 0 \\times 0.0625 + 1 \\times 0.0625 = 0.625\n\\]\n\\[\n\\sum_{i,j} W_{ij} E_{ij} = 0.1 \\times 0 + 0.1 \\times 0.0625 + 0.1 \\times 0.25 + 0.1 \\times 0.5625 + 0.1 \\times 1 + 0.1 \\times 0.0625 + 0.1 \\times 0 + 0.1 \\times 0.0625 + 0.1 \\times 0.25 + 0.1 \\times 0.5625 + 0.2 \\times 0.25 + 0.2 \\times 0.0625 + 0.2 \\times 0 + 0.2 \\times 0.0625 + 0.2 \\times 0.25 + 0.2 \\times 0.25 + 0.2 \\times 0.0625 + 0.2 \\times 0 + 0.2 \\times 0.0625 + 0.2 \\times 0.25 + 0.2 \\times 0.5625 + 0.2 \\times 0.25 + 0.2 \\times 0.0625 + 0.2 \\times 0 + 0.2 \\times 0.0625 = 1.5\n\\]\n\\[\nQWK = 1 - \\frac{0.625}{1.5} = 1 - 0.4167 = 0.5833\n\\]\n\n\nSo sánh trọng số Linear, Quadratic của hệ số kappa của Cohen (Cohen’s kappa coefficient)\nTrong đánh giá các mô hình phân loại thứ bậc, hai loại trọng số thường được sử dụng là trọng số tuyến tính (linear weights) và trọng số bậc hai (quadratic weights). Các trọng số này ảnh hưởng đến cách tính toán độ sai lệch giữa các dự đoán và giá trị thực tế.\n\nCông thức Tính Trọng số\n\nLinear Weight: \\[\nW_{ij} = \\frac{|i - j|}{N - 1}\n\\] Trong đó:\n\n\\(i\\) và \\(j\\) là các nhãn.\n\\(N\\) là tổng số nhãn.\n\nQuadratic Weight: \\[\nW_{ij} = \\frac{(i - j)^2}{(N - 1)^2}\n\\] Trong đó:\n\n\\(i\\) và \\(j\\) là các nhãn.\n\\(N\\) là tổng số nhãn.\n\n\n\n\n\nSo sánh Trọng số Linear và Quadratic cho các giá trị phân loại\nGiả sử chúng ta có các giá trị phân loại từ 0 đến 4:\n\n\n\nThực tế\nDự báo\n\n\n\n\n0\n0\n\n\n1\n0\n\n\n2\n0\n\n\n3\n0\n\n\n4\n0\n\n\n\n\nTrọng số Linear\nVới \\(N = 5\\):\n\\[ W_{ij}^{\\text{linear}} = \\frac{|i - j|}{4} \\]\n\n\n\nThực tế (\\(i\\))\nDự báo (\\(j\\))\n\\(\\|i - j\\|\\)\nTrọng số Linear\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n1\n0.25\n\n\n2\n0\n2\n0.5\n\n\n3\n0\n3\n0.75\n\n\n4\n0\n4\n1\n\n\n\n\n\nTrọng số Quadratic\nVới \\(N = 5\\):\n\\[ W_{ij}^{\\text{quadratic}} = \\frac{(i - j)^2}{16} \\]\n\n\n\nThực tế (\\(i\\))\nDự báo (\\(j\\))\n\\((i - j)^2\\)\nTrọng số Quadratic\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n1\n0.0625\n\n\n2\n0\n4\n0.25\n\n\n3\n0\n9\n0.5625\n\n\n4\n0\n16\n1\n\n\n\n\n\n\nNhận xét\n\nLinear Weights: Trọng số tăng tuyến tính với sự khác biệt giữa nhãn thực tế và nhãn dự đoán. Điều này có nghĩa là mỗi đơn vị sai lệch sẽ có một mức độ ảnh hưởng tương đương.\nQuadratic Weights: Trọng số tăng bậc hai với sự khác biệt giữa nhãn thực tế và nhãn dự đoán. Điều này có nghĩa là các sai lệch lớn hơn sẽ bị phạt nặng hơn rất nhiều so với các sai lệch nhỏ hơn.\n\n\n\nKết luận\n\nLinear Weights: Phù hợp hơn trong các trường hợp mà mọi sự khác biệt giữa các nhãn đều quan trọng như nhau.\nQuadratic Weights: Phù hợp hơn trong các trường hợp mà các sai lệch lớn cần được phạt nặng hơn, phản ánh mức độ nghiêm trọng của các dự đoán sai lệch lớn hơn.\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Tải dữ liệu\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\ndata = pd.read_csv(url, sep=';')\n\n# Kiểm tra dữ liệu\nprint(data.head())\nprint(data.info())\n\n# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\nX = data.drop('quality', axis=1)\ny = data['quality'] - 3  # Điều chỉnh các giá trị chất lượng\n\n# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Hiển thị một số thông tin về tập huấn luyện và tập kiểm tra\nprint(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n\n# Chuyển đổi biến mục tiêu thành dạng ordinal\ny_train = y_train.astype('category')\ny_test = y_test.astype('category')\n\n   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.4              0.70         0.00             1.9      0.076   \n1            7.8              0.88         0.00             2.6      0.098   \n2            7.8              0.76         0.04             2.3      0.092   \n3           11.2              0.28         0.56             1.9      0.075   \n4            7.4              0.70         0.00             1.9      0.076   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 11.0                  34.0   0.9978  3.51       0.56   \n1                 25.0                  67.0   0.9968  3.20       0.68   \n2                 15.0                  54.0   0.9970  3.26       0.65   \n3                 17.0                  60.0   0.9980  3.16       0.58   \n4                 11.0                  34.0   0.9978  3.51       0.56   \n\n   alcohol  quality  \n0      9.4        5  \n1      9.8        5  \n2      9.8        5  \n3      9.8        6  \n4      9.4        5  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1599 entries, 0 to 1598\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   fixed acidity         1599 non-null   float64\n 1   volatile acidity      1599 non-null   float64\n 2   citric acid           1599 non-null   float64\n 3   residual sugar        1599 non-null   float64\n 4   chlorides             1599 non-null   float64\n 5   free sulfur dioxide   1599 non-null   float64\n 6   total sulfur dioxide  1599 non-null   float64\n 7   density               1599 non-null   float64\n 8   pH                    1599 non-null   float64\n 9   sulphates             1599 non-null   float64\n 10  alcohol               1599 non-null   float64\n 11  quality               1599 non-null   int64  \ndtypes: float64(11), int64(1)\nmemory usage: 150.0 KB\nNone\nX_train shape: (1279, 11), y_train shape: (1279,)\nX_test shape: (320, 11), y_test shape: (320,)\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel\n\n# Xây dựng mô hình Ordinal Logistic Regression\nmodel = OrderedModel(y_train, X_train, distr='logit')\nresult = model.fit(method='bfgs')\n\n# In kết quả\nprint(result.summary())\n\nc:\\Users\\binhnn2\\anaconda3\\envs\\rdm\\lib\\site-packages\\statsmodels\\miscmodels\\ordinal_model.py:206: Warning: the endog has ordered == False, risk of capturing a wrong order for the categories. ordered == True preferred.\n  warnings.warn(\"the endog has ordered == False, \"\n\n\nOptimization terminated successfully.\n         Current function value: 0.966817\n         Iterations: 90\n         Function evaluations: 94\n         Gradient evaluations: 94\n                             OrderedModel Results                             \n==============================================================================\nDep. Variable:                quality   Log-Likelihood:                -1236.6\nModel:                   OrderedModel   AIC:                             2505.\nMethod:            Maximum Likelihood   BIC:                             2588.\nDate:                Fri, 05 Jul 2024                                         \nTime:                        16:28:50                                         \nNo. Observations:                1279                                         \nDf Residuals:                    1263                                         \nDf Model:                          11                                         \n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nfixed acidity            0.0783      0.091      0.859      0.390      -0.100       0.257\nvolatile acidity        -3.1342      0.449     -6.987      0.000      -4.013      -2.255\ncitric acid             -0.6750      0.520     -1.299      0.194      -1.693       0.343\nresidual sugar           0.0359      0.054      0.664      0.506      -0.070       0.142\nchlorides               -4.8182      1.462     -3.295      0.001      -7.684      -1.952\nfree sulfur dioxide      0.0185      0.008      2.371      0.018       0.003       0.034\ntotal sulfur dioxide    -0.0125      0.003     -4.676      0.000      -0.018      -0.007\ndensity                 -2.5943     75.908     -0.034      0.973    -151.372     146.183\npH                      -0.9766      0.669     -1.459      0.145      -2.288       0.335\nsulphates                2.5650      0.397      6.463      0.000       1.787       3.343\nalcohol                  0.8942      0.095      9.401      0.000       0.708       1.081\n0/1                     -2.4053     74.344     -0.032      0.974    -148.117     143.306\n1/2                      0.5993      0.169      3.548      0.000       0.268       0.930\n2/3                      1.3067      0.044     30.034      0.000       1.221       1.392\n3/4                      1.0376      0.042     24.546      0.000       0.955       1.120\n4/5                      1.1218      0.091     12.268      0.000       0.943       1.301\n========================================================================================\n\n\n\n# Các giá trị \\tau\nnum_of_thresholds = 5\nmodel.transform_threshold_params(result.params[-num_of_thresholds:])\n\narray([       -inf, -2.40532186, -0.58442067,  3.10969434,  5.93215937,\n        9.00252064,         inf])\n\n\n\n# Dự đoán trên tập kiểm tra\npred_proba = result.predict(X_test)\npred_one_hot = np.eye(len(set(y)))[np.round(pred_proba).astype(int)]\n\n# Chuyển đổi one-hot encoding sang mảng một chiều\npred = np.argmax(pred_one_hot, axis=1)[:,1]\n\n# Chuyển đổi lại y_test thành dạng int để phù hợp với các hàm đánh giá\ny_test_int = y_test.astype(int).to_numpy()\n\n\nfrom sklearn.metrics import cohen_kappa_score, confusion_matrix\nfrom scipy.stats import spearmanr\n# Đảm bảo kích thước phù hợp\nif len(pred) == len(y_test_int):\n    # Đánh giá mô hình sử dụng QWK\n    qwk = cohen_kappa_score(y_test_int, pred, weights='quadratic')\n    print(\"Quadratic Weighted Kappa: \", qwk)\n\n    # Đánh giá mô hình sử dụng Spearman's Rank Correlation\n    spearman_corr, _ = spearmanr(y_test_int, pred)\n    print(\"Spearman's Rank Correlation: \", spearman_corr)\n\n    # Đánh giá mô hình sử dụng Cohen's Kappa\n    cohen_kappa = cohen_kappa_score(y_test_int, pred)\n    print(\"Cohen's Kappa: \", cohen_kappa)\nelse:\n    print(\"Error: The size of y_test and pred do not match.\")\n\nQuadratic Weighted Kappa:  0.17843809134148214\nSpearman's Rank Correlation:  0.35484507423547046\nCohen's Kappa:  0.2695153974695631\n\n\n\n\nCác bước thực hiện mô hình có biến phụ thuộc dạng thứ bậc\n\nXác định nhãn\nChia train/test: Stratify\nChọn biến: RFECV (by kappa), Spearman Correlation\nThuật toán sử dụng: Logistic, RandomForest, Xgboost, LightGBM\nHàm loss: tùy chỉnh hàm loss: kappa loss\nĐánh giá mô hình: Cohen’s kappa coefficient\n\n\n\nTham khảo\n\nCohen’s Kappa:\n\nBài viết này trên trang DATAtab cung cấp một hướng dẫn chi tiết về cách tính toán và sử dụng chỉ số Kappa của Cohen. Chỉ số này được sử dụng để đo lường mức độ đồng thuận giữa hai hoặc nhiều người đánh giá trong các nhiệm vụ phân loại. Hướng dẫn bao gồm cả lý thuyết cơ bản về chỉ số Kappa và cách tính toán nó với ví dụ minh họa.\n\nWeighted Cohen’s Kappa:\n\nBài viết này giải thích chi tiết về chỉ số Weighted Cohen’s Kappa, một mở rộng của Cohen’s Kappa để xử lý các dữ liệu phân loại có thứ tự. Nội dung bao gồm các công thức tính toán cho trọng số tuyến tính và trọng số bậc hai, cùng với các ví dụ minh họa cụ thể để giúp hiểu rõ hơn về cách áp dụng chỉ số này trong thực tế.\n\nWine Quality EDA, Prediction, and Deploy:\n\nNotebook này trên Kaggle của Lus Fernando Torres thực hiện phân tích khám phá dữ liệu (EDA), xây dựng mô hình dự đoán chất lượng rượu vang và triển khai mô hình. Nó bao gồm các bước xử lý dữ liệu, khám phá dữ liệu trực quan, xây dựng và đánh giá các mô hình học máy khác nhau, cũng như hướng dẫn triển khai mô hình để sử dụng thực tế.\n\n1st Place Solution - RAPIDS & XGBoost:\n\nĐây là notebook của người chiến thắng giải nhất trong một cuộc thi Kaggle, sử dụng RAPIDS và XGBoost để xây dựng mô hình. Notebook này chi tiết các bước và chiến lược mà tác giả sử dụng để đạt được kết quả tốt nhất trong cuộc thi, bao gồm xử lý dữ liệu, lựa chọn đặc trưng, và tinh chỉnh mô hình.\n\nPS S03E05 Pytorch NN Modeling:\n\nNotebook này trên Kaggle của Ryanirl tập trung vào việc xây dựng mô hình mạng nơ-ron nhân tạo (NN) bằng PyTorch cho tập dữ liệu của một cuộc thi Kaggle (PS S03E05). Nội dung bao gồm các bước từ chuẩn bị dữ liệu, xây dựng và huấn luyện mô hình, đến đánh giá và tinh chỉnh mô hình.\n\nS3E5 XGB, LGB, Cat:\n\nNotebook của Tetsutani trên Kaggle thực hiện so sánh giữa ba mô hình học máy phổ biến: XGBoost, LightGBM, và CatBoost trên tập dữ liệu của cuộc thi S3E5. Nó bao gồm các bước chuẩn bị dữ liệu, huấn luyện từng mô hình, và đánh giá hiệu suất của chúng để tìm ra mô hình tốt nhất.\n\nDrop Features Reduction with t-SNE Model:\n\nNotebook này của David H Guerrero trên Kaggle trình bày cách giảm chiều dữ liệu bằng phương pháp t-SNE và chọn lọc đặc trưng dựa trên tầm quan trọng của chúng. Tác giả sử dụng phương pháp t-SNE để trực quan hóa dữ liệu và quyết định loại bỏ những đặc trưng không cần thiết, nhằm cải thiện hiệu suất mô hình.\n\nOrdinal Logistic Regression Solution:\n\nNotebook của Ronen Nakash trên Kaggle giải thích cách xây dựng mô hình hồi quy logistic thứ bậc để giải quyết các bài toán phân loại thứ bậc. Nội dung bao gồm các bước từ chuẩn bị dữ liệu, xây dựng và huấn luyện mô hình, đến đánh giá và tinh chỉnh mô hình, giúp hiểu rõ hơn về việc áp dụng hồi quy logistic thứ bậc trong thực tế.\n\nOrdinal Regression - Statsmodels:\n\nBài viết trên trang Statsmodels cung cấp hướng dẫn chi tiết về cách sử dụng lớp OrderedModel cho hồi quy thứ bậc với các phân phối khác nhau (probit, logit, custom). Nội dung bao gồm các bước tải dữ liệu, xây dựng mô hình, diễn giải các hệ số, tính toán các ngưỡng, dự đoán và đánh giá hiệu suất mô hình.\n\n\n\n\nMột số code thực hành\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nimport warnings\nimport lightgbm as lgb\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\n# Load the dataset\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\ndata = pd.read_csv(url, sep=';')\n\n# Define features and target\nX = data.drop('quality', axis=1)\ny = data['quality'] - 3\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert the data into LightGBM Dataset\ndtrain = lgb.Dataset(X_train_scaled, label=y_train)\ndtest = lgb.Dataset(X_test_scaled, label=y_test, reference=dtrain)\n\n# Set the parameters for LightGBM\nparams = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y)),  # Number of classes\n    'metric': 'multi_logloss',\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42,\n    'verbose': -1\n}\n\n# Train the LightGBM model\nbst = lgb.train(params, dtrain, num_boost_round=100, valid_sets=[dtest])\n\n# Predict on the test set\ny_pred_prob = bst.predict(X_test_scaled, num_iteration=bst.best_iteration)\ny_pred = np.argmax(y_pred_prob, axis=1)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nkappa = cohen_kappa_score(y_test, y_pred)\nqwk = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n\nprint(f'Test Accuracy: {accuracy:.2f}')\nprint(f'Cohen\\'s Kappa: {kappa:.2f}')\nprint(f'Quadratic Weighted Kappa: {qwk:.2f}')\n\nTest Accuracy: 0.68\nCohen's Kappa: 0.48\nQuadratic Weighted Kappa: 0.66\n\n\n\nSử dụng kappa evaluation function thay cho multi_logloss\n\n# Custom kappa evaluation function\ndef kappa_eval(preds, train_data):\n    y_true = train_data.get_label()\n    y_pred = np.argmax(preds.reshape(len(np.unique(y_true)), -1), axis=0)\n    kappa = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n    return 'kappa', kappa, True\n\n# Convert the data into LightGBM Dataset\ndtrain = lgb.Dataset(X_train_scaled, label=y_train)\ndtest = lgb.Dataset(X_test_scaled, label=y_test, reference=dtrain)\n\n# Set the parameters for LightGBM\nparams = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y)),  # Number of classes\n    'metric': 'multi_logloss',  # Default metric\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42,\n    'verbose': -1\n}\n\n# Train the LightGBM model with custom kappa evaluation\nbst = lgb.train(params, dtrain, num_boost_round=100, valid_sets=[dtest], feval=kappa_eval)\n\n# Predict on the test set\ny_pred_prob = bst.predict(X_test_scaled, num_iteration=bst.best_iteration)\ny_pred = np.argmax(y_pred_prob, axis=1)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nkappa = cohen_kappa_score(y_test, y_pred)\nqwk = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n\nprint(f'Test Accuracy: {accuracy:.2f}')\nprint(f'Cohen\\'s Kappa: {kappa:.2f}')\nprint(f'Quadratic Weighted Kappa: {qwk:.2f}')\n\nTest Accuracy: 0.68\nCohen's Kappa: 0.48\nQuadratic Weighted Kappa: 0.66\n\n\n\n\nChia dữ liệu thành 3 tập sử dụng kappa evaluation function thay cho multi_logloss\n\n# Split the data into train, eval, and test sets (60% train, 20% eval, 20% test)\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_eval, y_train, y_eval = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_eval_scaled = scaler.transform(X_eval)\nX_test_scaled = scaler.transform(X_test)\n\n# Custom kappa evaluation function\ndef kappa_eval(preds, train_data):\n    y_true = train_data.get_label()\n    y_pred = np.argmax(preds.reshape(len(np.unique(y_true)), -1), axis=0)\n    kappa = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n    return 'kappa', kappa, True\n\n# Convert the data into LightGBM Datasets\ndtrain = lgb.Dataset(X_train_scaled, label=y_train)\ndeval = lgb.Dataset(X_eval_scaled, label=y_eval, reference=dtrain)\ndtest = lgb.Dataset(X_test_scaled, label=y_test, reference=dtrain)\n\n# Set the parameters for LightGBM\nparams = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y)),  # Number of classes\n    'metric': 'multi_logloss',  # Default metric\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42,\n    'verbose': -1\n}\n\n# Train the LightGBM model with custom kappa evaluation\nbst = lgb.train(params, dtrain, num_boost_round=100, valid_sets=[deval], feval=kappa_eval)\n\n# Predict on the test set\ny_pred_prob = bst.predict(X_test_scaled, num_iteration=bst.best_iteration)\ny_pred = np.argmax(y_pred_prob, axis=1)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nkappa = cohen_kappa_score(y_test, y_pred)\nqwk = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n\nprint(f'Test Accuracy: {accuracy:.2f}')\nprint(f'Cohen\\'s Kappa: {kappa:.2f}')\nprint(f'Quadratic Weighted Kappa: {qwk:.2f}')\n\nTest Accuracy: 0.65\nCohen's Kappa: 0.44\nQuadratic Weighted Kappa: 0.60\n\n\n\n\nThay metric = ‘custom’\n\n# Set the parameters for LightGBM\nparams = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y)),  # Number of classes\n    'metric': 'custom',  # Default metrics\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42,\n    'verbose': -1\n}\n\n# Train the LightGBM model with custom kappa evaluation\nbst = lgb.train(params, dtrain, num_boost_round=100, valid_sets=[deval], feval=kappa_eval)\n\n# Predict on the test set\ny_pred_prob = bst.predict(X_test_scaled, num_iteration=bst.best_iteration)\ny_pred = np.argmax(y_pred_prob, axis=1)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nkappa = cohen_kappa_score(y_test, y_pred)\nqwk = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n\nprint(f'Test Accuracy: {accuracy:.2f}')\nprint(f'Cohen\\'s Kappa: {kappa:.2f}')\nprint(f'Quadratic Weighted Kappa: {qwk:.2f}')\n\nTest Accuracy: 0.65\nCohen's Kappa: 0.44\nQuadratic Weighted Kappa: 0.60",
    "crumbs": [
      "Statistic",
      "Mô Hình Hồi Quy Logistic Thứ Tự"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html",
    "href": "statistic/PSI_KS_ENTROPY.html",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "",
    "text": "Chỉ số Ổn định tổng thể (PSI - Population Stability Index) là một phép đo thường được sử dụng trong ngành ngân hàng, đặc biệt trong việc xếp hạng tín dụng, để giám sát sự ổn định và hiệu suất của các mô hình xếp hạng theo thời gian. Đây là cách để định lượng sự thay đổi trong phân phối tổng thể, điều này có thể ảnh hưởng đến sức mạnh dự đoán của một mô hình.\nPSI được tính như sau:\n\\[ PSI = \\sum_{i=1}^{N} (Actual_{i} - Expected_{i}) \\log \\left( \\frac{Actual_{i}}{Expected_{i}} \\right) \\]\nTrong đó:\n\n\\(Actual_{i}\\) và \\(Expected_{i}\\) là tỷ lệ quan sát rơi vào bin (i) cho dữ liệu thực tế (mới hoặc kiểm tra) và dữ liệu mong đợi (cũ hoặc đào tạo), tương ứng. - (N) là tổng số bins.\n\nCách giải thích thông thường về các giá trị PSI như sau:\n\nPSI &lt; 0.1: Không có sự thay đổi đáng kể về tổng thể. Mô hình thường được coi là ổn định.\n0.1 ≤ PSI &lt; 0.25: Có một số thay đổi nhỏ trong tổng thể, có thể cần được điều tra thêm.\nPSI ≥ 0.25: Sự thay đổi đáng kể về tổng thể. Mô hình có thể không còn phù hợp và cần được cập nhật hoặc xây dựng lại.\n\nLưu ý rằng những ngưỡng này không phải là cố định và có thể thay đổi tùy thuộc vào đặc điểm cụ thể của tình huống và mức độ rủi ro bạn sẵn sàng chấp nhận.\n\nimport numpy as np\nimport warnings\n\ndef calculate_psi(expected, actual, categorical=False, bins=None):\n    \"\"\"\n    Calculate the Population Stability Index (PSI) between two sets of data.\n\n    Args:\n        expected (numpy.ndarray): The expected values.\n        actual (numpy.ndarray): The actual values to be compared against the expected values.\n        categorical (bool, optional): Set to True if the variables are categorical. Default is False.\n        bins (int, optional): Number of bins for binning numeric data. Default is None (auto).\n\n    Returns:\n        float: The calculated PSI value.\n    \"\"\"\n    # Check if the variables are categorical\n    if categorical:\n        # Get unique categories\n        categories = np.unique(np.concatenate([expected, actual]))\n\n        # Issue a warning if the number of unique categories exceeds 20\n        if len(categories) &gt; 20:\n            warnings.warn(\"Warning: Number of unique categories exceeds 20.\")\n\n        # Calculate the expected and actual proportions for each category\n        expected_probs = np.array([np.sum(expected == cat) for cat in categories]) / len(expected)\n        actual_probs = np.array([np.sum(actual == cat) for cat in categories]) / len(actual)\n    else:\n        expected = expected[~np.isnan(expected)]\n        actual = actual[~np.isnan(actual)]\n        # Apply binning for numeric columns\n        if bins is None:\n            bins = 10  # Default to 10 bins, you can change this value as needed\n\n        # Calculate the bin edges based on percentiles\n        bin_edges = np.percentile(np.hstack((expected, actual)), np.linspace(0, 100, bins + 1))\n\n        # Calculate the expected and actual proportions for each bin\n        expected_probs, _ = np.histogram(expected, bins=bin_edges)\n        actual_probs, _ = np.histogram(actual, bins=bin_edges)\n\n        # Normalize to get proportions\n        expected_probs = expected_probs / len(expected)\n        actual_probs = actual_probs / len(actual)\n        \n    # Initialize PSI\n    psi_value = 0\n\n    # Loop over each bin or category\n    for i in range(len(expected_probs)):\n        # Avoid division by zero and log of zero\n        if expected_probs[i] == 0 or actual_probs[i] == 0:\n            continue\n        # Calculate the PSI for this bin or category\n        psi_value += (expected_probs[i] - actual_probs[i]) * np.log(expected_probs[i] / actual_probs[i])\n\n    return psi_value\n\n# Example usage\nexpected_data = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 6, 6, 7, 8, 9, 10])\nactual_data = np.array([1, 1, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 7, 8, 9, 10])\n\npsi = calculate_psi(expected_data, actual_data, categorical=False, bins=5)\nprint(\"Population Stability Index (PSI):\", psi)\n\nPopulation Stability Index (PSI): 0.028379201320332823",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html#psi",
    "href": "statistic/PSI_KS_ENTROPY.html#psi",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "",
    "text": "Chỉ số Ổn định tổng thể (PSI - Population Stability Index) là một phép đo thường được sử dụng trong ngành ngân hàng, đặc biệt trong việc xếp hạng tín dụng, để giám sát sự ổn định và hiệu suất của các mô hình xếp hạng theo thời gian. Đây là cách để định lượng sự thay đổi trong phân phối tổng thể, điều này có thể ảnh hưởng đến sức mạnh dự đoán của một mô hình.\nPSI được tính như sau:\n\\[ PSI = \\sum_{i=1}^{N} (Actual_{i} - Expected_{i}) \\log \\left( \\frac{Actual_{i}}{Expected_{i}} \\right) \\]\nTrong đó:\n\n\\(Actual_{i}\\) và \\(Expected_{i}\\) là tỷ lệ quan sát rơi vào bin (i) cho dữ liệu thực tế (mới hoặc kiểm tra) và dữ liệu mong đợi (cũ hoặc đào tạo), tương ứng. - (N) là tổng số bins.\n\nCách giải thích thông thường về các giá trị PSI như sau:\n\nPSI &lt; 0.1: Không có sự thay đổi đáng kể về tổng thể. Mô hình thường được coi là ổn định.\n0.1 ≤ PSI &lt; 0.25: Có một số thay đổi nhỏ trong tổng thể, có thể cần được điều tra thêm.\nPSI ≥ 0.25: Sự thay đổi đáng kể về tổng thể. Mô hình có thể không còn phù hợp và cần được cập nhật hoặc xây dựng lại.\n\nLưu ý rằng những ngưỡng này không phải là cố định và có thể thay đổi tùy thuộc vào đặc điểm cụ thể của tình huống và mức độ rủi ro bạn sẵn sàng chấp nhận.\n\nimport numpy as np\nimport warnings\n\ndef calculate_psi(expected, actual, categorical=False, bins=None):\n    \"\"\"\n    Calculate the Population Stability Index (PSI) between two sets of data.\n\n    Args:\n        expected (numpy.ndarray): The expected values.\n        actual (numpy.ndarray): The actual values to be compared against the expected values.\n        categorical (bool, optional): Set to True if the variables are categorical. Default is False.\n        bins (int, optional): Number of bins for binning numeric data. Default is None (auto).\n\n    Returns:\n        float: The calculated PSI value.\n    \"\"\"\n    # Check if the variables are categorical\n    if categorical:\n        # Get unique categories\n        categories = np.unique(np.concatenate([expected, actual]))\n\n        # Issue a warning if the number of unique categories exceeds 20\n        if len(categories) &gt; 20:\n            warnings.warn(\"Warning: Number of unique categories exceeds 20.\")\n\n        # Calculate the expected and actual proportions for each category\n        expected_probs = np.array([np.sum(expected == cat) for cat in categories]) / len(expected)\n        actual_probs = np.array([np.sum(actual == cat) for cat in categories]) / len(actual)\n    else:\n        expected = expected[~np.isnan(expected)]\n        actual = actual[~np.isnan(actual)]\n        # Apply binning for numeric columns\n        if bins is None:\n            bins = 10  # Default to 10 bins, you can change this value as needed\n\n        # Calculate the bin edges based on percentiles\n        bin_edges = np.percentile(np.hstack((expected, actual)), np.linspace(0, 100, bins + 1))\n\n        # Calculate the expected and actual proportions for each bin\n        expected_probs, _ = np.histogram(expected, bins=bin_edges)\n        actual_probs, _ = np.histogram(actual, bins=bin_edges)\n\n        # Normalize to get proportions\n        expected_probs = expected_probs / len(expected)\n        actual_probs = actual_probs / len(actual)\n        \n    # Initialize PSI\n    psi_value = 0\n\n    # Loop over each bin or category\n    for i in range(len(expected_probs)):\n        # Avoid division by zero and log of zero\n        if expected_probs[i] == 0 or actual_probs[i] == 0:\n            continue\n        # Calculate the PSI for this bin or category\n        psi_value += (expected_probs[i] - actual_probs[i]) * np.log(expected_probs[i] / actual_probs[i])\n\n    return psi_value\n\n# Example usage\nexpected_data = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 6, 6, 7, 8, 9, 10])\nactual_data = np.array([1, 1, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 7, 8, 9, 10])\n\npsi = calculate_psi(expected_data, actual_data, categorical=False, bins=5)\nprint(\"Population Stability Index (PSI):\", psi)\n\nPopulation Stability Index (PSI): 0.028379201320332823",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html#ks-kolmogorov-smirnov",
    "href": "statistic/PSI_KS_ENTROPY.html#ks-kolmogorov-smirnov",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "KS (Kolmogorov-Smirnov)",
    "text": "KS (Kolmogorov-Smirnov)\nKiểm định KS (Kolmogorov-Smirnov) là một kiểm định thống kê phi tham số dùng để so sánh hai phân phối tích lũy (CDFs) hoặc một mẫu dữ liệu với một phân phối lý thuyết. Nó có hai ứng dụng chính:\n\nKiểm tra sự phù hợp của mẫu: Được sử dụng để kiểm tra xem một tập dữ liệu có tuân theo một phân phối lý thuyết cụ thể (như phân phối chuẩn, phân phối đều, v.v.) hay không.\nSo sánh hai mẫu dữ liệu: Được sử dụng để kiểm tra xem hai tập dữ liệu có xuất phát từ cùng một phân phối gốc hay không.\n\nCông thức tính toán cho chỉ số KS là:\n\\[ D = \\max |F_1(x) - F_2(x)| \\]\nTrong đó: - ( F_1(x) ) và ( F_2(x) ) là hai hàm phân phối tích lũy cần so sánh. - ( D ) là giá trị lớn nhất của sự khác biệt tuyệt đối giữa hai hàm phân phối tích lũy trên toàn bộ phạm vi x.\nMột đặc điểm quan trọng của kiểm định KS là nó không yêu cầu giả định về dạng của phân phối, làm cho nó trở thành một công cụ mạnh mẽ và linh hoạt khi so sánh phân phối.\n\nfrom scipy.stats import ks_2samp\n\n# Generate two sample datasets\ndata1 = np.random.normal(0, 1, 1000)\ndata2 = np.random.normal(0.5, 1.5, 1000)\n\n# Compute the KS statistic and p-value\nks_statistic, p_value = ks_2samp(data1, data2)\n\nks_statistic, p_value\n\n(0.221, 8.402229010639847e-22)",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html#divergence-test-entropy",
    "href": "statistic/PSI_KS_ENTROPY.html#divergence-test-entropy",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "Divergence Test (entropy)",
    "text": "Divergence Test (entropy)\nKiểm định Divergence, thường được gọi là Divergence Kullback-Leibler (KL), là một chỉ số đo sự khác biệt giữa một phân phối xác suất so với một phân phối xác suất thứ hai mong đợi. Nó được sử dụng để so sánh hai phân phối xác suất cho cùng một sự kiện.\nCho hai phân phối xác suất, \\(P\\) và \\(Q\\), Divergence Kullback-Leibler của \\(Q\\) so với \\(P\\) được định nghĩa như sau:\n\\[ D_{KL}(P||Q) = \\sum_{i} P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)\\]\nTrong đó: - \\(P(i)\\) là xác suất của sự kiện \\(i\\) theo phân phối \\(P\\), - \\(Q(i)\\) là xác suất của sự kiện \\(i\\) theo phân phối \\(Q\\), - Tổng được tính trên tất cả các sự kiện \\(i\\) có thể xảy ra.\nMột số điểm quan trọng về KL Divergence:\n\nKhông đối xứng: \\(D_{KL}(P||Q)\\) không bằng \\(D_{KL}(Q||P)\\). Điều này có nghĩa là Divergence KL của \\(Q\\) so với \\(P\\) không giống như Divergence KL của \\(P\\) so với \\(Q\\).\nKhông âm: Divergence KL luôn không âm, và nó bằng không chỉ khi \\(P\\) và \\(Q\\) là cùng một phân phối.\nĐơn vị: Divergence KL được đo bằng bit nếu logarithm có cơ số 2 (log2), hoặc bằng nats nếu logarithm có cơ số \\(e\\) (logarithm tự nhiên).\n\nTrên thực tế, KL Divergence có thể được sử dụng để đo sự khác biệt giữa phân phối thực tế và dự đoán, hoặc giữa một phân phối quan sát và một phân phối lý thuyết. Nó đặc biệt phổ biến trong các lĩnh vực như lý thuyết thông tin và học máy.\n\nimport numpy as np\n\ndef kl_divergence(p, q):\n    \"\"\"Compute KL divergence of two probability distributions.\"\"\"\n    return np.sum(p * np.log(p / q))\n\n# Example distributions\np = np.array([0.4, 0.5, 0.1])\nq = np.array([0.3, 0.4, 0.3])\n\n# Ensure the distributions are valid (i.e., sum to 1 and non-negative)\nassert np.all(p &gt;= 0) and np.all(q &gt;= 0)\nassert np.isclose(p.sum(), 1) and np.isclose(q.sum(), 1)\n\n# Calculate KL Divergence\ndivergence_value = kl_divergence(p, q)\nprint(f\"KL Divergence between p and q: {divergence_value:.4f}\")\n\nKL Divergence between p and q: 0.1168\n\n\n\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef kl_divergence(p, q):\n    \"\"\"Compute KL divergence of two probability distributions.\"\"\"\n    return entropy(p, q)\n\n# Example distributions\np = np.array([0.4, 0.5, 0.1])\nq = np.array([0.3, 0.4, 0.3])\n\n# Calculate KL Divergence from p to q\ndivergence_value = kl_divergence(p, q)\n\nprint(f\"KL Divergence from p to q: {divergence_value:.4f}\")\n\nKL Divergence from p to q: 0.1168",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html#so-sánh-ks-và-divergence",
    "href": "statistic/PSI_KS_ENTROPY.html#so-sánh-ks-và-divergence",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "So sánh KS và Divergence",
    "text": "So sánh KS và Divergence\n\nMục đích:\n\nKiểm định KS: Đây là một kiểm định phi tham số được sử dụng để xác định xem hai mẫu có xuất phát từ cùng một phân phối hay không. Thống kê KS đo sự khác biệt lớn nhất giữa các hàm phân phối tích lũy (CDFs) của hai mẫu.\nKiểm định Divergence (KL Divergence): Nó đo cách một phân phối xác suất khác biệt so với một phân phối xác suất thứ hai mong đợi. Nó thường được sử dụng trong lý thuyết thông tin để đo “khoảng cách” giữa hai phân phối.\n\nKết quả:\n\nKiểm định KS: Kết quả là một chỉ số (D) đại diện cho sự khác biệt lớn nhất giữa hai CDFs và một giá trị p kiểm tra giả thuyết rằng hai mẫu được rút ra từ cùng một phân phối.\nKiểm định Divergence (KL Divergence): Kết quả là một giá trị không âm, trong đó giá trị 0 chỉ ra rằng hai phân phối là giống nhau. Lưu ý rằng KL Divergence không đối xứng, tức là \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\).\n\nGiả định:\n\nKiểm định KS: Không giả định về phân phối của dữ liệu.\nKiểm định Divergence (KL Divergence): Giả định \\(Q(i) &gt; 0\\) cho bất kỳ \\(i\\) nào sao cho \\(P(i) &gt; 0\\), nếu không sự khác biệt sẽ vô cùng.\n\nỨng dụng:\n\nKiểm định KS: Thường được sử dụng trong kiểm định giả thuyết để xác định xem một mẫu dữ liệu có tuân theo một phân phối cụ thể hay không.\nKiểm định Divergence (KL Divergence): Rộng rãi được sử dụng trong lý thuyết thông tin, học máy và thống kê, đặc biệt khi so sánh một phân phối thực nghiệm với một phân phối lý thuyết.\n\nGiải thích:\n\nKiểm định KS: Một giá trị p nhỏ cho thấy rằng hai mẫu đến từ các phân phối khác nhau.\nKiểm định Divergence (KL Divergence): Một Divergence KL lớn hơn chỉ ra rằng hai phân phối khác biệt hơn so với nhau.\n\n\nTóm lại, mặc dù cả Kiểm định KS và KL Divergence đều được sử dụng để so sánh các phân phối, nhưng chúng có các phương pháp, giải thích và ứng dụng khác nhau. Sự lựa chọn giữa chúng phụ thuộc vào vấn đề cụ thể và bản chất của dữ liệu.",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html#so-sánh-giá-trị-của-các-kiểm-định",
    "href": "statistic/PSI_KS_ENTROPY.html#so-sánh-giá-trị-của-các-kiểm-định",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "So sánh giá trị của các kiểm định",
    "text": "So sánh giá trị của các kiểm định\nGiá trị tối thiểu (min) và tối đa (max) của KS Statistic và KL Divergence có thể xảy ra trong các trường hợp sau:\n\nKS Statistic (Kolmogorov-Smirnov Statistic):\n\nMin: Giá trị KS Statistic sẽ gần bằng 0 khi hai phân phối hoàn toàn giống nhau hoặc rất giống nhau. Điều này có nghĩa rằng các dãy số có thể hoàn toàn trùng nhau.\nMax: Giá trị KS Statistic sẽ đạt tới giá trị lớn nhất khi hai phân phối hoàn toàn khác biệt và không có sự tương đồng nào giữa chúng. Trong trường hợp này, hai phân phối có dãy số hoàn toàn không trùng nhau.\n\nKL Divergence (Kullback-Leibler Divergence):\n\nMin: KL Divergence sẽ gần bằng 0 khi hai phân phối hoàn toàn giống nhau. Điều này có nghĩa rằng không có sự khác biệt nào giữa hai phân phối về thông tin.\nMax: Giá trị KL Divergence sẽ lớn nhất khi hai phân phối hoàn toàn khác biệt và không chia sẻ bất kỳ thông tin nào. Trong trường hợp này, phân phối này không thể ước lượng hoặc dự đoán từ phân phối kia.\n\n\n\nimport numpy as np\nfrom scipy.stats import ks_2samp\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\n\n# Number of examples\nnum_examples = 5\n\n# Create a figure and axis for the plot\nfig, axs = plt.subplots(num_examples, 2, figsize=(12, 16))\nfig.subplots_adjust(hspace=0.5)\n\nfor example in range(num_examples):\n    # Generate two sample datasets with different loc and scale parameters\n    np.random.seed(example)\n    loc1 = np.random.uniform(-2, 2)  # Random loc for sample1 between -1 and 1\n    scale1 = np.random.uniform(0.5, 2)  # Random scale for sample1 between 0.5 and 2\n    sample1 = np.random.normal(loc1, scale1, 1000)\n    \n    loc2 = np.random.uniform(1, 10)  # Random loc for sample2 between -1 and 1\n    scale2 = np.random.uniform(0.5, 2)  # Random scale for sample2 between 0.5 and 2\n    sample2 = np.random.normal(loc2, scale2, 1000)\n    \n    # Compute KS statistic and p-value\n    ks_statistic, ks_p_value = ks_2samp(sample1, sample2)\n    \n    # Compute KL Divergence\n    hist_sample1, _ = np.histogram(sample1, bins=10)\n    hist_sample2, _ = np.histogram(sample2, bins=10)\n    kl_divergence_value = entropy(hist_sample1, hist_sample2)\n    \n    # Plot the histograms of the two samples with mean and std in the legend\n    legend_label = f'Sample 1\\nMean: {np.mean(sample1):.2f}\\nStd: {np.std(sample1):.2f}'\n    legend_label_sample2 = f'Sample 2\\nMean: {np.mean(sample2):.2f}\\nStd: {np.std(sample2):.2f}'\n    axs[example, 0].hist(sample1, bins=30, alpha=0.5, color='blue', label=legend_label)\n    axs[example, 0].hist(sample2, bins=30, alpha=0.5, color='orange', label=legend_label_sample2)\n    axs[example, 0].set_title(f'Example {example + 1}: Distributions')\n    axs[example, 0].legend()\n    \n    # Plot KS Statistic and KL Divergence with labels in the legend\n    axs[example, 1].bar(['KS Statistic', 'KL Divergence'], [ks_statistic, kl_divergence_value], color=['blue', 'green'])\n    legend_label = f'KS Statistic: {ks_statistic:.4f}\\nKL Divergence: {kl_divergence_value:.4f}'\n    axs[example, 1].set_title(f'Example {example + 1}: Comparison')\n    axs[example, 1].legend([legend_label])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nKhi 2 phân phối có khác biệt về giá trị trung bình thì KS sẽ khác nhau\nCòn khi khác biệt về dạng phân phối thì Entropy sẽ khác nhau\n\n\nimport numpy as np\nfrom scipy.stats import ks_2samp\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\n\n# Number of examples\nnum_examples = 5\n\n# Create a figure and axis for the plot\nfig, axs = plt.subplots(num_examples, 2, figsize=(12, 16))\nfig.subplots_adjust(hspace=0.5)\n\nfor example in range(num_examples):\n    # Generate two sample datasets with different loc and scale parameters\n    np.random.seed(example)\n    loc1 = np.random.uniform(-2, 2)  # Random loc for sample1 between -1 and 1\n    scale1 = np.random.uniform(0.5, 2)  # Random scale for sample1 between 0.5 and 2\n    sample1 = np.random.normal(loc1, scale1, 1000)\n    \n    add_value = np.random.uniform(1, 10)  # Random loc for sample2 between -1 and 1    \n    sample2 = sample1 + add_value\n    \n    # Compute KS statistic and p-value\n    ks_statistic, ks_p_value = ks_2samp(sample1, sample2)\n    \n    # Compute KL Divergence\n    hist_sample1, _ = np.histogram(sample1, bins=10)\n    hist_sample2, _ = np.histogram(sample2, bins=10)\n    kl_divergence_value = entropy(hist_sample1, hist_sample2)\n    \n    # Plot the histograms of the two samples with mean and std in the legend\n    legend_label = f'Sample 1\\nMean: {np.mean(sample1):.2f}\\nStd: {np.std(sample1):.2f}'\n    legend_label_sample2 = f'Sample 2\\nMean: {np.mean(sample2):.2f}\\nStd: {np.std(sample2):.2f}'\n    axs[example, 0].hist(sample1, bins=30, alpha=0.5, color='blue', label=legend_label)\n    axs[example, 0].hist(sample2, bins=30, alpha=0.5, color='orange', label=legend_label_sample2)\n    axs[example, 0].set_title(f'Example {example + 1}: Distributions')\n    axs[example, 0].legend()\n    \n    # Plot KS Statistic and KL Divergence with labels in the legend\n    axs[example, 1].bar(['KS Statistic', 'KL Divergence'], [ks_statistic, kl_divergence_value], color=['blue', 'green'])\n    legend_label = f'KS Statistic: {ks_statistic:.4f}\\nKL Divergence: {kl_divergence_value:.4f}'\n    axs[example, 1].set_title(f'Example {example + 1}: Comparison')\n    axs[example, 1].legend([legend_label])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nSử dụng 2 loại phân phối khác nhau, giá trị Entropy có sự khác biệt rõ rệt\n\n\nimport numpy as np\nfrom scipy.stats import ks_2samp, beta, gamma\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\n\n# Function to calculate KL Divergence\ndef kl_divergence(p, q):\n    return entropy(p, q)\n\n# Number of examples\nnum_examples = 5\n\n# Create a figure and axis for the plot\nfig, axs = plt.subplots(num_examples, 2, figsize=(12, 16))\nfig.subplots_adjust(hspace=0.5)\n\nfor example in range(num_examples):\n    # Generate two sample datasets with different loc and scale parameters\n    np.random.seed(example)\n    \n    # Sample1 follows a gamma distribution\n    shape1 = np.random.uniform(1, 5)  # Random shape parameter for gamma distribution\n    scale1 = np.random.uniform(0.5, 2)  # Random scale parameter for gamma distribution\n    sample1 = gamma.rvs(shape1, scale=scale1, size=1000)\n    \n    # Sample2 follows a beta distribution\n    a2 = np.random.uniform(1, 10)  # Random shape parameter for beta distribution\n    b2 = np.random.uniform(1, 10)  # Random scale parameter for beta distribution\n    sample2 = beta.rvs(a2, b2, size=1000)\n    \n    # Compute KS statistic and p-value\n    ks_statistic, ks_p_value = ks_2samp(sample1, sample2)\n    \n    # Compute KL Divergence\n    hist_sample1, _ = np.histogram(sample1, bins=10)\n    hist_sample2, _ = np.histogram(sample2, bins=10)\n    kl_divergence_value = kl_divergence(hist_sample1, hist_sample2)\n    \n    # Plot the histograms of the two samples with mean and std in the legend\n    legend_label_sample1 = f'Sample 1 (Gamma)\\nShape: {shape1:.2f}, Scale: {scale1:.2f}\\nMean: {np.mean(sample1):.2f}\\nStd: {np.std(sample1):.2f}'\n    legend_label_sample2 = f'Sample 2 (Beta)\\na={a2:.2f}, b={b2:.2f}\\nMean: {np.mean(sample2):.2f}\\nStd: {np.std(sample2):.2f}'\n    axs[example, 0].hist(sample1, bins=30, alpha=0.5, color='blue', label=legend_label_sample1)\n    axs[example, 0].hist(sample2, bins=30, alpha=0.5, color='orange', label=legend_label_sample2)\n    axs[example, 0].set_title(f'Example {example + 1}: Distributions')\n    axs[example, 0].legend()\n    \n    # Plot KS Statistic and KL Divergence with labels in the legend\n    axs[example, 1].bar(['KS Statistic', 'KL Divergence'], [ks_statistic, kl_divergence_value], color=['blue', 'green'])\n    legend_label = f'KS Statistic: {ks_statistic:.4f}\\nKL Divergence: {kl_divergence_value:.4f}'\n    axs[example, 1].set_title(f'Example {example + 1}: Comparison')\n    axs[example, 1].legend([legend_label])\n\nplt.show()",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/DiD.html",
    "href": "statistic/DiD.html",
    "title": "Difference-in-Differences (DiD)",
    "section": "",
    "text": "Sự khác biệt trong khác biệt (DiD) là một phương pháp thống kê thường được sử dụng trong đánh giá tác động để ước tính tác động nhân quả của một phương pháp điều trị hoặc sự thay đổi của chính sách nào đó. Phương pháp này rất hữu hiệu khi bạn có dữ liệu quan sát và muốn so sánh kết quả của một nhóm được can thiệp hay bị ảnh hưởng từ chính sách (the “treatment group”) với kết quả của một nhóm không can thiệp (the “control group”) trước và sau khi việc can thiệp được thực hiện. Dưới đây là các bước chính để tiến hành đánh giá tác động bằng DiD:\n\nXác định nhóm bị can thiệp và nhóm đối chứng (Treatment and Control Groups):\n\nXác định nhóm bị can thiệp hoặc thay đổi chính sách.\nLựa chọn nhóm đối chứng tương tự như nhóm can thiệp nhưng chưa bị tác động của chính sách. Nhóm này lý tưởng nhất phải có những đặc điểm và xu hướng tương tự theo thời gian.\n\nThu thập dữ liệu:\n\nThu thập dữ liệu về biến kết quả được quan tâm của cả nhóm can thiệp và nhóm đối chứng trong nhiều khoảng thời gian, trước và sau can thiệp. Bạn cần ít nhất hai thời điểm trước và hai thời điểm sau can thiệp.\n\nGiả định về xu hướng song song:\n\nMột giả định quan trọng trong phân tích DiD là nếu không có biện pháp can thiệp, nhóm can thiệp và đối chứng sẽ có xu hướng song song theo thời gian. Nói cách khác, kết quả của họ sẽ tiến triển tương tự nếu việc can thiệp không xảy ra.\n\nƯớc tính mô hình DiD:\n\nChạy mô hình hồi quy bao gồm các biến chỉ số cho cả nhóm can thiệp và các khoảng thời gian sau can thiệp. Công cụ ước tính DiD là hệ số về sự tương tác giữa các biến chỉ báo này.\n\nMô hình DiD:\n\\[Y_{it} = β_0 + β_1 * Treatment_i + β_2 * Post_t + β_3 * (Treatment_i * Post_t) + ε_{it}\\]\n\n\\(Y_{it}\\) là biến kết quả của cá nhân i tại thời điểm t.\n\\(Treatment_i\\) là chỉ báo nhị phân cho biết cá nhân i thuộc nhóm can thiệp (1) hay nhóm đối chứng (0).\n\\(Post_t\\) là chỉ báo nhị phân cho biết khoảng thời gian là sau can thiệp (1) hay trước (0).\n\\(β_3\\) là ước lượng DiD, biểu thị hiệu quả can thiệp.\n\nGiải thích kết quả:\n\nHệ số \\(β_3\\) thể hiện hiệu quả can thiệp ước tính. Giá trị dương cho biết chính sách có tác động tích cực đến kết quả, giá trị âm cho biết tác động tiêu cực.\n\nKiểm tra ổn định:\n\nTiến hành phân tích độ nhạy để đánh giá độ tin cậy của kết quả. Điều này có thể bao gồm việc thử nghiệm các tiêu chí đánh giá khác nhau của nhóm kiểm soát, khoảng thời gian hoặc hệ số của mô hình.\n\nGiải quyết các vấn đề tiềm ẩn:\n\nGiải quyết mọi vấn đề tiềm ẩn như sai lệch lựa chọn, lỗi đo lường hoặc tính nội sinh có thể ảnh hưởng đến tính hợp lệ của kết quả của bạn.\n\n\n\nTừ công thức hồi quy DiD:\n\\[Y_{it} = β_0 + β_1 * Treatment_i + β_2 * Post_t + β_3 * (Treatment_i * Post_t) + ε_{it}\\]\nTa có thể tách riêng công thức cho nhóm treatment và nhóm control như sau:\n\nCho nhóm treatment (Treatment = 1):\n\n\\[Y_{it}^{Treatment} = (β_0 + β_1) + (β_2 + β_3) * Post_t + ε_{it}^{Treatment}\\]\n\nCho nhóm control (Treatment = 0):\n\n\\[Y_{it}^{Control} = β_0 + β_2 * Post_t + ε_{it}^{Control}\\]\nTrong đó:\n\n\\(Y_{it}^{Treatment}\\) là biến phản ứng (ví dụ: chi tiêu trung bình hàng tháng) cho nhóm treatment tại thời điểm \\(t\\).\n\\(Y_{it}^{Control}\\) là biến phản ứng cho nhóm control tại thời điểm \\(t\\).\n\\(β_0\\) là hệ số chặn (intercept) chung cho cả hai nhóm.\n\\(β_1\\) là hệ số ước lượng cho biến Treatment (nhóm treatment vs. nhóm control).\n\\(β_2\\) là hệ số ước lượng cho biến Post (thời gian sau can thiệp vs. trước can thiệp).\n\\(β_3\\) là hệ số ước lượng cho biến tương tác (Treatment và Post).\n\\(ε_{it}^{Treatment}\\) và \\(ε_{it}^{Control}\\) là thành phần sai số (error term) tương ứng cho từng nhóm.\n\nCông thức này cho phép bạn tính ước lượng cho từng nhóm riêng biệt và thấy cách can thiệp (Treatment) ảnh hưởng đến biến phản ứng (Y) trong từng nhóm.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulated data\nnp.random.seed(0)\n\n# Create a DataFrame with pre- and post-reform data for treatment and control groups\ndata = pd.DataFrame({\n    'Group': ['A', 'A', 'B', 'B'],\n    'Period': ['Before', 'After', 'Before', 'After'],\n    'Credit_Limit': [5000, 5300, 5200, 5100],  # Simulated credit limit data\n    'Spending': [3500, 4100, 4200, 4300],  # Simulated credit card spending\n})\n\n# Calculate the DiD estimate using the parallel trends assumption\ncontrol_group_before_mean = data[(data['Group'] == 'B') & (data['Period'] == 'Before')]['Spending'].mean()\ncontrol_group_after_mean = data[(data['Group'] == 'B') & (data['Period'] == 'After')]['Spending'].mean()\n\ntreatment_group_before_mean = data[(data['Group'] == 'A') & (data['Period'] == 'Before')]['Spending'].mean()\n\ncounterfactual_treatment_group_after = treatment_group_before_mean + (control_group_after_mean - control_group_before_mean)\ncounterfactual_control_group_after = control_group_before_mean  # Counterfactual for control group\n\n# Visualize the results with line graphs\nplt.figure(figsize=(10, 6))\nplt.title('Difference-in-Differences (DiD) Estimate: Impact on Credit Card Spending')\nplt.xlabel('Time Period')\nplt.ylabel('Average Monthly Credit Card Spending')\nplt.grid(True)\n\n# Plot spending for the treatment group before and after the intervention\ntreatment_group = data[data['Group'] == 'A']\nplt.plot(treatment_group['Period'], treatment_group['Spending'], marker='o', label='Treatment Group (A)')\n\n# Plot spending for the control group before and after the intervention\ncontrol_group = data[data['Group'] == 'B']\nplt.plot(control_group['Period'], control_group['Spending'], marker='x', label='Control Group (B)')\n\n# Plot spending for the treatment group assuming no intervention (counterfactual for treatment group)\nplt.plot(treatment_group['Period'], [treatment_group_before_mean, counterfactual_treatment_group_after],\n         linestyle='--', label='Treatment Group (No Intervention)')\n\n# Plot spending for the control group assuming no intervention (counterfactual for control group)\nplt.plot(control_group['Period'], [control_group_before_mean, counterfactual_control_group_after],\n         linestyle='--', label='Control Group (No Intervention)')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# Simulated data\nnp.random.seed(0)\n\n# Create a DataFrame with pre- and post-intervention data for treatment and control groups\nyears = list(range(2010, 2021))\ntreatment_status = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]  # 0 for control, 1 for treatment\ncredit_limit = [5000, 5000, 5000, 5000, 5000, 10000, 10000, 10000, 10000, 10000, 10000]\nspending = [3500, 4000, 4200, 4800, 5500, 8000, 8500, 9000, 9200, 9500, 9800]\n\ndata = pd.DataFrame({\n    'Year': years,\n    'Treatment': treatment_status,\n    'Credit_Limit': credit_limit,\n    'Spending': spending,\n})\n\n# Create an indicator variable for the post-intervention period\ndata['Post'] = (data['Year'] &gt;= 2015).astype(int)\n\n# Create the interaction term 'Treatment*Post'\ndata['Treatment*Post'] = data['Treatment'] * data['Post']\n\n# Run a DiD regression\nX = data[['Treatment', 'Post', 'Treatment*Post', 'Credit_Limit']]\nX = sm.add_constant(X)  # Add a constant term\ny = data['Spending']\n\n\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Spending   R-squared:                       0.927\nModel:                            OLS   Adj. R-squared:                  0.919\nMethod:                 Least Squares   F-statistic:                     113.9\nDate:                Thu, 07 Sep 2023   Prob (F-statistic):           2.08e-06\nTime:                        10:09:13   Log-Likelihood:                -86.750\nNo. Observations:                  11   AIC:                             177.5\nDf Residuals:                       9   BIC:                             178.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst            -49.9999    174.960     -0.286      0.782    -445.788     345.788\nTreatment         50.0000    174.960      0.286      0.782    -345.788     445.788\nPost              50.0000    174.960      0.286      0.782    -345.788     445.788\nTreatment*Post    50.0000    174.960      0.286      0.782    -345.788     445.788\nCredit_Limit       0.8900      0.035     25.434      0.000       0.811       0.969\n==============================================================================\nOmnibus:                        0.470   Durbin-Watson:                   1.375\nProb(Omnibus):                  0.791   Jarque-Bera (JB):                0.496\nSkew:                           0.039   Prob(JB):                        0.780\nKurtosis:                       1.962   Cond. No.                     1.63e+37\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.71e-66. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\nc:\\Users\\binhnn2\\anaconda3\\envs\\rdm\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1736: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=11\n  warnings.warn(\"kurtosistest only valid for n&gt;=20 ... continuing \"\n\n\nTác động của các hệ số:\n\n\\(β_1\\) đo lường tác động của can thiệp (Treatment) lên biến phản ứng (\\(Y_{it}\\)) cho nhóm treatment so với nhóm control trước can thiệp. Nếu \\(β_1\\) có ý nghĩa thống kê, điều này ngụ ý rằng nếu không có bất kỳ can thiệp gì thì nhóm treatment đã khác nhóm control một lượng là \\(β_1\\).\n\\(β_2\\) đo lường tác động của thời gian (Post) lên biến phản ứng (\\(Y_{it}\\)) cho nhóm control. Nếu \\(β_2\\) dương và có ý nghĩa thống kê, nếu không có bất kỳ hành động can thiệp nào thì khoảng thời gian sau năm 2015 vẫn chi tiêu cao hơn trước.\n\\(β_3\\) đo lường hiệu ứng can thiệp (DiD) trên biến phản ứng (\\(Y_{it}\\)). Nếu \\(β_3\\) dương và có ý nghĩa thống kê, điều này ngụ ý rằng can thiệp đã tạo ra sự thay đổi khác biệt giữa nhóm treatment và nhóm control sau can thiệp. Điều này là ước lượng chính của bạn trong mô hình DiD.\n\n\n# Visualize the results\nplt.figure(figsize=(12, 6))\nplt.title('Difference-in-Differences Analysis: Impact on Credit Card Spending')\nplt.axvline(x=2015, color='red', linestyle='--', label='Intervention Year')\n\n# Plot the control group\ncontrol_group = data[data['Treatment'] == 0]\nplt.plot(control_group['Year'], control_group['Spending'], marker='o', label='Control Group')\n\n# Plot the treatment group\ntreatment_group = data[data['Treatment'] == 1]\nplt.plot(treatment_group['Year'], treatment_group['Spending'], marker='x', label='Treatment Group')\n\nplt.xlabel('Year')\nplt.ylabel('Average Monthly Credit Card Spending')\nplt.legend()\nplt.grid(True)\n\nplt.show()",
    "crumbs": [
      "Statistic",
      "Difference-in-Differences (DiD)"
    ]
  },
  {
    "objectID": "statistic/confidenceIntervalWithSmallSampleSize.html",
    "href": "statistic/confidenceIntervalWithSmallSampleSize.html",
    "title": "Ước lượng khoảng tin cậy với cỡ mẫu nhỏ",
    "section": "",
    "text": "Để ước tính khoảng tin cậy cho trung bình của tổng thể với kích thước mẫu nhỏ, bạn cần xem xét cẩn thận do sự không chắc chắn gia tăng khi có dữ liệu hạn chế. Khi kích thước mẫu nhỏ, thường sử dụng phân phối t thay vì phân phối chuẩn để tính đến biến động. Dưới đây là cách bạn có thể ước tính khoảng tin cậy cho trung bình của dân số với kích thước mẫu nhỏ:\n\nThu thập và Tóm tắt Dữ liệu: Thu thập dữ liệu mẫu của bạn và tính trung bình mẫu (\\(\\bar{x}\\)) và độ lệch chuẩn mẫu (\\(s\\)).\nChọn Mức Độ Tin Cậy: Quyết định mức độ tin cậy mong muốn cho khoảng tin cậy, thường là 95% hoặc 99%.\nTính Giá Trị t-Critical: Tìm giá trị t-critical liên quan đến mức độ tin cậy đã chọn và bậc tự do, với bậc tự do cho mẫu kích thước \\(n\\) là \\(n - 1\\).\nTính Toán Sai Số Chuẩn: Tính sai số chuẩn (\\(SE\\)) của trung bình mẫu bằng công thức: \\[ SE = \\frac{s}{\\sqrt{n}} \\]\nTính Toán Sai Số Biên: Tính sai số biên (\\(MOE\\)) bằng công thức: \\[ MOE = t_{\\text{critical}} \\times SE \\]\nTính Khoảng Tin Cậy: Khoảng tin cậy được tính bằng cách trừ và cộng sai số biên từ trung bình mẫu: \\[ \\text{Khoảng Tin Cậy} = \\bar{x} \\pm MOE \\]\n\nDưới đây là ví dụ về cách tính khoảng tin cậy bằng Python:\nLưu ý với kích thước mẫu nhỏ, khoảng tin cậy kết quả có thể rộng, cho thấy mức độ không chắc chắn tương đối cao. Khi kích thước mẫu tăng lên, khoảng tin cậy sẽ trở nên hẹp hơn, cung cấp một ước tính chính xác hơn về trung bình của tổng thể.\n\nimport scipy.stats as stats\nimport numpy as np\n\n# Dữ liệu mẫu\nsample_data = [12, 14, 15, 17, 18, 19, 20]\nn = len(sample_data)\n\n# Tính trung bình mẫu và độ lệch chuẩn mẫu\nsample_mean = np.mean(sample_data)\nsample_std = np.std(sample_data, ddof=1)  # Sửa đổi Bessel cho độ lệch chuẩn mẫu\n\n# Đặt mức độ tin cậy\nconfidence_level = 0.95\n\n# Tính độ tự do\ndegrees_of_freedom = n - 1\n\n# Tính giá trị t-critical\nt_critical = stats.t.ppf(1 - (1 - confidence_level) / 2, df=degrees_of_freedom)\n\n# Tính sai số chuẩn\nSE = sample_std / np.sqrt(n)\n\n# Tính sai số biên\nMOE = t_critical * SE\n\n# Tính khoảng tin cậy\nconfidence_interval = (sample_mean - MOE, sample_mean + MOE)\n\nprint(\"Trung bình mẫu:\", sample_mean)\nprint(\"Khoảng tin cậy:\", confidence_interval)\n\nTrung bình mẫu: 16.428571428571427\nKhoảng tin cậy: (13.766410649936082, 19.09073220720677)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Parameters for the t-distribution\ndegrees_of_freedom = [3, 10, 30]\nx_range = np.linspace(-5, 5, 500)\n\n# Generate data for the normal distribution\nnormal_pdf = stats.norm.pdf(x_range, 0, 1)\n\n# Generate data for t-distributions with different degrees of freedom\nt_distributions = [stats.t.pdf(x_range, df) for df in degrees_of_freedom]\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(x_range, normal_pdf, label='Normal Distribution')\n\nfor i, df in enumerate(degrees_of_freedom):\n    plt.plot(x_range, t_distributions[i], label=f't-Distribution (df={df})')\n\nplt.title('Comparison of Normal and t-Distributions')\nplt.xlabel('x')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nTrong biểu đồ này, bạn sẽ thấy đường cong mật độ xác suất cho phân phối chuẩn tiêu chuẩn và phân phối t-Student với các độ tự do khác nhau (3, 10 và 30). Khi độ tự do tăng lên, phân phối t-Student tiến dần tới phân phối chuẩn. Với các độ tự do nhỏ hơn, phân phối t-Student có đuôi dày và đỉnh phẳnghơn so với phân phối chuẩn. Khi độ tự do tăng lên, phân phối t-Student trở nên giống phân phối chuẩn hơn.",
    "crumbs": [
      "Statistic",
      "Ước lượng khoảng tin cậy với cỡ mẫu nhỏ"
    ]
  },
  {
    "objectID": "machine-learning/Autoencoders.html",
    "href": "machine-learning/Autoencoders.html",
    "title": "Autoencoders - anomaly detection",
    "section": "",
    "text": "Bộ mã hóa tự động (Autoencoders) là một loại kiến trúc mạng thần kinh thường được sử dụng cho nhiều tác vụ khác nhau, bao gồm cả phát hiện sự bất thường. Chúng đặc biệt hiệu quả trong việc phát hiện sự bất thường không có giám sát, trong đó mô hình học cách tái tạo lại dữ liệu thông thường và xác định các điểm bất thường là các điểm dữ liệu sai lệch đáng kể so với phiên bản được xây dựng lại.\nSau đây là cách bộ mã hóa tự động hoạt động để phát hiện sự bất thường:\n\nKiến trúc: Bộ mã hóa tự động bao gồm bộ mã hóa và bộ giải mã (encoder - decoder). Bộ mã hóa nén dữ liệu đầu vào thành biểu diễn có chiều thấp hơn và bộ giải mã cố gắng tái tạo lại dữ liệu đầu vào ban đầu từ biểu diễn này.\nEncoder: Nén đầu vào thành một biểu diễn không gian ẩn. Lớp mã hóa mã hóa hình ảnh đầu vào dưới dạng biểu diễn nén ở kích thước giảm; bây giờ, ảnh nén trông giống ảnh gốc nhưng không giống ảnh gốc.\nDecoder: Bộ giải mã giải mã hình ảnh được mã hóa trở lại hình ảnh gốc có cùng kích thước. Bộ giải mã đưa dữ liệu từ không gian ẩn thấp hơn đến giai đoạn tái tạo, trong đó kích thước của đầu ra được giải mã bằng đầu ra X ban đầu. Nhưng nếu chúng ta xem nó là nén Ảnh thì sẽ có nén không mất dữ liệu, nhưng trong trường hợp Autoencoders, thông tin bị mất do nó nén và giải nén đầu vào. Khi giải nén, nó cố gắng đạt gần đầu vào, nhưng đầu ra không hoàn toàn giống ban đầu.\nKhông gian ẩn (Latent Space): Giảm số chiều của dữ liệu thấp hơn do bộ mã hóa tạo ra (thường được gọi là không gian ẩn hay nén dữ liệu). Ví dụ số chiều dữ liệu ban đầu là 10x10x1 sau khi giảm chiều hoặc nén lại còn 5x1 (kích thước latent space còn 5x1). Bây giờ, mỗi điểm dữ liệu 10x10x1 chỉ được xác định bởi 5 số\nLỗi tái tạo (Reconstruction Error): Do quá trình nén (giảm chiều) rồi tái tạo về số chiều ban đầu làm cho thông tin đầu vào và đầu ra bị khác nhau. Sai khác này được gọi là Lỗi tái tạo (Reconstruction Error). Các điểm bất thường được phát hiện dựa trên lỗi tái tạo của chúng. Với những điểm bất thường, lỗi tái tạo thường cao hơn nhiều so với dữ liệu thông thường.\nĐào tạo: Bộ mã hóa tự động thường được đào tạo trên tập dữ liệu chỉ chứa dữ liệu thông thường. Mục tiêu là giảm thiểu lỗi tái tạo, nghĩa là bộ mã hóa tự động sẽ có thể tái tạo lại dữ liệu bình thường một cách chính xác trong khi gặp khó khăn trong việc tái tạo lại các điểm bất thường.\nNgưỡng: Việc phát hiện bất thường bằng bộ mã hóa tự động thường liên quan đến việc đặt ngưỡng cho lỗi tái tạo. Điểm dữ liệu có lỗi tái tạo vượt quá ngưỡng được coi là bất thường.\n\nBộ mã hóa tự động có một số lợi thế để phát hiện sự bất thường:\n\nChúng có thể nắm bắt các mẫu phức tạp trong dữ liệu, khiến chúng phù hợp với dữ liệu nhiều chiều.\nChúng không được giám sát nên không yêu cầu dữ liệu bất thường được dán nhãn để huấn luyện.\nChúng có khả năng thích ứng với các loại dữ liệu khác nhau, bao gồm dữ liệu số và hình ảnh.",
    "crumbs": [
      "Machine learning",
      "Autoencoders - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/Autoencoders.html#autoencoders",
    "href": "machine-learning/Autoencoders.html#autoencoders",
    "title": "Autoencoders - anomaly detection",
    "section": "",
    "text": "Bộ mã hóa tự động (Autoencoders) là một loại kiến trúc mạng thần kinh thường được sử dụng cho nhiều tác vụ khác nhau, bao gồm cả phát hiện sự bất thường. Chúng đặc biệt hiệu quả trong việc phát hiện sự bất thường không có giám sát, trong đó mô hình học cách tái tạo lại dữ liệu thông thường và xác định các điểm bất thường là các điểm dữ liệu sai lệch đáng kể so với phiên bản được xây dựng lại.\nSau đây là cách bộ mã hóa tự động hoạt động để phát hiện sự bất thường:\n\nKiến trúc: Bộ mã hóa tự động bao gồm bộ mã hóa và bộ giải mã (encoder - decoder). Bộ mã hóa nén dữ liệu đầu vào thành biểu diễn có chiều thấp hơn và bộ giải mã cố gắng tái tạo lại dữ liệu đầu vào ban đầu từ biểu diễn này.\nEncoder: Nén đầu vào thành một biểu diễn không gian ẩn. Lớp mã hóa mã hóa hình ảnh đầu vào dưới dạng biểu diễn nén ở kích thước giảm; bây giờ, ảnh nén trông giống ảnh gốc nhưng không giống ảnh gốc.\nDecoder: Bộ giải mã giải mã hình ảnh được mã hóa trở lại hình ảnh gốc có cùng kích thước. Bộ giải mã đưa dữ liệu từ không gian ẩn thấp hơn đến giai đoạn tái tạo, trong đó kích thước của đầu ra được giải mã bằng đầu ra X ban đầu. Nhưng nếu chúng ta xem nó là nén Ảnh thì sẽ có nén không mất dữ liệu, nhưng trong trường hợp Autoencoders, thông tin bị mất do nó nén và giải nén đầu vào. Khi giải nén, nó cố gắng đạt gần đầu vào, nhưng đầu ra không hoàn toàn giống ban đầu.\nKhông gian ẩn (Latent Space): Giảm số chiều của dữ liệu thấp hơn do bộ mã hóa tạo ra (thường được gọi là không gian ẩn hay nén dữ liệu). Ví dụ số chiều dữ liệu ban đầu là 10x10x1 sau khi giảm chiều hoặc nén lại còn 5x1 (kích thước latent space còn 5x1). Bây giờ, mỗi điểm dữ liệu 10x10x1 chỉ được xác định bởi 5 số\nLỗi tái tạo (Reconstruction Error): Do quá trình nén (giảm chiều) rồi tái tạo về số chiều ban đầu làm cho thông tin đầu vào và đầu ra bị khác nhau. Sai khác này được gọi là Lỗi tái tạo (Reconstruction Error). Các điểm bất thường được phát hiện dựa trên lỗi tái tạo của chúng. Với những điểm bất thường, lỗi tái tạo thường cao hơn nhiều so với dữ liệu thông thường.\nĐào tạo: Bộ mã hóa tự động thường được đào tạo trên tập dữ liệu chỉ chứa dữ liệu thông thường. Mục tiêu là giảm thiểu lỗi tái tạo, nghĩa là bộ mã hóa tự động sẽ có thể tái tạo lại dữ liệu bình thường một cách chính xác trong khi gặp khó khăn trong việc tái tạo lại các điểm bất thường.\nNgưỡng: Việc phát hiện bất thường bằng bộ mã hóa tự động thường liên quan đến việc đặt ngưỡng cho lỗi tái tạo. Điểm dữ liệu có lỗi tái tạo vượt quá ngưỡng được coi là bất thường.\n\nBộ mã hóa tự động có một số lợi thế để phát hiện sự bất thường:\n\nChúng có thể nắm bắt các mẫu phức tạp trong dữ liệu, khiến chúng phù hợp với dữ liệu nhiều chiều.\nChúng không được giám sát nên không yêu cầu dữ liệu bất thường được dán nhãn để huấn luyện.\nChúng có khả năng thích ứng với các loại dữ liệu khác nhau, bao gồm dữ liệu số và hình ảnh.",
    "crumbs": [
      "Machine learning",
      "Autoencoders - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/Autoencoders.html#so-sánh-autoencoders-với-pca-phân-tích-thành-phần-chính-và-isolation-forest-trong-phát-hiện-bất-thường",
    "href": "machine-learning/Autoencoders.html#so-sánh-autoencoders-với-pca-phân-tích-thành-phần-chính-và-isolation-forest-trong-phát-hiện-bất-thường",
    "title": "Autoencoders - anomaly detection",
    "section": "So sánh Autoencoders với PCA (Phân tích thành phần chính) và Isolation Forest trong phát hiện bất thường:",
    "text": "So sánh Autoencoders với PCA (Phân tích thành phần chính) và Isolation Forest trong phát hiện bất thường:\n\nAutoencoders:\n\nThuận lợi:\n\nCó thể nắm bắt các mẫu phức tạp, phi tuyến trong dữ liệu.\nThích hợp cho dữ liệu nhiều chiều.\nCó thể tinh chỉnh cho các nhiệm vụ cụ thể.\n\nNhược điểm:\n\nYêu cầu nhiều dữ liệu và thời gian huấn luyện hơn so với PCA.\nCó thể cần phải điều chỉnh siêu tham số.\nCó thể bị overfit nếu không được điều chỉnh đúng cách.\n\n\nPCA:\n\nThuận lợi:\n\nĐơn giản và hiệu quả tính toán.\nHữu ích cho việc giảm kích thước và trực quan hóa.\nCung cấp các tính năng trực giao, có thể nắm bắt một số mẫu.\n\nNhược điểm:\n\nPhương pháp tuyến tính; có thể không nắm bắt được các mẫu phức tạp, phi tuyến.\nGiả sử dữ liệu được phân phối Gaussian.\nKhông có tiêu chí cụ thể về dị thường; dựa vào các thuộc tính thống kê.\n\n\nIsolation Forest:\n\nThuận lợi:\n\nĐược thiết kế riêng cho mục đích phát hiện sự bất thường.\nHoạt động tốt với cả dữ liệu có nhiều chiều và ít chiều.\nCó thể xử lý các kiểu dữ liệu hỗn hợp (số và phân loại).\n\nNhược điểm:\n\nDễ bị overfitting trên các tập dữ liệu nhỏ.\nCó thể yêu cầu tinh chỉnh các siêu tham số.\nKhông giải thích được tại sao trường hợp cụ thể lại bất thường\n\n\n\nCác yếu tố so sánh cần xem xét khi lựa chọn phương pháp cho nhiệm vụ phát hiện dấu vết bất thường của bạn:\n\nTính phức tạp của Dữ liệu: Autoencoders phù hợp cho dữ liệu phức tạp, phi tuyến tính, trong khi PCA và Isolation Forest thích hợp cho dữ liệu đơn giản, tuyến tính hơn.\nSố chiều: Nếu làm việc với dữ liệu có số chiều cao, autoencoders và PCA có thể hữu ích hơn. PCA đặc biệt tốt cho việc giảm chiều dữ liệu.\nKhả năng diễn giải: PCA cung cấp biến đổi đặc trưng và điểm số thành phần có thể diễn giải. Autoencoders và Isolation Forest không cung cấp điều này theo cách tự nhiên.\nTính toán: PCA hiệu quả tính toán nhất, theo sau là Isolation Forest. Autoencoders yêu cầu nhiều tài nguyên tính toán hơn cho quá trình đào tạo.\nKích thước Dữ liệu: Isolation Forest có thể gây overfitting trên tập dữ liệu nhỏ hơn. Autoencoders và PCA có thể mạnh mẽ hơn đối với tập dữ liệu nhỏ.\nLoại đặc trưng: Nếu dữ liệu của bạn bao gồm cả số và phân loại, Isolation Forest phù hợp hơn vì nó có thể xử lý cả hai loại dữ liệu. Autoencoders và PCA thường được sử dụng cho dữ liệu số.\nSự có sẵn của Dữ liệu Đào tạo: Autoencoders yêu cầu nhiều dữ liệu đào tạo hơn so với Isolation Forest và PCA, có thể xử lý các tập dữ liệu nhỏ hơn.\nTinh chỉnh siêu tham số: Tất cả các phương pháp có thể yêu cầu một mức độ tinh chỉnh siêu tham số, nhưng autoencoders có thể yêu cầu nhiều hơn.\nMục đích Sử dụng: Xem xét yêu cầu cụ thể và tính chất của vấn đề phát hiện dấu vết bất thường của bạn. Mỗi phương pháp có thể hoạt động tố\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\n# Load the German Credit Data (you can download it from the source)\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\n\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Convert categorical columns to one-hot encoding\ncategorical_columns = ['existing_checking', 'credit_history', 'purpose', 'savings', 'employment',\n                       'personal_status', 'other_debtors', 'property', 'other_installment_plans',\n                       'housing', 'job', 'telephone', 'foreign_worker']\n\ndata = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n\n# Split the data into features and labels\nX = data.drop('class', axis=1).values\ny = data['class'].values\n\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Define and train the autoencoder model\ninput_dim = X.shape[1]\nencoding_dim = 10  # Number of neurons in the encoding layer\n\ninput_layer = Input(shape=(input_dim,))\nencoded = Dense(encoding_dim, activation='relu')(input_layer)\ndecoded = Dense(input_dim, activation='linear')(encoded)\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')\n\nautoencoder.fit(X, X, epochs=50, batch_size=64, shuffle=True)\n\n# Evaluate the autoencoder model on the entire dataset\nreconstruction_errors = np.mean(np.square(X - autoencoder.predict(X)), axis=1)\n\n# Set a threshold for anomalies\nthreshold = np.percentile(reconstruction_errors, 95)  # Example threshold (adjust as needed)\n\n# Detect anomalies for the entire dataset\nanomalies_detected = reconstruction_errors &gt; threshold\n\n# Print the number of anomalies detected\nprint(f'Anomalies detected: {np.sum(anomalies_detected)} out of {len(X)}')\n\nEpoch 1/50\n16/16 [==============================] - 0s 2ms/step - loss: 1.1801\nEpoch 2/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.1139\nEpoch 3/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.0666\nEpoch 4/50\n16/16 [==============================] - 0s 2ms/step - loss: 1.0310\nEpoch 5/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.0032\nEpoch 6/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9797\nEpoch 7/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9594\nEpoch 8/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9411\nEpoch 9/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9241\nEpoch 10/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9077\nEpoch 11/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8921\nEpoch 12/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8774\nEpoch 13/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8631\nEpoch 14/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.8497\nEpoch 15/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.8370\nEpoch 16/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8249\nEpoch 17/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8138\nEpoch 18/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8033\nEpoch 19/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7934\nEpoch 20/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7843\nEpoch 21/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.7759\nEpoch 22/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7681\nEpoch 23/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7609\nEpoch 24/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7544\nEpoch 25/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7483\nEpoch 26/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7425\nEpoch 27/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7373\nEpoch 28/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7322\nEpoch 29/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7277\nEpoch 30/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7234\nEpoch 31/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7193\nEpoch 32/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7154\nEpoch 33/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7118\nEpoch 34/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7082\nEpoch 35/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7049\nEpoch 36/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7017\nEpoch 37/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6988\nEpoch 38/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6958\nEpoch 39/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6931\nEpoch 40/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6904\nEpoch 41/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6878\nEpoch 42/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6853\nEpoch 43/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6828\nEpoch 44/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6804\nEpoch 45/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6781\nEpoch 46/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6758\nEpoch 47/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6737\nEpoch 48/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.6716\nEpoch 49/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.6697\nEpoch 50/50\n16/16 [==============================] - 0s 10ms/step - loss: 0.6675\n32/32 [==============================] - 0s 936us/step\nAnomalies detected: 50 out of 1000\n\n\n\n# Select the rows of data that are anomalies\nanomalous_data = data[anomalies_detected]\nanomalous_data\n\n\n\n\n\n\n\n\nduration\ncredit_amount\ninstallment_rate\nresidence_since\nage\nexisting_credits\npeople_liable\nclass\nexisting_checking_A12\nexisting_checking_A13\n...\nproperty_A124\nother_installment_plans_A142\nother_installment_plans_A143\nhousing_A152\nhousing_A153\njob_A172\njob_A173\njob_A174\ntelephone_A192\nforeign_worker_A202\n\n\n\n\n38\n10\n1225\n2\n2\n37\n1\n1\n1\n0\n1\n...\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n42\n18\n6204\n2\n4\n44\n1\n2\n1\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n65\n27\n5190\n4\n4\n48\n4\n2\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n72\n8\n1164\n3\n4\n51\n2\n2\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n83\n24\n1755\n4\n4\n58\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n105\n24\n11938\n2\n3\n39\n2\n2\n2\n1\n0\n...\n0\n0\n1\n1\n0\n0\n0\n1\n1\n0\n\n\n156\n9\n1288\n3\n4\n48\n2\n2\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n\n\n157\n12\n339\n4\n1\n45\n1\n1\n1\n0\n0\n...\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n162\n15\n1262\n4\n3\n36\n2\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n203\n12\n902\n4\n4\n21\n1\n1\n2\n0\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n207\n12\n1424\n4\n3\n26\n1\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n209\n12\n1413\n3\n2\n55\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n\n\n215\n6\n932\n1\n3\n39\n2\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n\n\n226\n48\n10961\n1\n2\n27\n2\n1\n2\n1\n0\n...\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n244\n12\n3447\n4\n3\n35\n1\n2\n1\n0\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n\n\n274\n30\n11998\n1\n1\n34\n1\n1\n2\n0\n0\n...\n1\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n287\n48\n7582\n2\n4\n31\n1\n1\n1\n1\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n310\n48\n5381\n3\n4\n40\n1\n1\n1\n1\n0\n...\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n337\n15\n1275\n4\n2\n24\n1\n1\n2\n0\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n350\n9\n1236\n1\n4\n23\n1\n1\n1\n0\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n\n\n374\n60\n14782\n3\n4\n60\n2\n1\n2\n1\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n429\n18\n1190\n2\n4\n55\n3\n2\n2\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n431\n24\n11328\n2\n3\n29\n2\n1\n2\n1\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n436\n6\n660\n2\n4\n23\n1\n1\n1\n0\n0\n...\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n438\n42\n3394\n4\n4\n65\n2\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n442\n20\n2629\n2\n3\n29\n2\n1\n1\n1\n0\n...\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n449\n15\n1512\n3\n3\n61\n2\n1\n2\n1\n0\n...\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n458\n6\n343\n4\n1\n27\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n463\n12\n754\n4\n4\n38\n2\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n473\n6\n1238\n4\n4\n36\n1\n2\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n0\n1\n1\n0\n\n\n579\n24\n937\n4\n3\n27\n2\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n\n\n583\n36\n2384\n4\n1\n33\n1\n1\n2\n1\n0\n...\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n588\n18\n1217\n4\n3\n47\n1\n1\n2\n0\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n594\n24\n1358\n4\n3\n40\n1\n1\n2\n0\n0\n...\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n610\n12\n741\n4\n3\n22\n1\n1\n2\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n613\n24\n3632\n1\n4\n22\n1\n1\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n665\n24\n6314\n4\n2\n27\n2\n1\n1\n0\n0\n...\n1\n0\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n666\n30\n3496\n4\n2\n34\n1\n2\n1\n1\n0\n...\n0\n1\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n721\n6\n433\n4\n2\n24\n1\n2\n2\n1\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n735\n36\n3990\n3\n2\n29\n1\n1\n1\n1\n0\n...\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n754\n12\n1555\n4\n4\n55\n2\n2\n2\n0\n0\n...\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n\n\n782\n12\n1410\n2\n2\n31\n1\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n813\n48\n3051\n3\n4\n54\n1\n1\n2\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n816\n6\n1338\n1\n4\n62\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n818\n36\n15857\n2\n3\n43\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n856\n10\n894\n4\n3\n40\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n873\n15\n874\n4\n1\n24\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n915\n48\n18424\n1\n2\n32\n1\n1\n2\n1\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n\n\n964\n6\n454\n3\n1\n22\n1\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n\n\n970\n15\n1514\n4\n2\n22\n1\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n50 rows × 49 columns\n\n\n\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define and train the autoencoder model\ninput_dim = X_train.shape[1]\nencoding_dim = 10  # Number of neurons in the encoding layer\n\ninput_layer = Input(shape=(input_dim,))\nencoded = Dense(encoding_dim, activation='relu')(input_layer)\ndecoded = Dense(input_dim, activation='linear')(encoded)\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')\n\nautoencoder.fit(X_train, X_train, epochs=50, batch_size=64, shuffle=True, validation_data=(X_test, X_test))\n\n# Evaluate the autoencoder model on the test set\nreconstruction_errors = np.mean(np.square(X_test - autoencoder.predict(X_test)), axis=1)\n\n# Set a threshold for anomalies\nthreshold = np.percentile(reconstruction_errors, 95)  # Example threshold (adjust as needed)\n\n# Detect anomalies\nanomalies_detected = reconstruction_errors &gt; threshold\n\n# Print the number of anomalies detected\nprint(f'Anomalies detected: {np.sum(anomalies_detected)} out of {len(X_test)}')\n\nEpoch 1/50\n13/13 [==============================] - 0s 11ms/step - loss: 1.2654 - val_loss: 1.1622\nEpoch 2/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.1879 - val_loss: 1.1093\nEpoch 3/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.1322 - val_loss: 1.0692\nEpoch 4/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.0886 - val_loss: 1.0381\nEpoch 5/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.0551 - val_loss: 1.0139\nEpoch 6/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.0286 - val_loss: 0.9934\nEpoch 7/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.0062 - val_loss: 0.9760\nEpoch 8/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.9867 - val_loss: 0.9606\nEpoch 9/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.9696 - val_loss: 0.9468\nEpoch 10/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.9543 - val_loss: 0.9342\nEpoch 11/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.9399 - val_loss: 0.9223\nEpoch 12/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.9263 - val_loss: 0.9112\nEpoch 13/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.9133 - val_loss: 0.9005\nEpoch 14/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.9009 - val_loss: 0.8902\nEpoch 15/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8892 - val_loss: 0.8801\nEpoch 16/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.8779 - val_loss: 0.8707\nEpoch 17/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.8670 - val_loss: 0.8614\nEpoch 18/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.8566 - val_loss: 0.8528\nEpoch 19/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.8468 - val_loss: 0.8442\nEpoch 20/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8375 - val_loss: 0.8360\nEpoch 21/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8283 - val_loss: 0.8283\nEpoch 22/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8197 - val_loss: 0.8211\nEpoch 23/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8116 - val_loss: 0.8140\nEpoch 24/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8040 - val_loss: 0.8074\nEpoch 25/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7966 - val_loss: 0.8010\nEpoch 26/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7898 - val_loss: 0.7953\nEpoch 27/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7833 - val_loss: 0.7896\nEpoch 28/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7771 - val_loss: 0.7845\nEpoch 29/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7714 - val_loss: 0.7796\nEpoch 30/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7658 - val_loss: 0.7751\nEpoch 31/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7608 - val_loss: 0.7706\nEpoch 32/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7560 - val_loss: 0.7665\nEpoch 33/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7515 - val_loss: 0.7628\nEpoch 34/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7473 - val_loss: 0.7590\nEpoch 35/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7431 - val_loss: 0.7558\nEpoch 36/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7393 - val_loss: 0.7526\nEpoch 37/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7356 - val_loss: 0.7495\nEpoch 38/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7322 - val_loss: 0.7468\nEpoch 39/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7289 - val_loss: 0.7437\nEpoch 40/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7259 - val_loss: 0.7413\nEpoch 41/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7228 - val_loss: 0.7385\nEpoch 42/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7201 - val_loss: 0.7362\nEpoch 43/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7174 - val_loss: 0.7340\nEpoch 44/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7149 - val_loss: 0.7318\nEpoch 45/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.7123 - val_loss: 0.7297\nEpoch 46/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7100 - val_loss: 0.7275\nEpoch 47/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7078 - val_loss: 0.7256\nEpoch 48/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7056 - val_loss: 0.7235\nEpoch 49/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7035 - val_loss: 0.7215\nEpoch 50/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7197\n7/7 [==============================] - 0s 1ms/step\nAnomalies detected: 10 out of 200\n\n\n\n# Split the data into features and labels\nX = data.drop('class', axis=1).values\ny = data['class'].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Define and train the autoencoder model\ninput_dim = X.shape[1]\nencoding_dim = 10  # Number of neurons in the encoding layer\n\ninput_layer = Input(shape=(input_dim,))\nencoded = Dense(encoding_dim, activation='relu')(input_layer)\ndecoded = Dense(input_dim, activation='linear')(encoded)\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')\n\nautoencoder.fit(X, X, epochs=50, batch_size=64, shuffle=True)\n\n# Evaluate the autoencoder model on the entire dataset\nreconstruction_errors = np.mean(np.square(X - autoencoder.predict(X)), axis=1)\n\n# Set a threshold for anomalies\nthreshold = np.percentile(reconstruction_errors, 95)  # Example threshold (adjust as needed)\n\n# Detect anomalies for the entire dataset\nanomalies_detected = reconstruction_errors &gt; threshold\n\n# Print the number of anomalies detected\nprint(f'Anomalies detected: {np.sum(anomalies_detected)} out of {len(X)}')\n\nEpoch 1/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.2795\nEpoch 2/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.1882\nEpoch 3/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.1245\nEpoch 4/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.0789\nEpoch 5/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.0441\nEpoch 6/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.0170\nEpoch 7/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9944\nEpoch 8/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9754\nEpoch 9/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9588\nEpoch 10/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9433\nEpoch 11/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9291\nEpoch 12/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9158\nEpoch 13/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9028\nEpoch 14/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8905\nEpoch 15/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.8784\nEpoch 16/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8665\nEpoch 17/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8548\nEpoch 18/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8433\nEpoch 19/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8321\nEpoch 20/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8211\nEpoch 21/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.8105\nEpoch 22/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.8005\nEpoch 23/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7909\nEpoch 24/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7817\nEpoch 25/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7733\nEpoch 26/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7654\nEpoch 27/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7583\nEpoch 28/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7516\nEpoch 29/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7455\nEpoch 30/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.7400\nEpoch 31/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7346\nEpoch 32/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7297\nEpoch 33/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7253\nEpoch 34/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.7211\nEpoch 35/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7170\nEpoch 36/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7133\nEpoch 37/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7099\nEpoch 38/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7067\nEpoch 39/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7036\nEpoch 40/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7006\nEpoch 41/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6979\nEpoch 42/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6952\nEpoch 43/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6927\nEpoch 44/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6903\nEpoch 45/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6881\nEpoch 46/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6860\nEpoch 47/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6839\nEpoch 48/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.6819\nEpoch 49/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.6800\nEpoch 50/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6782\n32/32 [==============================] - 0s 935us/step\nAnomalies detected: 50 out of 1000",
    "crumbs": [
      "Machine learning",
      "Autoencoders - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/IsolationForest.html",
    "href": "machine-learning/IsolationForest.html",
    "title": "Isolation Forest - anomaly detection",
    "section": "",
    "text": "Bước 1: Chuẩn bị Dữ liệu - Bắt đầu bằng việc thu thập và tải dữ liệu mà bạn muốn phát hiện các dấu hiệu bất thường.\nBước 2: Tải và Import Thư viện - Trong Python, sử dụng các thư viện sau: numpy, pandas, và sklearn.ensemble.\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nBước 3: Chuẩn bị dữ liệu - Đảm bảo rằng dữ liệu của bạn đã được chuẩn bị và lấy ra từ các cột không mong muốn.\nBước 4: Tạo Mô hình Isolation Forest - Sử dụng IsolationForest để tạo một mô hình phát hiện dấu hiệu bất thường. Bạn có thể tuỳ chỉnh mô hình bằng cách thay đổi các tham số, chẳng hạn như contamination (tỷ lệ dấu hiệu bất thường trong dữ liệu).\nmodel = IsolationForest(contamination=0.05)  # Tuỳ chỉnh contamination tại đây\nBước 5: Huấn luyện Mô hình - Sử dụng dữ liệu huấn luyện của bạn để huấn luyện mô hình Isolation Forest.\nmodel.fit(X)\nBước 6: Dự đoán Dấu hiệu Bất thường - Sử dụng mô hình đã huấn luyện để dự đoán dấu hiệu bất thường trên tập dữ liệu kiểm tra.\npredictions = model.predict(X)\nBước 7: Lấy Điểm Anomaly - Sử dụng phương thức decision_function để lấy điểm anomaly cho mỗi mẫu dữ liệu. Điểm càng thấp càng có khả năng là dấu hiệu bất thường.\nanomaly_scores = model.decision_function(X)\nBước 8: Xác định Dấu hiệu Bất thường - Dùng ngưỡng (threshold) để xác định dấu hiệu bất thường dựa trên điểm anomaly. Bạn có thể tuỳ chỉnh ngưỡng theo ý muốn.\nthreshold = -0.1  # Ngưỡng ngầm định, điều chỉnh tùy theo nhu cầu\nanomalies = X[anomaly_scores &lt; threshold]\nBước 9: Xuất kết quả - Xuất các mẫu bất thường mà bạn đã xác định từ dữ liệu.\nprint(\"Dấu hiệu bất thường:\")\nprint(anomalies)\nLưu ý rằng bạn có thể tùy chỉnh các tham số và ngưỡng để điều chỉnh độ nhạy của việc phát hiện dấu hiệu bất thường dựa trên nhu cầu của bạn.\n\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\n\n\n# Create an Isolation Forest model\ncontamination = 0.05\nmodel = IsolationForest(contamination=contamination)\n\n# Fit the model to your data\nmodel.fit(X)\n\n# Predict anomalies (1 for inliers, -1 for outliers)\npredictions = model.predict(X)\n\n# Get the anomaly scores (the lower the score, the more likely it's an anomaly)\nanomaly_scores = model.decision_function(X)\n\n# Identify anomalies\nanomalies = data[predictions == -1]\n\nc:\\Users\\binhnn2\\anaconda3\\envs\\env_pycaret\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n  warnings.warn(\n\n\n\nanomalies\n\n\n\n\n\n\n\n\nexisting_checking\nduration\ncredit_history\npurpose\ncredit_amount\nsavings\nemployment\ninstallment_rate\npersonal_status\nother_debtors\n...\nproperty\nage\nother_installment_plans\nhousing\nexisting_credits\njob\npeople_liable\ntelephone\nforeign_worker\nclass\n\n\n\n\n44\nA11\n48\nA34\nA41\n6143\nA61\nA75\n4\nA92\nA101\n...\nA124\n58\nA142\nA153\n2\nA172\n1\nA191\nA201\n2\n\n\n55\nA14\n6\nA31\nA40\n783\nA65\nA73\n1\nA93\nA103\n...\nA121\n26\nA142\nA152\n1\nA172\n2\nA191\nA201\n1\n\n\n87\nA12\n36\nA32\nA46\n12612\nA62\nA73\n1\nA93\nA101\n...\nA124\n47\nA143\nA153\n1\nA173\n2\nA192\nA201\n2\n\n\n99\nA12\n20\nA33\nA41\n7057\nA65\nA74\n3\nA93\nA101\n...\nA122\n36\nA141\nA151\n2\nA174\n2\nA192\nA201\n1\n\n\n105\nA12\n24\nA34\nA410\n11938\nA61\nA73\n2\nA93\nA102\n...\nA123\n39\nA143\nA152\n2\nA174\n2\nA192\nA201\n2\n\n\n110\nA12\n6\nA33\nA49\n1449\nA62\nA75\n1\nA91\nA101\n...\nA123\n31\nA141\nA152\n2\nA173\n2\nA191\nA201\n1\n\n\n154\nA12\n24\nA33\nA49\n6967\nA62\nA74\n4\nA93\nA101\n...\nA123\n36\nA143\nA151\n1\nA174\n1\nA192\nA201\n1\n\n\n157\nA11\n12\nA31\nA48\n339\nA61\nA75\n4\nA94\nA101\n...\nA123\n45\nA141\nA152\n1\nA172\n1\nA191\nA201\n1\n\n\n175\nA14\n30\nA31\nA41\n7485\nA65\nA71\n4\nA92\nA101\n...\nA121\n53\nA141\nA152\n1\nA174\n1\nA192\nA201\n2\n\n\n181\nA12\n36\nA33\nA49\n4455\nA61\nA73\n2\nA91\nA101\n...\nA121\n30\nA142\nA152\n2\nA174\n1\nA192\nA201\n2\n\n\n186\nA12\n9\nA31\nA41\n5129\nA61\nA75\n2\nA92\nA101\n...\nA124\n74\nA141\nA153\n1\nA174\n2\nA192\nA201\n2\n\n\n191\nA12\n48\nA30\nA49\n3844\nA62\nA74\n4\nA93\nA101\n...\nA124\n34\nA143\nA153\n1\nA172\n2\nA191\nA201\n2\n\n\n226\nA12\n48\nA32\nA43\n10961\nA64\nA74\n1\nA93\nA102\n...\nA124\n27\nA141\nA152\n2\nA173\n1\nA192\nA201\n2\n\n\n263\nA14\n12\nA34\nA46\n2748\nA61\nA75\n2\nA92\nA101\n...\nA124\n57\nA141\nA153\n3\nA172\n1\nA191\nA201\n1\n\n\n272\nA12\n48\nA31\nA40\n12169\nA65\nA71\n4\nA93\nA102\n...\nA124\n36\nA143\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n287\nA12\n48\nA33\nA410\n7582\nA62\nA71\n2\nA93\nA101\n...\nA124\n31\nA143\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n304\nA14\n48\nA34\nA40\n10127\nA63\nA73\n2\nA93\nA101\n...\nA124\n44\nA141\nA153\n1\nA173\n1\nA191\nA201\n2\n\n\n306\nA14\n30\nA32\nA41\n4811\nA65\nA74\n2\nA92\nA101\n...\nA122\n24\nA142\nA151\n1\nA172\n1\nA191\nA201\n1\n\n\n333\nA14\n48\nA34\nA41\n11590\nA62\nA73\n2\nA92\nA101\n...\nA123\n24\nA141\nA151\n2\nA172\n1\nA191\nA201\n2\n\n\n343\nA12\n18\nA32\nA49\n4439\nA61\nA75\n1\nA93\nA102\n...\nA121\n33\nA141\nA152\n1\nA174\n1\nA192\nA201\n1\n\n\n373\nA14\n60\nA34\nA40\n13756\nA65\nA75\n2\nA93\nA101\n...\nA124\n63\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n374\nA12\n60\nA31\nA410\n14782\nA62\nA75\n3\nA92\nA101\n...\nA124\n60\nA141\nA153\n2\nA174\n1\nA192\nA201\n2\n\n\n395\nA12\n39\nA33\nA46\n11760\nA62\nA74\n2\nA93\nA101\n...\nA124\n32\nA143\nA151\n1\nA173\n1\nA192\nA201\n1\n\n\n449\nA12\n15\nA33\nA45\n1512\nA64\nA73\n3\nA94\nA101\n...\nA122\n61\nA142\nA152\n2\nA173\n1\nA191\nA201\n2\n\n\n496\nA12\n36\nA32\nA42\n9034\nA62\nA72\n4\nA93\nA102\n...\nA124\n29\nA143\nA151\n1\nA174\n1\nA192\nA201\n2\n\n\n539\nA13\n18\nA32\nA42\n3049\nA61\nA72\n1\nA92\nA101\n...\nA122\n45\nA142\nA152\n1\nA172\n1\nA191\nA201\n1\n\n\n549\nA14\n48\nA34\nA41\n8858\nA65\nA74\n2\nA93\nA101\n...\nA124\n35\nA143\nA153\n2\nA173\n1\nA192\nA201\n1\n\n\n551\nA14\n6\nA31\nA43\n1750\nA63\nA75\n2\nA93\nA101\n...\nA122\n45\nA141\nA152\n1\nA172\n2\nA191\nA201\n1\n\n\n595\nA12\n6\nA31\nA40\n931\nA62\nA72\n1\nA92\nA101\n...\nA122\n32\nA142\nA152\n1\nA172\n1\nA191\nA201\n2\n\n\n602\nA12\n24\nA31\nA46\n1837\nA61\nA74\n4\nA92\nA101\n...\nA124\n34\nA141\nA153\n1\nA172\n1\nA191\nA201\n2\n\n\n607\nA12\n36\nA32\nA43\n2671\nA62\nA73\n4\nA92\nA102\n...\nA124\n50\nA143\nA153\n1\nA173\n1\nA191\nA201\n2\n\n\n611\nA13\n10\nA32\nA40\n1240\nA62\nA75\n1\nA92\nA101\n...\nA124\n48\nA143\nA153\n1\nA172\n2\nA191\nA201\n2\n\n\n613\nA11\n24\nA31\nA41\n3632\nA61\nA73\n1\nA92\nA103\n...\nA123\n22\nA141\nA151\n1\nA173\n1\nA191\nA202\n1\n\n\n616\nA12\n60\nA33\nA43\n9157\nA65\nA73\n2\nA93\nA101\n...\nA124\n27\nA143\nA153\n1\nA174\n1\nA191\nA201\n1\n\n\n663\nA12\n6\nA33\nA42\n1050\nA61\nA71\n4\nA93\nA101\n...\nA122\n35\nA142\nA152\n2\nA174\n1\nA192\nA201\n1\n\n\n666\nA12\n30\nA31\nA42\n3496\nA64\nA73\n4\nA93\nA101\n...\nA123\n34\nA142\nA152\n1\nA173\n2\nA192\nA201\n1\n\n\n667\nA14\n48\nA31\nA49\n3609\nA61\nA73\n1\nA92\nA101\n...\nA121\n27\nA142\nA152\n1\nA173\n1\nA191\nA201\n1\n\n\n684\nA12\n36\nA33\nA49\n9857\nA62\nA74\n1\nA93\nA101\n...\nA122\n31\nA143\nA152\n2\nA172\n2\nA192\nA201\n1\n\n\n703\nA12\n30\nA33\nA49\n2503\nA62\nA75\n4\nA93\nA101\n...\nA122\n41\nA142\nA152\n2\nA173\n1\nA191\nA201\n1\n\n\n721\nA12\n6\nA31\nA46\n433\nA64\nA72\n4\nA92\nA101\n...\nA122\n24\nA141\nA151\n1\nA173\n2\nA191\nA201\n2\n\n\n754\nA14\n12\nA33\nA45\n1555\nA64\nA75\n4\nA93\nA101\n...\nA124\n55\nA143\nA153\n2\nA173\n2\nA191\nA201\n2\n\n\n774\nA13\n12\nA34\nA40\n1480\nA63\nA71\n2\nA93\nA101\n...\nA124\n66\nA141\nA153\n3\nA171\n1\nA191\nA201\n1\n\n\n808\nA12\n42\nA31\nA41\n9283\nA61\nA71\n1\nA93\nA101\n...\nA124\n55\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n829\nA12\n48\nA33\nA49\n6681\nA65\nA73\n4\nA93\nA101\n...\nA124\n38\nA143\nA153\n1\nA173\n2\nA192\nA201\n1\n\n\n876\nA11\n18\nA31\nA43\n1940\nA61\nA72\n3\nA93\nA102\n...\nA124\n36\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n889\nA14\n28\nA31\nA41\n7824\nA65\nA72\n3\nA93\nA103\n...\nA121\n40\nA141\nA151\n2\nA173\n2\nA192\nA201\n1\n\n\n895\nA14\n36\nA33\nA41\n8947\nA65\nA74\n3\nA93\nA101\n...\nA123\n31\nA142\nA152\n1\nA174\n2\nA192\nA201\n1\n\n\n915\nA12\n48\nA30\nA410\n18424\nA61\nA73\n1\nA92\nA101\n...\nA122\n32\nA141\nA152\n1\nA174\n1\nA192\nA202\n2\n\n\n927\nA11\n48\nA32\nA41\n10297\nA61\nA74\n4\nA93\nA101\n...\nA124\n39\nA142\nA153\n3\nA173\n2\nA192\nA201\n2\n\n\n935\nA12\n30\nA33\nA43\n1919\nA62\nA72\n4\nA93\nA101\n...\nA124\n30\nA142\nA152\n2\nA174\n1\nA191\nA201\n2\n\n\n\n\n50 rows × 21 columns\n\n\n\nỞ trên đã sử dụng tỷ lệ 5% mẫu để xác định anomaly. Ngoài ra có thể điều chỉnh ngưỡng anomaly scores phù hợp thay vì đưa ra tỷ lệ.\n\n# Identify anomalies based on anomaly scores (you can set a threshold)\nthreshold = -0.01  # Adjust the threshold as needed\nanomalies = X[anomaly_scores &lt; threshold]\nanomalies\n\n\n\n\n\n\n\n\nduration\ncredit_amount\ninstallment_rate\nresidence_since\nage\nexisting_credits\npeople_liable\nexisting_checking_A12\nexisting_checking_A13\nexisting_checking_A14\n...\nproperty_A124\nother_installment_plans_A142\nother_installment_plans_A143\nhousing_A152\nhousing_A153\njob_A172\njob_A173\njob_A174\ntelephone_A192\nforeign_worker_A202\n\n\n\n\n44\n48\n6143\n4\n4\n58\n2\n1\n0\n0\n0\n...\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n55\n6\n783\n1\n2\n26\n1\n2\n0\n0\n1\n...\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n87\n36\n12612\n1\n4\n47\n1\n2\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n99\n20\n7057\n3\n4\n36\n2\n2\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n175\n30\n7485\n4\n1\n53\n1\n1\n0\n0\n1\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n186\n9\n5129\n2\n4\n74\n1\n2\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n191\n48\n3844\n4\n4\n34\n1\n2\n1\n0\n0\n...\n1\n0\n1\n0\n1\n1\n0\n0\n0\n0\n\n\n263\n12\n2748\n2\n4\n57\n3\n1\n0\n0\n1\n...\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n272\n48\n12169\n4\n4\n36\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n287\n48\n7582\n2\n4\n31\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n304\n48\n10127\n2\n2\n44\n1\n1\n0\n0\n1\n...\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n\n\n306\n30\n4811\n2\n4\n24\n1\n1\n0\n0\n1\n...\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n374\n60\n14782\n3\n4\n60\n2\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n449\n15\n1512\n3\n3\n61\n2\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n496\n36\n9034\n4\n1\n29\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n\n\n549\n48\n8858\n2\n1\n35\n2\n1\n0\n0\n1\n...\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n595\n6\n931\n1\n1\n32\n1\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n602\n24\n1837\n4\n4\n34\n1\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n613\n24\n3632\n1\n4\n22\n1\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n616\n60\n9157\n2\n2\n27\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n0\n0\n\n\n663\n6\n1050\n4\n1\n35\n2\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n667\n48\n3609\n1\n1\n27\n1\n1\n0\n0\n1\n...\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n754\n12\n1555\n4\n4\n55\n2\n2\n0\n0\n1\n...\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n\n\n808\n42\n9283\n1\n2\n55\n1\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n889\n28\n7824\n3\n4\n40\n2\n2\n0\n0\n1\n...\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n895\n36\n8947\n3\n2\n31\n1\n2\n0\n0\n1\n...\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n915\n48\n18424\n1\n2\n32\n1\n1\n1\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n\n\n927\n48\n10297\n4\n4\n39\n3\n2\n0\n0\n0\n...\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n\n\n935\n30\n1919\n4\n3\n30\n2\n1\n1\n0\n0\n...\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n\n\n29 rows × 48 columns",
    "crumbs": [
      "Machine learning",
      "Isolation Forest - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/IsolationForest.html#sử-dụng-thuật-toán-isolation-forest-cho-việc-phát-hiện-dấu-hiệu-bất-thường-anomaly-detection",
    "href": "machine-learning/IsolationForest.html#sử-dụng-thuật-toán-isolation-forest-cho-việc-phát-hiện-dấu-hiệu-bất-thường-anomaly-detection",
    "title": "Isolation Forest - anomaly detection",
    "section": "",
    "text": "Bước 1: Chuẩn bị Dữ liệu - Bắt đầu bằng việc thu thập và tải dữ liệu mà bạn muốn phát hiện các dấu hiệu bất thường.\nBước 2: Tải và Import Thư viện - Trong Python, sử dụng các thư viện sau: numpy, pandas, và sklearn.ensemble.\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nBước 3: Chuẩn bị dữ liệu - Đảm bảo rằng dữ liệu của bạn đã được chuẩn bị và lấy ra từ các cột không mong muốn.\nBước 4: Tạo Mô hình Isolation Forest - Sử dụng IsolationForest để tạo một mô hình phát hiện dấu hiệu bất thường. Bạn có thể tuỳ chỉnh mô hình bằng cách thay đổi các tham số, chẳng hạn như contamination (tỷ lệ dấu hiệu bất thường trong dữ liệu).\nmodel = IsolationForest(contamination=0.05)  # Tuỳ chỉnh contamination tại đây\nBước 5: Huấn luyện Mô hình - Sử dụng dữ liệu huấn luyện của bạn để huấn luyện mô hình Isolation Forest.\nmodel.fit(X)\nBước 6: Dự đoán Dấu hiệu Bất thường - Sử dụng mô hình đã huấn luyện để dự đoán dấu hiệu bất thường trên tập dữ liệu kiểm tra.\npredictions = model.predict(X)\nBước 7: Lấy Điểm Anomaly - Sử dụng phương thức decision_function để lấy điểm anomaly cho mỗi mẫu dữ liệu. Điểm càng thấp càng có khả năng là dấu hiệu bất thường.\nanomaly_scores = model.decision_function(X)\nBước 8: Xác định Dấu hiệu Bất thường - Dùng ngưỡng (threshold) để xác định dấu hiệu bất thường dựa trên điểm anomaly. Bạn có thể tuỳ chỉnh ngưỡng theo ý muốn.\nthreshold = -0.1  # Ngưỡng ngầm định, điều chỉnh tùy theo nhu cầu\nanomalies = X[anomaly_scores &lt; threshold]\nBước 9: Xuất kết quả - Xuất các mẫu bất thường mà bạn đã xác định từ dữ liệu.\nprint(\"Dấu hiệu bất thường:\")\nprint(anomalies)\nLưu ý rằng bạn có thể tùy chỉnh các tham số và ngưỡng để điều chỉnh độ nhạy của việc phát hiện dấu hiệu bất thường dựa trên nhu cầu của bạn.\n\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\n\n\n# Create an Isolation Forest model\ncontamination = 0.05\nmodel = IsolationForest(contamination=contamination)\n\n# Fit the model to your data\nmodel.fit(X)\n\n# Predict anomalies (1 for inliers, -1 for outliers)\npredictions = model.predict(X)\n\n# Get the anomaly scores (the lower the score, the more likely it's an anomaly)\nanomaly_scores = model.decision_function(X)\n\n# Identify anomalies\nanomalies = data[predictions == -1]\n\nc:\\Users\\binhnn2\\anaconda3\\envs\\env_pycaret\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n  warnings.warn(\n\n\n\nanomalies\n\n\n\n\n\n\n\n\nexisting_checking\nduration\ncredit_history\npurpose\ncredit_amount\nsavings\nemployment\ninstallment_rate\npersonal_status\nother_debtors\n...\nproperty\nage\nother_installment_plans\nhousing\nexisting_credits\njob\npeople_liable\ntelephone\nforeign_worker\nclass\n\n\n\n\n44\nA11\n48\nA34\nA41\n6143\nA61\nA75\n4\nA92\nA101\n...\nA124\n58\nA142\nA153\n2\nA172\n1\nA191\nA201\n2\n\n\n55\nA14\n6\nA31\nA40\n783\nA65\nA73\n1\nA93\nA103\n...\nA121\n26\nA142\nA152\n1\nA172\n2\nA191\nA201\n1\n\n\n87\nA12\n36\nA32\nA46\n12612\nA62\nA73\n1\nA93\nA101\n...\nA124\n47\nA143\nA153\n1\nA173\n2\nA192\nA201\n2\n\n\n99\nA12\n20\nA33\nA41\n7057\nA65\nA74\n3\nA93\nA101\n...\nA122\n36\nA141\nA151\n2\nA174\n2\nA192\nA201\n1\n\n\n105\nA12\n24\nA34\nA410\n11938\nA61\nA73\n2\nA93\nA102\n...\nA123\n39\nA143\nA152\n2\nA174\n2\nA192\nA201\n2\n\n\n110\nA12\n6\nA33\nA49\n1449\nA62\nA75\n1\nA91\nA101\n...\nA123\n31\nA141\nA152\n2\nA173\n2\nA191\nA201\n1\n\n\n154\nA12\n24\nA33\nA49\n6967\nA62\nA74\n4\nA93\nA101\n...\nA123\n36\nA143\nA151\n1\nA174\n1\nA192\nA201\n1\n\n\n157\nA11\n12\nA31\nA48\n339\nA61\nA75\n4\nA94\nA101\n...\nA123\n45\nA141\nA152\n1\nA172\n1\nA191\nA201\n1\n\n\n175\nA14\n30\nA31\nA41\n7485\nA65\nA71\n4\nA92\nA101\n...\nA121\n53\nA141\nA152\n1\nA174\n1\nA192\nA201\n2\n\n\n181\nA12\n36\nA33\nA49\n4455\nA61\nA73\n2\nA91\nA101\n...\nA121\n30\nA142\nA152\n2\nA174\n1\nA192\nA201\n2\n\n\n186\nA12\n9\nA31\nA41\n5129\nA61\nA75\n2\nA92\nA101\n...\nA124\n74\nA141\nA153\n1\nA174\n2\nA192\nA201\n2\n\n\n191\nA12\n48\nA30\nA49\n3844\nA62\nA74\n4\nA93\nA101\n...\nA124\n34\nA143\nA153\n1\nA172\n2\nA191\nA201\n2\n\n\n226\nA12\n48\nA32\nA43\n10961\nA64\nA74\n1\nA93\nA102\n...\nA124\n27\nA141\nA152\n2\nA173\n1\nA192\nA201\n2\n\n\n263\nA14\n12\nA34\nA46\n2748\nA61\nA75\n2\nA92\nA101\n...\nA124\n57\nA141\nA153\n3\nA172\n1\nA191\nA201\n1\n\n\n272\nA12\n48\nA31\nA40\n12169\nA65\nA71\n4\nA93\nA102\n...\nA124\n36\nA143\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n287\nA12\n48\nA33\nA410\n7582\nA62\nA71\n2\nA93\nA101\n...\nA124\n31\nA143\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n304\nA14\n48\nA34\nA40\n10127\nA63\nA73\n2\nA93\nA101\n...\nA124\n44\nA141\nA153\n1\nA173\n1\nA191\nA201\n2\n\n\n306\nA14\n30\nA32\nA41\n4811\nA65\nA74\n2\nA92\nA101\n...\nA122\n24\nA142\nA151\n1\nA172\n1\nA191\nA201\n1\n\n\n333\nA14\n48\nA34\nA41\n11590\nA62\nA73\n2\nA92\nA101\n...\nA123\n24\nA141\nA151\n2\nA172\n1\nA191\nA201\n2\n\n\n343\nA12\n18\nA32\nA49\n4439\nA61\nA75\n1\nA93\nA102\n...\nA121\n33\nA141\nA152\n1\nA174\n1\nA192\nA201\n1\n\n\n373\nA14\n60\nA34\nA40\n13756\nA65\nA75\n2\nA93\nA101\n...\nA124\n63\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n374\nA12\n60\nA31\nA410\n14782\nA62\nA75\n3\nA92\nA101\n...\nA124\n60\nA141\nA153\n2\nA174\n1\nA192\nA201\n2\n\n\n395\nA12\n39\nA33\nA46\n11760\nA62\nA74\n2\nA93\nA101\n...\nA124\n32\nA143\nA151\n1\nA173\n1\nA192\nA201\n1\n\n\n449\nA12\n15\nA33\nA45\n1512\nA64\nA73\n3\nA94\nA101\n...\nA122\n61\nA142\nA152\n2\nA173\n1\nA191\nA201\n2\n\n\n496\nA12\n36\nA32\nA42\n9034\nA62\nA72\n4\nA93\nA102\n...\nA124\n29\nA143\nA151\n1\nA174\n1\nA192\nA201\n2\n\n\n539\nA13\n18\nA32\nA42\n3049\nA61\nA72\n1\nA92\nA101\n...\nA122\n45\nA142\nA152\n1\nA172\n1\nA191\nA201\n1\n\n\n549\nA14\n48\nA34\nA41\n8858\nA65\nA74\n2\nA93\nA101\n...\nA124\n35\nA143\nA153\n2\nA173\n1\nA192\nA201\n1\n\n\n551\nA14\n6\nA31\nA43\n1750\nA63\nA75\n2\nA93\nA101\n...\nA122\n45\nA141\nA152\n1\nA172\n2\nA191\nA201\n1\n\n\n595\nA12\n6\nA31\nA40\n931\nA62\nA72\n1\nA92\nA101\n...\nA122\n32\nA142\nA152\n1\nA172\n1\nA191\nA201\n2\n\n\n602\nA12\n24\nA31\nA46\n1837\nA61\nA74\n4\nA92\nA101\n...\nA124\n34\nA141\nA153\n1\nA172\n1\nA191\nA201\n2\n\n\n607\nA12\n36\nA32\nA43\n2671\nA62\nA73\n4\nA92\nA102\n...\nA124\n50\nA143\nA153\n1\nA173\n1\nA191\nA201\n2\n\n\n611\nA13\n10\nA32\nA40\n1240\nA62\nA75\n1\nA92\nA101\n...\nA124\n48\nA143\nA153\n1\nA172\n2\nA191\nA201\n2\n\n\n613\nA11\n24\nA31\nA41\n3632\nA61\nA73\n1\nA92\nA103\n...\nA123\n22\nA141\nA151\n1\nA173\n1\nA191\nA202\n1\n\n\n616\nA12\n60\nA33\nA43\n9157\nA65\nA73\n2\nA93\nA101\n...\nA124\n27\nA143\nA153\n1\nA174\n1\nA191\nA201\n1\n\n\n663\nA12\n6\nA33\nA42\n1050\nA61\nA71\n4\nA93\nA101\n...\nA122\n35\nA142\nA152\n2\nA174\n1\nA192\nA201\n1\n\n\n666\nA12\n30\nA31\nA42\n3496\nA64\nA73\n4\nA93\nA101\n...\nA123\n34\nA142\nA152\n1\nA173\n2\nA192\nA201\n1\n\n\n667\nA14\n48\nA31\nA49\n3609\nA61\nA73\n1\nA92\nA101\n...\nA121\n27\nA142\nA152\n1\nA173\n1\nA191\nA201\n1\n\n\n684\nA12\n36\nA33\nA49\n9857\nA62\nA74\n1\nA93\nA101\n...\nA122\n31\nA143\nA152\n2\nA172\n2\nA192\nA201\n1\n\n\n703\nA12\n30\nA33\nA49\n2503\nA62\nA75\n4\nA93\nA101\n...\nA122\n41\nA142\nA152\n2\nA173\n1\nA191\nA201\n1\n\n\n721\nA12\n6\nA31\nA46\n433\nA64\nA72\n4\nA92\nA101\n...\nA122\n24\nA141\nA151\n1\nA173\n2\nA191\nA201\n2\n\n\n754\nA14\n12\nA33\nA45\n1555\nA64\nA75\n4\nA93\nA101\n...\nA124\n55\nA143\nA153\n2\nA173\n2\nA191\nA201\n2\n\n\n774\nA13\n12\nA34\nA40\n1480\nA63\nA71\n2\nA93\nA101\n...\nA124\n66\nA141\nA153\n3\nA171\n1\nA191\nA201\n1\n\n\n808\nA12\n42\nA31\nA41\n9283\nA61\nA71\n1\nA93\nA101\n...\nA124\n55\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n829\nA12\n48\nA33\nA49\n6681\nA65\nA73\n4\nA93\nA101\n...\nA124\n38\nA143\nA153\n1\nA173\n2\nA192\nA201\n1\n\n\n876\nA11\n18\nA31\nA43\n1940\nA61\nA72\n3\nA93\nA102\n...\nA124\n36\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n889\nA14\n28\nA31\nA41\n7824\nA65\nA72\n3\nA93\nA103\n...\nA121\n40\nA141\nA151\n2\nA173\n2\nA192\nA201\n1\n\n\n895\nA14\n36\nA33\nA41\n8947\nA65\nA74\n3\nA93\nA101\n...\nA123\n31\nA142\nA152\n1\nA174\n2\nA192\nA201\n1\n\n\n915\nA12\n48\nA30\nA410\n18424\nA61\nA73\n1\nA92\nA101\n...\nA122\n32\nA141\nA152\n1\nA174\n1\nA192\nA202\n2\n\n\n927\nA11\n48\nA32\nA41\n10297\nA61\nA74\n4\nA93\nA101\n...\nA124\n39\nA142\nA153\n3\nA173\n2\nA192\nA201\n2\n\n\n935\nA12\n30\nA33\nA43\n1919\nA62\nA72\n4\nA93\nA101\n...\nA124\n30\nA142\nA152\n2\nA174\n1\nA191\nA201\n2\n\n\n\n\n50 rows × 21 columns\n\n\n\nỞ trên đã sử dụng tỷ lệ 5% mẫu để xác định anomaly. Ngoài ra có thể điều chỉnh ngưỡng anomaly scores phù hợp thay vì đưa ra tỷ lệ.\n\n# Identify anomalies based on anomaly scores (you can set a threshold)\nthreshold = -0.01  # Adjust the threshold as needed\nanomalies = X[anomaly_scores &lt; threshold]\nanomalies\n\n\n\n\n\n\n\n\nduration\ncredit_amount\ninstallment_rate\nresidence_since\nage\nexisting_credits\npeople_liable\nexisting_checking_A12\nexisting_checking_A13\nexisting_checking_A14\n...\nproperty_A124\nother_installment_plans_A142\nother_installment_plans_A143\nhousing_A152\nhousing_A153\njob_A172\njob_A173\njob_A174\ntelephone_A192\nforeign_worker_A202\n\n\n\n\n44\n48\n6143\n4\n4\n58\n2\n1\n0\n0\n0\n...\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n55\n6\n783\n1\n2\n26\n1\n2\n0\n0\n1\n...\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n87\n36\n12612\n1\n4\n47\n1\n2\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n99\n20\n7057\n3\n4\n36\n2\n2\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n175\n30\n7485\n4\n1\n53\n1\n1\n0\n0\n1\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n186\n9\n5129\n2\n4\n74\n1\n2\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n191\n48\n3844\n4\n4\n34\n1\n2\n1\n0\n0\n...\n1\n0\n1\n0\n1\n1\n0\n0\n0\n0\n\n\n263\n12\n2748\n2\n4\n57\n3\n1\n0\n0\n1\n...\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n272\n48\n12169\n4\n4\n36\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n287\n48\n7582\n2\n4\n31\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n304\n48\n10127\n2\n2\n44\n1\n1\n0\n0\n1\n...\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n\n\n306\n30\n4811\n2\n4\n24\n1\n1\n0\n0\n1\n...\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n374\n60\n14782\n3\n4\n60\n2\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n449\n15\n1512\n3\n3\n61\n2\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n496\n36\n9034\n4\n1\n29\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n\n\n549\n48\n8858\n2\n1\n35\n2\n1\n0\n0\n1\n...\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n595\n6\n931\n1\n1\n32\n1\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n602\n24\n1837\n4\n4\n34\n1\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n613\n24\n3632\n1\n4\n22\n1\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n616\n60\n9157\n2\n2\n27\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n0\n0\n\n\n663\n6\n1050\n4\n1\n35\n2\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n667\n48\n3609\n1\n1\n27\n1\n1\n0\n0\n1\n...\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n754\n12\n1555\n4\n4\n55\n2\n2\n0\n0\n1\n...\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n\n\n808\n42\n9283\n1\n2\n55\n1\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n889\n28\n7824\n3\n4\n40\n2\n2\n0\n0\n1\n...\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n895\n36\n8947\n3\n2\n31\n1\n2\n0\n0\n1\n...\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n915\n48\n18424\n1\n2\n32\n1\n1\n1\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n\n\n927\n48\n10297\n4\n4\n39\n3\n2\n0\n0\n0\n...\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n\n\n935\n30\n1919\n4\n3\n30\n2\n1\n1\n0\n0\n...\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n\n\n29 rows × 48 columns",
    "crumbs": [
      "Machine learning",
      "Isolation Forest - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/IsolationForest.html#nhược-điểm-của-isolation-forest",
    "href": "machine-learning/IsolationForest.html#nhược-điểm-của-isolation-forest",
    "title": "Isolation Forest - anomaly detection",
    "section": "Nhược điểm của Isolation Forest",
    "text": "Nhược điểm của Isolation Forest\nIsolation Forest là một thuật toán mạnh mẽ cho phát hiện dấu hiệu bất thường trong dữ liệu. Tuy nhiên, nó cũng có nhược điểm và hạn chế của riêng nó. Dưới đây là một số nhược điểm của Isolation Forest cùng với ví dụ:\nNhược Điểm:\n\nKhông hiệu quả đối với dấu hiệu bất thường gần nhau: Isolation Forest không hoạt động tốt trong việc phát hiện dấu hiệu bất thường nằm gần nhau hoặc trải dài dọc theo một đường.\nĐộ nhạy với giá trị ngưỡng: Việc đặt ngưỡng quyết định dấu hiệu bất thường có thể khá khó và phụ thuộc vào kiểu dữ liệu và phân phối của dữ liệu. Nếu bạn đặt ngưỡng quá thấp, có thể dẫn đến nhiều dấu hiệu giả mạo; ngược lại, nếu đặt ngưỡng quá cao, có thể bỏ lỡ nhiều dấu hiệu thực sự bất thường.\nKhả năng đối mặt với dữ liệu nhiều chiều: Isolation Forest có thể gặp khó khăn khi xử lý dữ liệu có số chiều rất lớn. Trong các không gian nhiều chiều, việc tạo ra các phân nhánh ngẫu nhiên có thể dẫn đến các cây con có chiều sâu quá thấp, làm giảm khả năng phát hiện dấu hiệu bất thường.\n\nVí dụ về Nhược Điểm của Isolation Forest:\nGiả sử bạn có dữ liệu về giao dịch tài chính trong một ngân hàng và bạn muốn phát hiện gian lận trong các giao dịch. Một số gian lận xảy ra với các mẫu tương tự với các giao dịch thông thường, nhưng với số lượng nhỏ và gần nhau trong không gian đặc trưng. Isolation Forest có thể không hiệu quả trong việc phát hiện các gian lận như vậy vì nó có thể đánh giá chúng là dấu hiệu thông thường do chúng không được cô lập.\nNhược điểm này có thể được khắc phục bằng cách sử dụng các kỹ thuật phát hiện gian lận cơ bản hơn và phù hợp hơn cho loại tình huống này hoặc bằng việc sử dụng các thuật toán khác như Local Outlier Factor (LOF) hoặc One-Class SVM.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(0)\n\n# Generate random data for the first blob (normal data)\nmean1 = [2, 2]\ncov1 = [[1, 0.5], [0.5, 1]]\ndata1 = np.random.multivariate_normal(mean1, cov1, 100)\n\n# Generate random data for the second blob (abnormal data)\nmean2 = [7, 7]\ncov2 = [[1, -0.5], [-0.5, 1]]\ndata2 = np.random.multivariate_normal(mean2, cov2, 100)\n\n# Combine the two sets of data\ndata = np.vstack((data1, data2))\n\n# Create a scatter plot of the data\nplt.scatter(data1[:, 0], data1[:, 1], marker='o', c='b', label='Normal Data')\nplt.scatter(data2[:, 0], data2[:, 1], marker='x', c='r', label='Abnormal Data')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title(\"Can't distinct normal and abnormal\")\nplt.legend()\n\nplt.grid()\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Isolation Forest - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "",
    "text": "import shap\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Load the German Credit Data\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ny_train[y_train == 2] = 0\ny_test[y_test == 2] = 0",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#load-libraries-and-data",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#load-libraries-and-data",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "",
    "text": "import shap\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Load the German Credit Data\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ny_train[y_train == 2] = 0\ny_test[y_test == 2] = 0",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#train-a-logistic-regression-model",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#train-a-logistic-regression-model",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "Train a Logistic Regression Model",
    "text": "Train a Logistic Regression Model\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score\n# Create a pipeline with feature selection using SelectKBest and logistic regression\npipeline = Pipeline([\n    ('scaler', StandardScaler()),                       # Standardize features\n    ('feature_selector', SelectKBest(score_func=f_classif, k=10)),  # Select top k features using f_classif\n    ('classifier', LogisticRegression(max_iter=1000))   # Logistic regression classifier\n])\n# Train the pipeline on the training data\npipeline.fit(X_train, y_train)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('feature_selector', SelectKBest()),\n                ('classifier', LogisticRegression(max_iter=1000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('feature_selector', SelectKBest()),\n                ('classifier', LogisticRegression(max_iter=1000))])StandardScalerStandardScaler()SelectKBestSelectKBest()LogisticRegressionLogisticRegression(max_iter=1000)\n\n\n\n# Predict on the test data\ny_pred = pipeline.predict(X_test)\n\n# Calculate accuracy\nauc = roc_auc_score(y_test, y_pred)\nprint(f\"auc: {auc:.2f}\")\n\nauc: 0.60",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#create-shap-explainer",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#create-shap-explainer",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "Create SHAP Explainer",
    "text": "Create SHAP Explainer\n\n# Create a callable model class that wraps the pipeline\nclass PipelineModel:\n    def __init__(self, pipeline):\n        self.pipeline = pipeline\n    \n    def __call__(self, x):\n        return self.pipeline.predict_proba(x)[:, 1]\n\n# Create a SHAP explainer for the callable model\ncallable_model = PipelineModel(pipeline)\nexplainer = shap.Explainer(callable_model, X_train)\n\n# Choose an instance to explain\ninstance_to_explain = X_test.iloc[0]  # Choose the instance you want to explain\n\n# Calculate SHAP values for the instance\nshap_values = explainer.shap_values(instance_to_explain.values.reshape(1, -1))\n\n# Print SHAP values for each feature\nprint(\"SHAP Values:\")\nprint(shap_values)\n\nSHAP Values:\n[[ 0.02579396  0.00329706  0.          0.         -0.0606596   0.\n   0.         -0.01900188  0.         -0.081538    0.00779974  0.\n   0.         -0.0338136   0.          0.          0.          0.09000797\n   0.          0.          0.          0.          0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.          0.          0.01083979  0.          0.          0.02214676\n   0.          0.          0.          0.          0.          0.        ]]\n\n\n\n.629+.09-.08-.03-.06+.03+.02-.02+.02\n\n0.599\n\n\n\n# Choose the specific feature for which you want to calculate change\nobs_index = 0  # Replace with the index of the feature you're interested in\nshap_object  = explainer(X_test)\nshap.plots.waterfall(shap_object[obs_index])\n\n\n\n\n\n\n\n\n\n# Choose the specific feature for which you want to calculate change\nobs_index = 0  # Replace with the index of the feature you're interested in\nshap_object  = explainer(X_train)\nshap.plots.waterfall(shap_object[obs_index])\n\n\n\n\n\n\n\n\n\nX_train[0:]\n\n\n\n\n\n\n\n\nduration\ncredit_amount\ninstallment_rate\nresidence_since\nage\nexisting_credits\npeople_liable\nexisting_checking_A12\nexisting_checking_A13\nexisting_checking_A14\n...\nproperty_A124\nother_installment_plans_A142\nother_installment_plans_A143\nhousing_A152\nhousing_A153\njob_A172\njob_A173\njob_A174\ntelephone_A192\nforeign_worker_A202\n\n\n\n\n29\n60\n6836\n3\n4\n63\n2\n1\n0\n0\n0\n...\n1\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n535\n21\n2319\n2\n1\n33\n1\n1\n0\n1\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n695\n6\n1236\n2\n4\n50\n1\n1\n0\n0\n1\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n557\n21\n5003\n1\n4\n29\n2\n1\n0\n0\n1\n...\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n836\n12\n886\n4\n2\n21\n1\n1\n0\n0\n1\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n106\n18\n6458\n2\n4\n39\n2\n2\n0\n0\n1\n...\n1\n0\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n270\n18\n2662\n4\n3\n32\n1\n1\n0\n0\n1\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n\n\n860\n24\n5804\n4\n2\n27\n2\n1\n0\n0\n1\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n435\n12\n1484\n2\n1\n25\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n102\n6\n932\n3\n2\n24\n1\n1\n0\n0\n1\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n800 rows × 48 columns",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#so-sánh-giữa-các-giá-trị-shap-và-hệ-số-đối-với-hồi-quy-logistic",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#so-sánh-giữa-các-giá-trị-shap-và-hệ-số-đối-với-hồi-quy-logistic",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "So sánh giữa các giá trị SHAP và Hệ số đối với hồi quy logistic",
    "text": "So sánh giữa các giá trị SHAP và Hệ số đối với hồi quy logistic\n\n\n\n\n\n\n\n\nKhía cạnh\nGiá trị SHAP\nHệ số (Hồi quy logistic)\n\n\n\n\nMục đích\nGiải thích các dự đoán riêng lẻ, tác động của feature\nGiải thích mối quan hệ tổng thể của feature-log-odds\n\n\nKhả năng giải thích\nDễ hiểu trực tiếp hơn đối với các dự đoán riêng lẻ\nGiải thích thường yêu cầu ngữ cảnh\n\n\nHiệu ứng tương tác\nGhi lại các tương tác phức tạp giữa các features\nGiả sử tương tác tuyến tính (hạn chế)\n\n\nKhả năng ứng dụng\nÁp dụng cho nhiều mô hình khác nhau, kể cả những mô hình phức tạp\nCụ thể cho các mô hình tuyến tính như hồi quy logistic\n\n\nTrực quan hóa\nCó sẵn nhiều phương pháp trực quan hóa khác nhau để hiểu rõ hơn\nFeature importance\n\n\nSo sánh\nCó thể so sánh giữa các mô hình khác nhau\nCụ thể cho kiến trúc mô hình\n\n\nTính toán\nCó thể tốn kém về mặt tính toán đối với các mô hình phức tạp\nTính toán hiệu quả cho hồi quy logistic\n\n\nTính linh hoạt\nHỗ trợ tính phi tuyến tính, hữu ích cho các mô hình phức tạp\nGiả định mối quan hệ tuyến tính, kém linh hoạt\n\n\nChi tiết giải thích\nTác động feature chi tiết đến từng dự đoán\nTác động chung của feature lên log-odds",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#cách-giải-thích-hệ-số-mô-hình-logistic",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#cách-giải-thích-hệ-số-mô-hình-logistic",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "Cách giải thích hệ số mô hình logistic",
    "text": "Cách giải thích hệ số mô hình logistic\nCông thức của Logistic Regression có thể được biểu diễn dưới dạng log-odds như sau:\n\\[ \\text{log-odds} = \\ln \\left( \\frac{P(y=1 \\,|\\, \\mathbf{x})}{1 - P(y=1 \\,|\\, \\mathbf{x})} \\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p \\]\nTrong đó: - \\(P(y=1 \\,|\\, \\mathbf{x})\\) là xác suất có điều kiện rằng kết quả \\(y\\) bằng 1 dưới điều kiện các đặc trưng \\(\\mathbf{x}\\). - \\(\\beta_0\\) là hệ số chặn. - \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) là các hệ số cho các đặc trưng \\(x_1, x_2, \\ldots, x_p\\). - \\(\\ln\\) đại diện cho hàm logarit tự nhiên.\nBiểu thức \\(\\frac{P(y=1 \\,|\\, \\mathbf{x})}{1 - P(y=1 \\,|\\, \\mathbf{x})}\\) là tỷ lệ giữa xác suất nhóm 1 và xác suất nhóm 0, còn được gọi là tỷ lệ cơ hội (odds). Hàm logarit tự nhiên \\(\\ln\\) biến đổi tỷ lệ cơ hội thành giá trị log-odds.\nCông thức log-odds thể hiện mối liên hệ tuyến tính giữa các đặc trưng và log-odds của xác suất dương. Trong quá trình huấn luyện mô hình, các hệ số \\(\\beta_i\\) được điều chỉnh để tối ưu hóa khả năng dự đoán chính xác dựa trên dữ liệu huấn luyện.\nCác hệ số của mô hình hồi quy logistic biểu thị mối quan hệ giữa các biến độc lập (features) và tỷ lệ log-odds của biến phụ thuộc (kết quả nhị phân). Các hệ số này có thể được giải thích để hiểu những thay đổi trong features ảnh hưởng như thế nào đến xác suất của lớp dương (positive class).\nĐây là cách bạn có thể giải thích các hệ số của mô hình hồi quy logistic:\n\nHệ số dương:\n\nHệ số dương (ví dụ: \\(\\beta_1 &gt; 0\\)) chỉ ra rằng việc tăng giá trị biến độc lập tương ứng dẫn đến tăng tỷ lệ log-odds của lớp dương.\nHay là khi giá trị biến độc lập tăng thêm một đơn vị thì xác suất xuất hiện lớp dương cũng tăng.\n\nHệ số âm:\n\nHệ số âm (ví dụ: \\(\\beta_2 &lt; 0\\)) chỉ ra rằng việc tăng giá trị feature tương ứng dẫn đến giảm tỷ lệ log-odds của lớp dương.\nHay là khi giá trị biến độc lập tăng lên một đơn vị thì xác suất xuất hiện lớp dương sẽ giảm.\n\nĐộ lớn của các hệ số:\n\nĐộ lớn của các hệ số biểu thị mức độ mạnh của mối quan hệ giữa feature và outcome.\nGiá trị tuyệt đối lớn hơn cho thấy tác động mạnh hơn đến log-odds và do đó đến xác suất dự đoán.\n\nGiải thích về tỷ lệ Odds:\n\nVí dụ: nếu \\(\\beta_3 = 0.5\\), tỷ lệ chênh lệch là \\(e^{0.5} \\approx 1.648\\). Điều này có nghĩa là khi tính năng này tăng thêm một đơn vị, tỷ lệ xảy ra kết quả tích cực sẽ tăng theo hệ số xấp xỉ 1,648.\n\n\nDưới đây là ví dụ về cách bạn có thể giải thích hệ số dương: “Khi tăng một đơn vị trong biến ‘Tuổi’, tỷ lệ log của một khách hàng không trả được nợ sẽ tăng thêm 0,25. Điều này có nghĩa là khi độ tuổi của khách hàng tăng lên một năm, khả năng vỡ nợ cũng tăng lên.”\nVà một ví dụ về hệ số âm: “Đối với mỗi đơn vị tăng trong biến ‘Thu nhập’, tỷ lệ log của một khách hàng không trả được nợ sẽ giảm 0,15. Điều này cho thấy rằng mức thu nhập cao hơn có liên quan đến khả năng giảm vỡ nợ.”\nCông thức SHAP Value:\nCông thức chính của SHAP (SHapley Additive exPlanations) value cho một đặc trưng \\(i\\) đối với một dự đoán cụ thể là:\n\\[ SHAP_i = \\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} \\left[ f(x_S \\cup \\{i\\}) - f(x_S) \\right] \\]\nTrong đó: - \\(N\\) là tập hợp tất cả các đặc trưng. - \\(S\\) là một tập con của \\(N\\) không chứa đặc trưng \\(i\\). - \\(f(x_S)\\) là dự đoán của mô hình cho mẫu dữ liệu \\(x\\) khi chỉ giữ lại các đặc trưng trong tập \\(S\\). - \\(f(x_S \\cup \\{i\\})\\) là dự đoán của mô hình cho mẫu dữ liệu \\(x\\) khi thêm đặc trưng \\(i\\) vào tập \\(S\\).\nGiải thích SHAP Value:\nSHAP value giải thích sự đóng góp của từng đặc trưng vào giá trị dự đoán của mô hình cho một mẫu dữ liệu cụ thể. Đối với mỗi đặc trưng, SHAP value là sự khác biệt giữa giá trị dự đoán khi thêm đặc trưng đó và khi không thêm đặc trưng, được trung bình trên tất cả các tập con của các đặc trưng khác.\n\nNếu \\(SHAP_i &gt; 0\\), đặc trưng \\(i\\) có xu hướng tăng giá trị dự đoán so với kỳ vọng cơ bản (trung bình trên tất cả các tập con).\nNếu \\(SHAP_i &lt; 0\\), đặc trưng \\(i\\) có xu hướng giảm giá trị dự đoán so với kỳ vọng cơ bản.\nNếu \\(SHAP_i = 0\\), đặc trưng \\(i\\) không có sự đóng góp vào sự khác biệt giữa dự đoán thực tế và kỳ vọng cơ bản.\n\nSHAP value cho mỗi đặc trưng cung cấp cái nhìn về việc tại sao dự đoán cuối cùng lại có giá trị cụ thể. Bằng cách tổng hợp các SHAP value lại, bạn có thể hiểu được cách các đặc trưng ảnh hưởng đến dự đoán và làm thế nào chúng kết hợp lại để tạo ra giá trị dự đoán cho mẫu dữ liệu đó.",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/EnhanceTraditionalScoreCardByReinforcementLearning.html",
    "href": "machine-learning/EnhanceTraditionalScoreCardByReinforcementLearning.html",
    "title": "Use Reinforcement learning to enhance traditional scorecard",
    "section": "",
    "text": "Sử dụng học tăng cường để tăng cường các mô hình tính điểm tín dụng truyền thống liên quan đến việc tận dụng điểm mạnh của cả hai phương pháp. Dưới đây là hướng dẫn từng bước về cách thực hiện:\n\nXác định mục tiêu kinh doanh: Xác định rõ mục tiêu kinh doanh của bạn và vấn đề bạn muốn giải quyết bằng cách sử dụng phương pháp học tăng cường. Xác định những khía cạnh của quy trình chấm điểm tín dụng mà bạn muốn cải thiện.\nData Preparation:\n\nThu thập và xử lý tiền dữ liệu tín dụng lịch sử, bao gồm thông tin người nộp đơn, biến đầu vào và kết quả (default or non-default).\nChia dữ liệu thành các tập huấn luyện, xác nhận và kiểm tra (training, validation, and test sets).\n\nXác định chức năng phần thưởng: Thiết kế chức năng phần thưởng phản ánh các mục tiêu của hệ thống tính điểm tín dụng của bạn. Chức năng khen thưởng sẽ khuyến khích agent đưa ra quyết định phù hợp với mục tiêu kinh doanh của bạn, chẳng hạn như tối đa hóa lợi nhuận hoặc giảm thiểu khả năng vỡ nợ.\nFeature Engineering: Trích xuất và tiền xử lý các biến đầu vào từ dữ liệu tín dụng mà tác nhân học tăng cường sẽ sử dụng để đưa ra quyết định. Dữ liệu này có thể bao gồm nhân khẩu học của người nộp đơn, chỉ số tài chính, lịch sử tín dụng, v.v.\nKết hợp các mô hình:\n\nHuấn luyện mô hình chấm điểm tín dụng truyền thống (ví dụ: hồi quy logistic, cây quyết định) bằng cách sử dụng dữ liệu đào tạo của bạn.\nSử dụng các dự đoán của mô hình này như một phần của biểu diễn trạng thái trong thiết lập học tăng cường.\n\nXây dựng thiết lập Reinforcement Learning:\n\nXác định không gian trạng thái: Kết hợp các kết quả đầu ra của mô hình truyền thống với các đặc điểm liên quan khác như biểu diễn trạng thái.\nXác định không gian hành động: Xác định các hành động có thể thực hiện (phê duyệt, từ chối) hoặc xem xét các quyết định khác như đặt giới hạn tín dụng.\nThực hiện các chiến lược thăm dò: Cân bằng thăm dò và khai thác để thu thập dữ liệu cho việc học.\n\nReinforcement Learning Algorithm:\n\nSử dụng thuật toán như kẻ cướp theo ngữ cảnh hoặc kẻ cướp nhiều vũ trang cho phép đưa ra các quyết định tuần tự dựa trên trạng thái và hành động.\nCập nhật các tham số mô hình dựa trên phần thưởng nhận được từ chức năng phần thưởng.\n\nTraining and Evaluation:\n\nHuấn luyện tác nhân học tăng cường bằng cách sử dụng dữ liệu huấn luyện và xác thực hiệu suất của nó trên bộ xác thực.\nTinh chỉnh các siêu tham số và chiến lược để tối ưu hóa hiệu suất.\n\nKhả năng diễn giải và tính công bằng của mô hình:\n\nĐảm bảo rằng các quyết định của mô hình học tăng cường có thể diễn giải và giải thích được, đặc biệt là trong bối cảnh quy định.\nThực hiện các chiến lược nhận thức về sự công bằng để ngăn chặn sự thiên vị và đảm bảo sự công bằng trong quá trình ra quyết định.\n\nThử nghiệm và triển khai:\n\nThử nghiệm mô hình tính điểm tín dụng nâng cao trên dữ liệu thử nghiệm chưa từng thấy để đánh giá hiệu quả hoạt động trong thế giới thực của nó.\nTriển khai mô hình trong môi trường được kiểm soát và theo dõi hiệu suất của nó theo thời gian.\n\nHọc tập và tối ưu hóa liên tục:\n\nTriển khai thiết lập học trực tuyến trong đó mô hình tiếp tục học từ dữ liệu mới khi có sẵn.\nThường xuyên đánh giá và cập nhật mô hình để thích ứng với các xu hướng và hành vi thay đổi.\n\n\nHãy nhớ rằng việc áp dụng học tập tăng cường trong việc chấm điểm tín dụng đòi hỏi phải xem xét cẩn thận các khía cạnh pháp lý, đạo đức và quy định. Tính minh bạch, công bằng và tuân thủ luật pháp là rất quan trọng khi đưa ra quyết định tín dụng bằng hệ thống AI.\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Simulated credit data\n# Features: Age, Income, Credit History, Loan Amount, ...\nX = np.random.rand(1000, 5)\ny = np.random.choice([0, 1], size=1000)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a traditional credit scoring model (Logistic Regression)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate the traditional model\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Traditional Model Accuracy:\", accuracy)\n\n# Reinforcement Learning Setup (Simplified)\nclass CreditScoringEnvironment:\n    def __init__(self, features):\n        self.features = features\n        self.current_instance = 0\n    \n    def reset(self):\n        self.current_instance = 0\n        return self.features[self.current_instance]\n    \n    def step(self, action):\n        reward = 1 if action == y[self.current_instance] else -1\n        self.current_instance += 1\n        done = self.current_instance &gt;= len(self.features)\n        if done:\n            return None, reward, done, {}\n        else:\n            return self.features[self.current_instance], reward, done, {}\n\n# Initialize environment\nenv = CreditScoringEnvironment(X_test)\n\n# Reinforcement Learning Algorithm (Simplified)\nnum_episodes = 100\nlearning_rate = 0.1\ndiscount_factor = 0.95\nexploration_prob = 0.2\n\nQ = np.zeros((len(X_test), 2))  # Q-values (approve, deny)\n\nfor episode in range(num_episodes):\n    state = env.reset()\n    state_index = env.current_instance\n    \n    done = False\n    \n    while not done:\n        if np.random.rand() &lt; exploration_prob:\n            action = np.random.choice([0, 1])  # Exploration\n        else:\n            action = np.argmax(Q[state_index, :])  # Exploitation\n        \n        new_state, reward, done, _ = env.step(action)\n        \n        if new_state is not None:\n            new_state_index = env.current_instance\n            \n            Q[state_index, action] = (1 - learning_rate) * Q[state_index, action] + \\\n                                     learning_rate * (reward + discount_factor * np.max(Q[new_state_index, :]))\n            \n            state_index = new_state_index\n\n# Evaluate the enhanced model\ny_rl_pred = np.argmax(Q, axis=1)\nrl_accuracy = accuracy_score(y_test, y_rl_pred)\nprint(\"Enhanced Model Accuracy:\", rl_accuracy)\n\nTraditional Model Accuracy: 0.475\nEnhanced Model Accuracy: 0.525",
    "crumbs": [
      "Machine learning",
      "Use Reinforcement learning to enhance traditional scorecard"
    ]
  },
  {
    "objectID": "machine-learning/modelCalibration.html",
    "href": "machine-learning/modelCalibration.html",
    "title": "Model calibration",
    "section": "",
    "text": "Model calibration is a process used in machine learning to ensure that the predicted probabilities from a classification model reflect the true likelihood of the corresponding outcomes. In classification tasks, a well-calibrated model should produce probability estimates that are both reliable and accurate. This is important because many machine learning applications, such as risk assessment, medical diagnosis, and fraud detection, rely on these probability estimates to make decisions.\nCalibration involves adjusting the predicted probabilities from a model to align them with the actual outcomes. There are several methods to calibrate models, and the choice of method depends on the specific algorithm and problem. Some common calibration techniques include:\n\nPlatt Scaling: This is a simple method that fits a logistic regression model to the output of the original model. It essentially learns a transformation to map the model’s raw scores to calibrated probabilities.\nIsotonic Regression: Isotonic regression is a non-parametric approach that fits a piecewise-constant, non-decreasing function to the predicted probabilities. It can be more flexible than Platt scaling in some cases.\nBeta Calibration: This method models the predicted probabilities as Beta distributions and estimates the parameters of these distributions to better match the observed outcomes.\nHistogram Binning: This technique divides the predicted probabilities into bins and estimates the true class frequencies within each bin. It then uses these frequencies to adjust the predicted probabilities.\nBayesian Calibration: Bayesian methods can be used to update the model’s probability estimates based on prior knowledge or data. This approach can be especially useful when you have a limited amount of data.\n\nThe goal of model calibration is to ensure that, for instance, if a model predicts a 70% probability of an event happening, it should be correct approximately 70% of the time in practice. Calibrated models provide more meaningful and reliable probability estimates, making them easier to interpret and use for decision-making. In applications where it’s important to understand the level of confidence associated with predictions, model calibration is a critical step in improving the model’s utility.",
    "crumbs": [
      "Machine learning",
      "Model calibration"
    ]
  },
  {
    "objectID": "machine-learning/modelCalibration.html#model-calibration",
    "href": "machine-learning/modelCalibration.html#model-calibration",
    "title": "Model calibration",
    "section": "",
    "text": "Model calibration is a process used in machine learning to ensure that the predicted probabilities from a classification model reflect the true likelihood of the corresponding outcomes. In classification tasks, a well-calibrated model should produce probability estimates that are both reliable and accurate. This is important because many machine learning applications, such as risk assessment, medical diagnosis, and fraud detection, rely on these probability estimates to make decisions.\nCalibration involves adjusting the predicted probabilities from a model to align them with the actual outcomes. There are several methods to calibrate models, and the choice of method depends on the specific algorithm and problem. Some common calibration techniques include:\n\nPlatt Scaling: This is a simple method that fits a logistic regression model to the output of the original model. It essentially learns a transformation to map the model’s raw scores to calibrated probabilities.\nIsotonic Regression: Isotonic regression is a non-parametric approach that fits a piecewise-constant, non-decreasing function to the predicted probabilities. It can be more flexible than Platt scaling in some cases.\nBeta Calibration: This method models the predicted probabilities as Beta distributions and estimates the parameters of these distributions to better match the observed outcomes.\nHistogram Binning: This technique divides the predicted probabilities into bins and estimates the true class frequencies within each bin. It then uses these frequencies to adjust the predicted probabilities.\nBayesian Calibration: Bayesian methods can be used to update the model’s probability estimates based on prior knowledge or data. This approach can be especially useful when you have a limited amount of data.\n\nThe goal of model calibration is to ensure that, for instance, if a model predicts a 70% probability of an event happening, it should be correct approximately 70% of the time in practice. Calibrated models provide more meaningful and reliable probability estimates, making them easier to interpret and use for decision-making. In applications where it’s important to understand the level of confidence associated with predictions, model calibration is a critical step in improving the model’s utility.",
    "crumbs": [
      "Machine learning",
      "Model calibration"
    ]
  },
  {
    "objectID": "machine-learning/modelCalibration.html#history-of-calibration-models",
    "href": "machine-learning/modelCalibration.html#history-of-calibration-models",
    "title": "Model calibration",
    "section": "History of calibration models",
    "text": "History of calibration models\nThe history of calibration models in the context of machine learning and statistics is closely tied to the need for well-calibrated probability estimates, especially in classification tasks. Here’s a brief overview of the history and development of calibration models:\n\nEarly Days of Probability Estimation (Pre-2000s): In the early years of machine learning and statistics, the primary focus was on developing classification models that predict class labels. Probability estimates were often not a central concern.\nRise of Support Vector Machines (SVMs): In the 1990s and early 2000s, Support Vector Machines gained popularity. These models produced decision functions that were not inherently calibrated probability estimates, leading to a need for calibration techniques.\nPlatt Scaling (1999): John Platt introduced Platt Scaling, a logistic regression-based calibration technique, in his paper “Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.” This method became one of the earliest and widely used approaches for converting SVM scores into well-calibrated probabilities.\nLogistic Regression Calibration: Logistic regression, a well-established statistical technique, was used as an early calibration method for various classifiers. It helped map model scores to calibrated probabilities.\nDevelopment of Calibration Metrics (2000s): During this period, researchers started to develop calibration metrics, such as the Brier Score and calibration plots, to quantitatively measure the calibration performance of models.\nBayesian Calibration (2000s): Bayesian approaches for calibration started to gain attention. These methods allow for the incorporation of prior knowledge and often provide robust calibration.\nIsotonic Regression (2000s): Isotonic regression became a popular non-parametric technique for calibration. It offers flexibility by allowing for monotonic transformations of probability estimates.\nImprovements in Machine Learning Models (2010s): The advancement of machine learning models, such as gradient-boosted trees and neural networks, led to the need for more advanced calibration methods. Techniques like Beta Calibration started to emerge.\nSoftware Libraries and Packages: As machine learning and data science gained prominence, software libraries and packages like scikit-learn and TensorFlow included calibration functionalities, making it easier for practitioners to apply calibration techniques.\nOngoing Research and Development: Research in calibration continues, focusing on more complex models, large-scale applications, and novel techniques. The field is evolving to address challenges in deep learning and ensemble methods.\n\nIn summary, the history of calibration models reflects the growing awareness of the importance of well-calibrated probability estimates in various machine learning applications. Over the years, a variety of techniques and methods have been developed to address calibration issues, enabling machine learning models to provide more reliable and interpretable probability estimates for informed decision-making. The field continues to evolve as new challenges and opportunities emerge in machine learning and data analysis.",
    "crumbs": [
      "Machine learning",
      "Model calibration"
    ]
  },
  {
    "objectID": "machine-learning/modelCalibration.html#beta-calibration",
    "href": "machine-learning/modelCalibration.html#beta-calibration",
    "title": "Model calibration",
    "section": "Beta Calibration",
    "text": "Beta Calibration\nBeta Calibration is one of the more recent calibration techniques that has gained attention in the field of machine learning. Its emergence can be attributed to several factors:\n\nImproved Calibration Needs: As machine learning models have become increasingly popular for a wide range of applications, the need for well-calibrated probability estimates has grown. Many machine learning models, including complex ensemble methods and deep learning models, often produce uncalibrated or poorly calibrated probability estimates. This need for improved calibration led to the exploration of novel calibration techniques, including Beta Calibration.\nFlexibility in Modeling Probability Distributions: Beta Calibration offers flexibility in modeling probability distributions. It assumes that predicted probabilities can be represented by a Beta distribution, which is a flexible parametric distribution. This flexibility allows it to capture a wide range of probability distribution shapes, making it suitable for a variety of real-world scenarios.\nSuitability for Imbalanced Datasets: In many practical applications, datasets are imbalanced, meaning one class significantly outnumbers the other. Traditional calibration techniques may not be effective in such cases. Beta Calibration’s ability to handle imbalanced data by modeling the distribution of predicted probabilities is a valuable feature.\nAddressing Overconfidence or Underconfidence: Beta Calibration is effective in addressing issues of model overconfidence or underconfidence. Some models may produce predicted probabilities that are too extreme (close to 0 or 1) and do not accurately reflect the true likelihood of events occurring. Beta Calibration can moderate these extreme probabilities to provide more reliable estimates.\nGrowing Research and Tools: As researchers and practitioners have recognized the importance of calibration, there has been an increased focus on developing and providing tools and libraries that support calibration techniques. The availability of software packages like the betacal package has made it easier for data scientists to apply Beta Calibration.\nComparative Studies: Comparative studies and evaluations of different calibration techniques have highlighted the effectiveness of Beta Calibration in certain scenarios. This has contributed to its adoption and prominence.\nVersatility in Calibration Challenges: Beta Calibration can be used in a wide range of applications and is not limited to specific types of models or datasets. Its versatility in addressing calibration challenges makes it a valuable addition to the calibration toolbox.\n\nIn summary, Beta Calibration emerged as a response to the growing need for more advanced and flexible calibration techniques that can handle a variety of machine learning models and data scenarios. Its ability to model predicted probabilities as Beta distributions and address issues like overconfidence or underconfidence has made it a valuable tool for improving the reliability and utility of machine learning models in practical applications.\n\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n# Load the German Credit Data\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\nX_cal, X_test, y_cal, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test)\ny_train[y_train == 2] = 0\ny_test[y_test == 2] = 0\ny_cal[y_cal == 2] = 0\n\n\ny_cal.value_counts()\n\n1    175\n0     75\nName: class, dtype: int64\n\n\n\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import brier_score_loss\nimport matplotlib.pyplot as plt\n\n# Load the German Credit Data\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\nX_cal, X_test, y_cal, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test)\n\n# Convert class labels to binary (0 and 1)\ny_train = (y_train == 1).astype(int)\ny_cal = (y_cal == 1).astype(int)\ny_test = (y_test == 1).astype(int)\n\n\n# Train a classifier (LightGBM in this example)\nclf = lgb.LGBMClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Predict probabilities on the test set\npredicted_probabilities = clf.predict_proba(X_test)[:, 1]\ncal_probas = clf.predict_proba(X_cal)[:, 1] \n\n# Isotonic Calibration\niso_calibrator = IsotonicRegression()\niso_calibrator.fit(cal_probas, y_cal)\niso_calibrated_probabilities = iso_calibrator.predict(cal_probas)\niso_brier = brier_score_loss(y_cal, iso_calibrated_probabilities)\nprint(f'Brier Score after Isotonic Calibration: {iso_brier:.4f}')\n\n# Logistic Calibration\nlogistic_calibrator = LogisticRegression(C=99999999999)\nlogistic_calibrator.fit(cal_probas.reshape(-1, 1), y_cal)\nlogistic_calibrated_probabilities = logistic_calibrator.predict_proba(cal_probas.reshape(-1, 1))[:, 1]\nlogistic_brier = brier_score_loss(y_cal, logistic_calibrated_probabilities)\nprint(f'Brier Score after Logistic Calibration: {logistic_brier:.4f}')\n\n# Beta Calibration\nfrom betacal import BetaCalibration\nbeta_calibrator = BetaCalibration()\nbeta_calibrator.fit(cal_probas, y_cal)\nbeta_calibrated_probabilities = beta_calibrator.predict(cal_probas)\nbeta_brier = brier_score_loss(y_cal, beta_calibrated_probabilities)\nprint(f'Brier Score after Beta Calibration: {beta_brier:.4f}')\n\nBrier Score after Isotonic Calibration: 0.1745\nBrier Score after Logistic Calibration: 0.1876\nBrier Score after Beta Calibration: 0.1827\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\n# Calculate Gini coefficient for Isotonic calibration\ngini_iso = 2 * roc_auc_score(y_cal, iso_calibrated_probabilities) - 1\nprint(f'Gini for Isotonic Calibration: {gini_iso:.4f}')\n\n# Calculate Gini coefficient for Logistic calibration\ngini_logistic = 2 * roc_auc_score(y_cal, logistic_calibrated_probabilities) - 1\nprint(f'Gini for Logistic Calibration: {gini_logistic:.4f}')\n\n# Calculate Gini coefficient for Beta calibration\ngini_beta = 2 * roc_auc_score(y_cal, beta_calibrated_probabilities) - 1\nprint(f'Gini for Beta Calibration: {gini_beta:.4f}')\n\nGini for Isotonic Calibration: 0.5006\nGini for Logistic Calibration: 0.4539\nGini for Beta Calibration: 0.4539\n\n\n\nfrom sklearn.calibration import calibration_curve\n# Generate calibration plots\nplt.figure(figsize=(12, 6))\n\n# Calibration Curve (Isotonic)\nplt.subplot(131)\nfraction_of_positives, mean_predicted_value = calibration_curve(y_cal, iso_calibrated_probabilities, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, marker='o', label='Calibrated Probabilities (Isotonic)')\nplt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve (Isotonic)')\nplt.legend()\n\n# Calibration Curve (Logistic)\nplt.subplot(132)\nfraction_of_positives, mean_predicted_value = calibration_curve(y_cal, logistic_calibrated_probabilities, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, marker='o', label='Calibrated Probabilities (Logistic)')\nplt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve (Logistic)')\nplt.legend()\n\n# Calibration Curve (Beta)\nplt.subplot(133)\nfraction_of_positives, mean_predicted_value = calibration_curve(y_cal, beta_calibrated_probabilities, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, marker='o', label='Calibrated Probabilities (Beta)')\nplt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve (Beta)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Model calibration"
    ]
  },
  {
    "objectID": "machine-learning/modelCalibration.html#brier-score",
    "href": "machine-learning/modelCalibration.html#brier-score",
    "title": "Model calibration",
    "section": "Brier Score",
    "text": "Brier Score\nThe Brier Score, also known as the Brier Loss or Brier’s Probability Score, is a commonly used metric to assess the calibration quality of probabilistic predictions generated by a classification model. The Brier Score quantifies the mean squared difference between predicted probabilities and actual outcomes. The formula for the Brier Score is as follows:\n\\[ \\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^{N} (P_i - O_i)^2 \\]\nWhere: - \\(N\\) is the total number of samples. - \\(P_i\\) is the predicted probability for the positive class (usually class 1) for sample i. - \\(O_i\\) is the actual outcome for sample i, which is binary (1 for a positive outcome, 0 for a negative outcome).\nThe Brier Score ranges from 0 to 1, where lower values indicate better calibration. A Brier Score of 0 means perfect calibration, where the predicted probabilities are identical to the actual outcomes. A Brier Score of 1 indicates the worst calibration, where the predicted probabilities are far from the true outcomes.\nThe Brier Score is used for calibration purposes for the following reasons:\n\nMeasuring Calibration Quality: The Brier Score provides a quantitative measure of how well the predicted probabilities align with the true outcomes. It directly assesses the calibration quality of a model’s probability estimates.\nSensitivity to Miscalibration: It is sensitive to both overconfidence and underconfidence in predicted probabilities. If a model is poorly calibrated, the Brier Score will be high, highlighting the need for calibration.\nComparison of Calibration Techniques: The Brier Score allows for the comparison of different calibration techniques. It can help you evaluate the effectiveness of calibration methods in improving the reliability of probability estimates.\nInterpretability: The Brier Score is easy to understand and interpret. Lower Brier Scores indicate better calibration, making it a useful metric for explaining the quality of probability estimates to stakeholders.\nDirect Application to Decision Thresholds: In binary classification tasks, the Brier Score can assist in selecting appropriate decision thresholds. By analyzing the Brier Score at different threshold values, you can make decisions that balance precision and recall based on the calibration performance.\n\nThresholds for Brier Score:\n\nPerfect Calibration (Threshold: 0): Achieving a Brier Score of 0 represents perfect calibration, where the model’s predicted probabilities precisely match the actual outcomes. While this is the ideal calibration, it is rarely attainable in practice.\nGood Calibration (Threshold: &lt; 0.25): Brier Scores below 0.25 are often regarded as a benchmark for good calibration. In such cases, the predicted probabilities are reasonably aligned with the true outcomes, indicating reliable probability estimates.\nFair Calibration (Threshold: 0.25 - 0.5): Brier Scores falling between 0.25 and 0.5 suggest moderately calibrated models. While not perfect, these models provide fairly reliable probability estimates, making them suitable for various applications.\nPoor Calibration (Threshold: &gt; 0.5): Brier Scores exceeding 0.5 typically indicate poor calibration. Such scores imply that the model’s predicted probabilities are unreliable for decision-making, as they deviate significantly from the actual outcomes.\nDecision-Making Threshold (Application-Dependent): The choice of an acceptable Brier Score threshold depends on the specific application’s criticality and requirements. In critical applications where decisions are heavily based on probabilities, a lower threshold (e.g., 0.1) might be necessary for confidence. In less critical scenarios, a threshold closer to 0.5 may be considered acceptable. The chosen threshold should align with the level of risk tolerance and the consequences of incorrect decisions within the application context.\n\nIn summary, the Brier Score is a valuable metric for evaluating the calibration of probabilistic predictions, making it an essential tool for assessing the reliability of classification models and the effectiveness of calibration techniques. Its simplicity and direct connection to calibration quality make it a widely used metric in machine learning and statistics.",
    "crumbs": [
      "Machine learning",
      "Model calibration"
    ]
  },
  {
    "objectID": "machine-learning/focal-loss.html",
    "href": "machine-learning/focal-loss.html",
    "title": "Focal loss - handle class imbalance",
    "section": "",
    "text": "Hàm mất mát Focal là một sự cải tiến so với hàm mất mát cross-entropy tiêu chuẩn cho phân loại nhị phân và đa lớp. Nó được giới thiệu trong bài báo có tiêu đề “Focal Loss for Dense Object Detection” của Tsung-Yi Lin và cộng sự, và chủ yếu được thiết kế cho các nhiệm vụ phát hiện đối tượng để giải quyết bài toán mất cân bằng giữa các lớp (class imbalance).\nÝ chính đằng sau hàm mất mát Focal là giảm trọng số đóng góp của các ví dụ dễ dàng và tập trung vào những ví dụ khó. Điều này giúp ngăn chặn số lượng lớn các ví dụ tiêu cực dễ dàng từ việc áp đặt lên bộ phát hiện trong quá trình đào tạo.\nCông thức cho hàm mất mát Focal cho phân loại nhị phân là:\n\\[ \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t) \\]\nTrong đó:\n\n\\(p_t\\) là xác suất của lớp đúng (true class). Nếu nhãn lớp đúng là 1, thì \\(p_t\\) là xác suất dự đoán của mô hình cho lớp 1; nếu nhãn lớp đúng là 0, thì $p_t = 1 - $ xác suất dự đoán của mô hình cho lớp 1.\n\\(\\alpha_t\\) là một yếu tố cân bằng. Thông thường đặt nằm giữa 0 và 1. Số này được sử dụng để xử lý mất cân bằng lớp.\n\\(\\gamma\\) là tham số tập trung (focusing parameter) mục đích điều chỉnh mức độ tập trung vào lớp dễ dàng phân loại. Khi \\(\\gamma = 0\\), hàm mất mát Focal tương đương với hàm mất mát cross-entropy. Khi \\(\\gamma\\) tăng, hiệu ứng của yếu tố điều chỉnh trở nên rõ ràng hơn.\n\nLợi thế chính của hàm mất mát Focal là nó đưa ra nhiều trọng số hơn cho các ví dụ bị phân loại sai và ít trọng số hơn cho các ví dụ được phân loại tốt. Điều này giúp trong các tình huống mà một số lớp bị đại diện ít hơn hoặc khi mô hình có khả năng bị áp đặt bởi các ví dụ tiêu cực dễ dàng.\n\n\n\nMục đích: \\(\\alpha\\) được sử dụng trong hàm mất mát Focal để xử lý sự mất cân bằng lớp bằng cách điều chỉnh mất mát cho các lớp dương và âm một cách khác nhau. Nó cung cấp một sự cân bằng giữa tầm quan trọng của lớp dương và lớp âm trong việc tính toán mất mát.\nẢnh hưởng lên Giá trị Mất mát:\n\nĐối với lớp dương (tức là khi nhãn thực \\(y = 1\\)): Giá trị mất mát được nhân với hệ số \\(\\alpha\\).\nĐối với lớp âm (tức là khi nhãn thực \\(y = 0\\)): Giá trị mất mát được nhân với hệ số \\(1 - \\alpha\\).\n\nPhạm vi Giá trị:\n\nThông thường, \\(\\alpha\\) nằm trong khoảng [0, 1].\nGiá trị \\(\\alpha\\) càng gần 1 càng làm cho mất mát lớp dương được tăng cường và mất mát cho lớp âm được giảm đi.\nNgược lại, một giá trị \\(\\alpha\\) gần 0 sẽ nhấn mạnh hơn đến lớp âm.\n\nLợi ích:\n\nBằng cách điều chỉnh \\(\\alpha\\), người ta có thể cung cấp trọng số nhiều hơn cho các lớp được đại diện ít hơn. Điều này có thể đặc biệt hữu ích trong các tình huống có sự mất cân bằng lớp nghiêm trọng, như trong các nhiệm vụ phát hiện đối tượng khi số lượng các ví dụ âm vượt trội so với các ví dụ dương.\nNó đảm bảo rằng mô hình không thiên vị về lớp có nhiều quan sát hơn và xem xét cả hai lớp khi cập nhật trọng số trong quá trình huấn luyện.\n\nThiết lập \\(\\alpha\\):\n\nTrong một số trường hợp, \\(\\alpha\\) có thể được thiết lập dựa trên phân phối lớp nghịch đảo. Ví dụ, nếu 80% các ví dụ là âm và 20% là dương, người ta có thể đặt \\(\\alpha\\) thành 0,2 cho lớp âm và 0,8 cho lớp dương.\nTrong những trường hợp khác, giá trị của \\(\\alpha\\) có thể được xác định thông qua kiểm định chéo (cross-validation) hoặc các phương pháp điều chỉnh tham số khác.\n\n\nTóm lại, tham số \\(\\alpha\\) trong hàm mất mát Focal cung cấp một cơ chế để xử lý sự mất cân bằng lớp bằng cách điều chỉnh mất mát cho các ví dụ dương và âm một cách khác nhau. Nó đảm bảo rằng cả hai lớp chính và phụ đều được đại diện đầy đủ trong quá trình đào tạo của mô hình.\n\n\n\n\\(\\gamma\\) được gọi là “tham số tập trung”. Nó đóng một vai trò quan trọng trong việc xác định mức độ mà mô hình nên tập trung vào các lớp bị phân loại sai so với những lớp được phân loại đúng.\nTác động của nó:\n\nMục đích: Mục đích chính của tham số \\(\\gamma\\) trong hàm mất mát Focal là giảm ảnh hưởng của các lớp dễ phân loại và tăng tầm quan trọng của việc hiệu chỉnh các lớp bị phân loại sai. Điều này đặc biệt hữu ích trong các tình huống mà tập dữ liệu có sự mất cân bằng giữa các lớp.\nTác động của việc Thay đổi \\(\\gamma\\): Thuật ngữ \\((1 - p_t)^\\gamma\\) trong hàm mất mát Focal là yếu tố điều chỉnh. Ở đây, \\(p_t\\) đại diện cho xác suất dự đoán của lớp đúng.\n\nNếu \\(p_t\\) gần bằng 1, nghĩa là ví dụ dễ dàng được phân loại, và \\((1 - p_t)^\\gamma\\) sẽ gần bằng 0, đặc biệt khi \\(\\gamma &gt; 0\\).\nNếu \\(p_t\\) xa 1 (tức là dự đoán là không chính xác hoặc không chắc chắn), thì \\((1 - p_t)^\\gamma\\) sẽ lớn hơn, tăng ảnh hưởng của ví dụ đó lên hàm mất mát.\n\\(\\gamma = 0\\): Hàm mất mát Focal giảm xuống còn bằng hàm mất mát cross-entropy tiêu chuẩn, vì yếu tố điều chỉnh trở thành 1 cho tất cả các ví dụ.\n\\(\\gamma &gt; 0\\): Tăng trọng số của các lớp khó phân loại và giảm trọng số của những lớp dễ phân loại. \\(\\gamma\\) càng lớn, mô hình càng tập trung nhiều vào các lớp khó.\n\nLợi ích: Bằng cách điều chỉnh \\(\\gamma\\), hàm mất mát Focal cho phép các mô hình, đặc biệt trong các nhiệm vụ phát hiện đối tượng, trở nên mạnh mẽ hơn trước số lượng lớn các lớp dễ. Thay vì tiêu tốn tài nguyên tính toán cho các lớp dễ dàng, mô hình tập trung nhiều hơn vào các lớp khó, thường chứa nhiều thông tin hơn.\n\nTóm lại, tham số \\(\\gamma\\) trong hàm mất mát Focal cung cấp một cơ chế để nhấn mạnh việc học từ các lớp bị phân loại sai so với những lớp dễ phân loại. Đó là một công cụ để xử lý sự mất cân bằng lớp và đảm bảo rằng mô hình chú ý nhiều hơn đến các lớp mà nó phân loại sai.",
    "crumbs": [
      "Machine learning",
      "Focal loss - handle class imbalance"
    ]
  },
  {
    "objectID": "machine-learning/focal-loss.html#focal-loss",
    "href": "machine-learning/focal-loss.html#focal-loss",
    "title": "Focal loss - handle class imbalance",
    "section": "",
    "text": "Hàm mất mát Focal là một sự cải tiến so với hàm mất mát cross-entropy tiêu chuẩn cho phân loại nhị phân và đa lớp. Nó được giới thiệu trong bài báo có tiêu đề “Focal Loss for Dense Object Detection” của Tsung-Yi Lin và cộng sự, và chủ yếu được thiết kế cho các nhiệm vụ phát hiện đối tượng để giải quyết bài toán mất cân bằng giữa các lớp (class imbalance).\nÝ chính đằng sau hàm mất mát Focal là giảm trọng số đóng góp của các ví dụ dễ dàng và tập trung vào những ví dụ khó. Điều này giúp ngăn chặn số lượng lớn các ví dụ tiêu cực dễ dàng từ việc áp đặt lên bộ phát hiện trong quá trình đào tạo.\nCông thức cho hàm mất mát Focal cho phân loại nhị phân là:\n\\[ \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t) \\]\nTrong đó:\n\n\\(p_t\\) là xác suất của lớp đúng (true class). Nếu nhãn lớp đúng là 1, thì \\(p_t\\) là xác suất dự đoán của mô hình cho lớp 1; nếu nhãn lớp đúng là 0, thì $p_t = 1 - $ xác suất dự đoán của mô hình cho lớp 1.\n\\(\\alpha_t\\) là một yếu tố cân bằng. Thông thường đặt nằm giữa 0 và 1. Số này được sử dụng để xử lý mất cân bằng lớp.\n\\(\\gamma\\) là tham số tập trung (focusing parameter) mục đích điều chỉnh mức độ tập trung vào lớp dễ dàng phân loại. Khi \\(\\gamma = 0\\), hàm mất mát Focal tương đương với hàm mất mát cross-entropy. Khi \\(\\gamma\\) tăng, hiệu ứng của yếu tố điều chỉnh trở nên rõ ràng hơn.\n\nLợi thế chính của hàm mất mát Focal là nó đưa ra nhiều trọng số hơn cho các ví dụ bị phân loại sai và ít trọng số hơn cho các ví dụ được phân loại tốt. Điều này giúp trong các tình huống mà một số lớp bị đại diện ít hơn hoặc khi mô hình có khả năng bị áp đặt bởi các ví dụ tiêu cực dễ dàng.\n\n\n\nMục đích: \\(\\alpha\\) được sử dụng trong hàm mất mát Focal để xử lý sự mất cân bằng lớp bằng cách điều chỉnh mất mát cho các lớp dương và âm một cách khác nhau. Nó cung cấp một sự cân bằng giữa tầm quan trọng của lớp dương và lớp âm trong việc tính toán mất mát.\nẢnh hưởng lên Giá trị Mất mát:\n\nĐối với lớp dương (tức là khi nhãn thực \\(y = 1\\)): Giá trị mất mát được nhân với hệ số \\(\\alpha\\).\nĐối với lớp âm (tức là khi nhãn thực \\(y = 0\\)): Giá trị mất mát được nhân với hệ số \\(1 - \\alpha\\).\n\nPhạm vi Giá trị:\n\nThông thường, \\(\\alpha\\) nằm trong khoảng [0, 1].\nGiá trị \\(\\alpha\\) càng gần 1 càng làm cho mất mát lớp dương được tăng cường và mất mát cho lớp âm được giảm đi.\nNgược lại, một giá trị \\(\\alpha\\) gần 0 sẽ nhấn mạnh hơn đến lớp âm.\n\nLợi ích:\n\nBằng cách điều chỉnh \\(\\alpha\\), người ta có thể cung cấp trọng số nhiều hơn cho các lớp được đại diện ít hơn. Điều này có thể đặc biệt hữu ích trong các tình huống có sự mất cân bằng lớp nghiêm trọng, như trong các nhiệm vụ phát hiện đối tượng khi số lượng các ví dụ âm vượt trội so với các ví dụ dương.\nNó đảm bảo rằng mô hình không thiên vị về lớp có nhiều quan sát hơn và xem xét cả hai lớp khi cập nhật trọng số trong quá trình huấn luyện.\n\nThiết lập \\(\\alpha\\):\n\nTrong một số trường hợp, \\(\\alpha\\) có thể được thiết lập dựa trên phân phối lớp nghịch đảo. Ví dụ, nếu 80% các ví dụ là âm và 20% là dương, người ta có thể đặt \\(\\alpha\\) thành 0,2 cho lớp âm và 0,8 cho lớp dương.\nTrong những trường hợp khác, giá trị của \\(\\alpha\\) có thể được xác định thông qua kiểm định chéo (cross-validation) hoặc các phương pháp điều chỉnh tham số khác.\n\n\nTóm lại, tham số \\(\\alpha\\) trong hàm mất mát Focal cung cấp một cơ chế để xử lý sự mất cân bằng lớp bằng cách điều chỉnh mất mát cho các ví dụ dương và âm một cách khác nhau. Nó đảm bảo rằng cả hai lớp chính và phụ đều được đại diện đầy đủ trong quá trình đào tạo của mô hình.\n\n\n\n\\(\\gamma\\) được gọi là “tham số tập trung”. Nó đóng một vai trò quan trọng trong việc xác định mức độ mà mô hình nên tập trung vào các lớp bị phân loại sai so với những lớp được phân loại đúng.\nTác động của nó:\n\nMục đích: Mục đích chính của tham số \\(\\gamma\\) trong hàm mất mát Focal là giảm ảnh hưởng của các lớp dễ phân loại và tăng tầm quan trọng của việc hiệu chỉnh các lớp bị phân loại sai. Điều này đặc biệt hữu ích trong các tình huống mà tập dữ liệu có sự mất cân bằng giữa các lớp.\nTác động của việc Thay đổi \\(\\gamma\\): Thuật ngữ \\((1 - p_t)^\\gamma\\) trong hàm mất mát Focal là yếu tố điều chỉnh. Ở đây, \\(p_t\\) đại diện cho xác suất dự đoán của lớp đúng.\n\nNếu \\(p_t\\) gần bằng 1, nghĩa là ví dụ dễ dàng được phân loại, và \\((1 - p_t)^\\gamma\\) sẽ gần bằng 0, đặc biệt khi \\(\\gamma &gt; 0\\).\nNếu \\(p_t\\) xa 1 (tức là dự đoán là không chính xác hoặc không chắc chắn), thì \\((1 - p_t)^\\gamma\\) sẽ lớn hơn, tăng ảnh hưởng của ví dụ đó lên hàm mất mát.\n\\(\\gamma = 0\\): Hàm mất mát Focal giảm xuống còn bằng hàm mất mát cross-entropy tiêu chuẩn, vì yếu tố điều chỉnh trở thành 1 cho tất cả các ví dụ.\n\\(\\gamma &gt; 0\\): Tăng trọng số của các lớp khó phân loại và giảm trọng số của những lớp dễ phân loại. \\(\\gamma\\) càng lớn, mô hình càng tập trung nhiều vào các lớp khó.\n\nLợi ích: Bằng cách điều chỉnh \\(\\gamma\\), hàm mất mát Focal cho phép các mô hình, đặc biệt trong các nhiệm vụ phát hiện đối tượng, trở nên mạnh mẽ hơn trước số lượng lớn các lớp dễ. Thay vì tiêu tốn tài nguyên tính toán cho các lớp dễ dàng, mô hình tập trung nhiều hơn vào các lớp khó, thường chứa nhiều thông tin hơn.\n\nTóm lại, tham số \\(\\gamma\\) trong hàm mất mát Focal cung cấp một cơ chế để nhấn mạnh việc học từ các lớp bị phân loại sai so với những lớp dễ phân loại. Đó là một công cụ để xử lý sự mất cân bằng lớp và đảm bảo rằng mô hình chú ý nhiều hơn đến các lớp mà nó phân loại sai.",
    "crumbs": [
      "Machine learning",
      "Focal loss - handle class imbalance"
    ]
  },
  {
    "objectID": "machine-learning/focal-loss.html#ví-dụ",
    "href": "machine-learning/focal-loss.html#ví-dụ",
    "title": "Focal loss - handle class imbalance",
    "section": "Ví dụ",
    "text": "Ví dụ\n\nTunning alpha and gamma\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nfrom lightgbm.callback import record_evaluation\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# Generate imbalanced data using make_classification\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, weights=[0.99, 0.01], random_state=42)  # 99% of class 0 and 1% of class 1\nX_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n# Define Focal Loss for LightGBM\ndef focal_loss_lgb(y_pred, dtrain, alpha, gamma):\n    a, g = alpha, gamma\n    y_true = dtrain.get_label()\n    p = 1 / (1 + np.exp(-y_pred))\n    loss = -(a * y_true + (1 - a) * (1 - y_true)) * ((1 - (y_true * p + (1 - y_true) * (1 - p))) ** g) * (y_true * np.log(p) + (1 - y_true) * np.log(1 - p))\n    return 'focal_loss', np.mean(loss), False\n\n# Gini coefficient\ndef gini(y_true, y_pred):\n    return 2 * roc_auc_score(y_true, y_pred) - 1\n\n# Optuna study for tuning alpha and gamma\ndef objective(trial):\n    # Parameters to be tuned\n    alpha = trial.suggest_float('alpha', 0.01, 1)\n    gamma = trial.suggest_float('gamma', 0.1, 5)\n    \n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val, reference=train_set)\n\n    param = {\n        'objective': 'binary',\n        'metric': 'custom',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n\n    # Using record_evaluation to capture validation results without printing\n    evals_result = {}\n    model = lgb.train(param, \n                      train_set, \n                      valid_sets=[val_set], \n                      feval=lambda preds, dtrain: focal_loss_lgb(preds, dtrain, alpha, gamma), \n                      callbacks=[record_evaluation(evals_result)])\n    \n    preds = model.predict(X_val)\n    return gini(y_val, preds)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100) \n\n# Results\nbest_params = study.best_params\nbest_score = study.best_value\n\n# Plotting\nalphas = [trial.params['alpha'] for trial in study.trials]\ngammas = [trial.params['gamma'] for trial in study.trials]\nscores_focal = [trial.value for trial in study.trials]\nplt.figure(figsize=(12, 5))\nplt.scatter(alphas, scores_focal, color='blue', label='Alpha')\nplt.scatter(gammas, scores_focal, color='red', label='Gamma')\nplt.xlabel('Parameter Value')\nplt.ylabel('Gini Score')\nplt.title('Effect of Alpha and Gamma on Gini Score')\nplt.legend()\nplt.show()\n\nprint(f\"Best Gini score: {best_score}\")\nprint(f\"Best parameters: {best_params}\")\n\n\n\n\n\n\n\n\nBest Gini score: 0.7631133671742809\nBest parameters: {'alpha': 0.45019195857112326, 'gamma': 0.4919531053413074, 'lambda_l1': 6.640753524365382, 'lambda_l2': 9.081726977645413, 'num_leaves': 21, 'feature_fraction': 0.9362220967011345, 'bagging_fraction': 0.4744110361566192, 'bagging_freq': 2, 'min_child_samples': 12}\n\n\n\n# Optuna study for tuning alpha and gamma\ndef objective(trial):\n    \n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val, reference=train_set)\n\n    params_ce = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n\n    # Using record_evaluation to capture validation results without printing\n    evals_result_ce = {}\n    model_ce = lgb.train(params_ce, train_set, valid_sets=[val_set], callbacks=[record_evaluation(evals_result_ce)])\n    preds_ce = model_ce.predict(X_val)    \n    return gini(y_val, preds_ce)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100) \n\n# Results\nbest_params = study.best_params\nbest_score = study.best_value\nscores_ce = [trial.value for trial in study.trials]\nprint(f\"Best Gini score: {best_score}\")\nprint(f\"Best parameters: {best_params}\")\n\nBest Gini score: 0.7445008460236886\nBest parameters: {'lambda_l1': 5.045686905737897, 'lambda_l2': 8.682377409692112, 'num_leaves': 158, 'feature_fraction': 0.7953399252370615, 'bagging_fraction': 0.43225443468376873, 'bagging_freq': 3, 'min_child_samples': 16}\n\n\n==&gt; Sử dụng focal loss sau 100 lần thử thì tìm được mô hình có gini tốt hơn so với sử dụng cross entropy\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(scores_ce, label='Cross-Entropy', color='red')\nplt.plot(scores_focal, label='Focal Loss', color='blue')\nplt.xlabel('Trial')\nplt.ylabel('ROC AUC Score')\nplt.title('Comparison of Focal Loss and Cross-Entropy over Trials')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Initialize lists to store results\nfocal_scores = []\nce_scores = []\nimbalance_ratios = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n\nfor ratio in imbalance_ratios:\n    # Generate imbalanced data\n    X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, weights=[1-ratio, ratio], random_state=42)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n    \n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val, reference=train_set)\n    \n    # Train with Focal Loss\n    params_focal = {\n        'objective': 'binary',\n        'metric': 'custom',\n        'boosting_type': 'gbdt'\n    }\n    \n    alpha_value = 0.25\n    gamma_value = 2.0\n    evals_result_focal = {}    \n    model_focal = lgb.train(params_focal, train_set, valid_sets=[val_set], \n                        feval=lambda preds, dtrain: focal_loss_lgb(preds, dtrain, alpha_value, gamma_value), \n                        callbacks=[record_evaluation(evals_result_focal)])\n    preds_focal = model_focal.predict(X_val)\n    roc_focal = roc_auc_score(y_val, preds_focal)\n    \n    # Train with Cross-Entropy\n    params_ce = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting_type': 'gbdt'\n    }\n    \n    evals_result_ce = {} \n    model_ce = lgb.train(params_ce, train_set, valid_sets=[val_set], callbacks=[record_evaluation(evals_result_ce)])\n    preds_ce = model_ce.predict(X_val)\n    roc_ce = roc_auc_score(y_val, preds_ce)\n    \n    # Store results\n    focal_scores.append(roc_focal)\n    ce_scores.append(roc_ce)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(imbalance_ratios, focal_scores, marker='o', label='Focal Loss', color='b')\nplt.plot(imbalance_ratios, ce_scores, marker='x', label='Cross-Entropy Loss', color='r')\nplt.xlabel('Imbalance Ratio')\nplt.ylabel('ROC AUC Score')\nplt.title('Effect of Loss Type on Imbalanced Data')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n[LightGBM] [Info] Number of positive: 11, number of negative: 789\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.013750 -&gt; initscore=-4.272871\n[LightGBM] [Info] Start training from score -4.272871\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 11, number of negative: 789\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.013750 -&gt; initscore=-4.272871\n[LightGBM] [Info] Start training from score -4.272871\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 43, number of negative: 757\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.053750 -&gt; initscore=-2.868163\n[LightGBM] [Info] Start training from score -2.868163\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 43, number of negative: 757\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.053750 -&gt; initscore=-2.868163\n[LightGBM] [Info] Start training from score -2.868163\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 82, number of negative: 718\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102500 -&gt; initscore=-2.169750\n[LightGBM] [Info] Start training from score -2.169750\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 82, number of negative: 718\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000299 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102500 -&gt; initscore=-2.169750\n[LightGBM] [Info] Start training from score -2.169750\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 161, number of negative: 639\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.201250 -&gt; initscore=-1.378500\n[LightGBM] [Info] Start training from score -1.378500\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 161, number of negative: 639\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000302 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.201250 -&gt; initscore=-1.378500\n[LightGBM] [Info] Start training from score -1.378500\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 239, number of negative: 561\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298750 -&gt; initscore=-0.853257\n[LightGBM] [Info] Start training from score -0.853257\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 239, number of negative: 561\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000259 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298750 -&gt; initscore=-0.853257\n[LightGBM] [Info] Start training from score -0.853257\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 318, number of negative: 482\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397500 -&gt; initscore=-0.415893\n[LightGBM] [Info] Start training from score -0.415893\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 318, number of negative: 482\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000370 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397500 -&gt; initscore=-0.415893\n[LightGBM] [Info] Start training from score -0.415893\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 396, number of negative: 404\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495000 -&gt; initscore=-0.020001\n[LightGBM] [Info] Start training from score -0.020001\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 396, number of negative: 404\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495000 -&gt; initscore=-0.020001\n[LightGBM] [Info] Start training from score -0.020001\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n\n\n\n\n\n\n\n\n==&gt; Sử dụng \\(\\alpha\\) và \\(\\gamma\\) chưa tunning trên nhiều kịch bản imbalance khác nhau, gini không có sự khác biệt",
    "crumbs": [
      "Machine learning",
      "Focal loss - handle class imbalance"
    ]
  },
  {
    "objectID": "explore.html",
    "href": "explore.html",
    "title": "Explore and setup",
    "section": "",
    "text": "With this tutorial, we have a working example website that we will explore together. We’ll learn a few rules and look for patterns to get an understanding of what things to do to help you start customizing and making it your own. And you can continue to use this website as a reference after the tutorial, along with Quarto documentation.\nWe’ll start our exploration online looking at the website architecture and GitHub repository. Then we’ll setup a copy for ourselves so that we can modify from a working example, which is a great way to learn something new. We’ll set it up so that any modifications (commits) will automatically be republished via GitHub Actions. Subsequent chapters will describe how to modify your repo using different tools (browser, RStudio, Jupyter).",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#exploring-online",
    "href": "explore.html#exploring-online",
    "title": "Explore and setup",
    "section": "Exploring online",
    "text": "Exploring online\n\nThe website itself\nThis website has 5 things you can see on the left sidebar:\n\nWelcome\nExploring and setup\nQuarto workflows\nLearning more\nTransition from Rmd\n\nMost of these are pages, but you’ll see that “Quarto Workflows” has an arrow; it is a folder with additional pages inside.\n\n\nThe website’s repo\nLet’s go to this website’s GitHub repository (also called a “repo”), https://github.com/openscapes/quarto-website-tutorial. You can also click there from any page in this tutorial website by clicking the GitHub octocat icon underneath the Openscapes logo in the left navbar (click it holding command on Mac, or control on a PC to open it in a different tab in your browser).\nHave a look at the filenames. We can recognize the names of the webpages we’ve seen above, and they have red arrows marking them in the image below. You’ll see the “quarto-workflows” folder and the rest in this site are .qmd files, which are plain text Quarto files that can combine Markdown text with code. index.qmd is the home page. If you click inside “quarto-workflows” you’ll see a mix of filetypes!\n\n\n\nquarto-website-tutorial GitHub repository with files for webpages marked with red arrows\n\n\nThe _site folder has html files with names that should be familiar: they match the .md files we were just exploring. This folder is where Quarto stores files to build the website.",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#quarto.yml-intro",
    "href": "explore.html#quarto.yml-intro",
    "title": "Explore and setup",
    "section": "_quarto.yml intro",
    "text": "_quarto.yml intro\nThere is also a _quarto.yml file, which is the website’s configuration file. It is essentially metadata for the website that includes the order that the pages/chapters will be in. This is where you update the organization of your website: which page comes before another. If we compare side-by-side, you’ll see that the pages that appear on our website are listed there.\n\n\n\n_quarto.yml and website side-by-side\n\n\nWe’ll learn more about how to interact with _quarto.yml in Quarto Workflows.",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#fork-to-your-account",
    "href": "explore.html#fork-to-your-account",
    "title": "Explore and setup",
    "section": "Fork to your account",
    "text": "Fork to your account\nLet’s start with an existing Quarto site and copy it into your space to edit. You’ll need a free GitHub account that you create at github.com (follow this advice about choosing your username).\nFirst, choose an existing website to copy. The simplest option is to start with this site: quarto-website-tutorial.\nOther options of potential interest:\n\n2021-Cloud-Hackathon\n2022-SWOT-Ocean-Cloud-Workshop\nOpenscapes Approach-Guide\n\nNext, follow these steps to fork and setup your repo with GitHub Actions from Gavin Fay, using the repo you chose. These instructions will take ~5 minutes.\nNow you’ve got a copy of your repo of choice in your own GitHub account, and you’re set to start making your own edits. Your GitHub repo is set up with a GitHub Action that will use Quarto to rebuild and republish your site anytime you make a commit: committing will trigger the GitHub Action to rebuild and republish the book.\nNote that the GitHub Action for this book does not include R or Python so those will need to be added if your website relies on code. See https://github.com/r-lib/actions for more details and examples.\n\nDownload instead of fork\nForking might not always be the way to go - you can’t fork into the same GitHub user account or organization so if for example you want to make a copy of 2021-Cloud-Hackathon repo within the same NASA-Openscapes GitHub Organization, you’ll need to download instead of fork. In this case, follow these steps to download and copy into a new repository, and set up the GitHub Action separately.\n\nDownload github repo files\nNavigate to https://github.com/openscapes/quarto-website-tutorial (or any other quarto site repo of choice). Click the green “Code” button and select “Download ZIP”. When it downloads on your computer, unzip the files.\n\n\nCreate a new GitHub repo\nNavigate to your GitHub account or organization, and create a new repository, naming it what you’d like. You’ll need a free GitHub account that you create at github.com (follow this advice about choosing your username). When you’re logged in, github.com will show a green button that says “New” which you’ll also see as you navigate to your username’s repository page.\n\n\nAdd original site files\nTo use the GitHub file uploader, click the button next to the green “Code” button that says “Add file”. Add file &gt; Upload files. Then, on your computer, select all the files in unzipped folder (command-A or control-A), and drag them to the GitHub uploader page. Scroll down to write a commit message, which effectively saves your files when you’re working in the browser.\nNote: if you’re comfortable cloning the new repository and copying files into it locally before committing and pushing back to GitHub, that can be preferable to the uploader, which does have limitations with complex repos (although the uploader works fine with this tutorial repo).",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#setup-github-action",
    "href": "explore.html#setup-github-action",
    "title": "Explore and setup",
    "section": "Set up GitHub publishing",
    "text": "Set up GitHub publishing\nIf you’ve used the GitHub uploader, you’ll need to set up GitHub publishing separately. We’ll do this in a few steps: we’ll set up a GitHub Action within your repo, and create a gh-pages branch.\nFirst, the GitHub Action. Go back to your main view of your GitHub repository by clicking on the name of your repository in blue at the top-left (the url in your browser window should say https://github.com/username/repo-name).\nNext to the green code button, click Add file &gt; Create new file. Name it exactly this: .github/workflows/quarto-publish.yml . In detail: start by typing the . with github and when you type the / it will give you a new text box to type workflows (plural!), then another /, and finally, quarto-publish.yml.\nNow you’ll have an empty new file. Paste the following in this empty file - you can click on the top-right of this box to copy all the code inside this code box:\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      # add software dependencies here\n\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # this secret is always available for github actions\nCommit this to save your new quarto-publish.yml file. This is your GitHub Action.\nNext, we’ll create a new gh-pages branch. Go back to the main view of your GitHub repository. On the far left from the green “Code” button, click the button that says “main”. In the pull-down menu, type gh-pages - all lowercase, with a hyphen. Click the bold text that says “Create branch: gh-pages from main”.\nNow click on the Settings tab in the top right of your repository. On the left sidebar, click Pages. At the top of Pages under “Source”, select gh-pages root, and press Save. You’ll then see a green highlighted text saying that your site is published at a “github.io” url.",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#confirm",
    "href": "explore.html#confirm",
    "title": "Explore and setup",
    "section": "Confirm your website is published",
    "text": "Confirm your website is published\nTo confirm that your website is published, go back to your main repository page. You’ll now see an orange dot showing that the GitHub Action is beginning to publish the page.\n\n\n\nOur repo with orange dot indicating in-progress GitHub Action build\n\n\nIf you do not see this orange dot, you might need to make a small commit to trigger the GitHub Actions build. If this is the case, click the pencil on the top-right of the README.md file as circled in the image below, add some small edit (like a space after a period), and scroll down to click commit. Now you should see the orange dot.\n\n\n\n\n\nWhen your orange do becomes a green check, you can go inspect your published site at “https://username.github.io/your-repo). For example: https://openscapes.github.io/quarto-website-tutorial.\n\n\n\nOur repo with green check indicating successful GitHub Action build",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#renaming-your-repo",
    "href": "explore.html#renaming-your-repo",
    "title": "Explore and setup",
    "section": "Renaming your repo",
    "text": "Renaming your repo\nIf you’d like to rename your repo, go to Settings and the option to rename is on the top of the main settings page.",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#onward",
    "href": "explore.html#onward",
    "title": "Explore and setup",
    "section": "Onward!",
    "text": "Onward!\nNow you are ready to start editing and publishing! The next chapter describes how starting off from the browser, using Markdown.",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "quarto-workflows/index.html",
    "href": "quarto-workflows/index.html",
    "title": "Quarto workflows",
    "section": "",
    "text": "How do you work in Quarto? You can use whichever tool you’re comfortable with (RStudio, Jupyter, GitHub, VS Code, etc). Developing your quarto site will have the same basic workflow, no matter which tool you use. It is very iterative, and each is explored more below.\n\nAuthoring: write text, code, images, etc in a file. Supported files include .md, .Rmd, .qmd, .ipynb…\nUpdate _quarto.yml as needed (for example, if you’ve created a new file you’d like included in your site)\nRender individual files and/or the whole website\nRepeat, repeat, repeat\nCommit and push your website to GitHub, your updates will publish automatically!\nRepeat all of the above to make the website as you’d like!\n\nNote: if editing from your internet browser we won’t render in Step 3. That step will not be separate, but combined with Step 5, which will only require a commit, not a push.",
    "crumbs": [
      "Quarto workflows"
    ]
  },
  {
    "objectID": "quarto-workflows/index.html#basic-workflow",
    "href": "quarto-workflows/index.html#basic-workflow",
    "title": "Quarto workflows",
    "section": "",
    "text": "How do you work in Quarto? You can use whichever tool you’re comfortable with (RStudio, Jupyter, GitHub, VS Code, etc). Developing your quarto site will have the same basic workflow, no matter which tool you use. It is very iterative, and each is explored more below.\n\nAuthoring: write text, code, images, etc in a file. Supported files include .md, .Rmd, .qmd, .ipynb…\nUpdate _quarto.yml as needed (for example, if you’ve created a new file you’d like included in your site)\nRender individual files and/or the whole website\nRepeat, repeat, repeat\nCommit and push your website to GitHub, your updates will publish automatically!\nRepeat all of the above to make the website as you’d like!\n\nNote: if editing from your internet browser we won’t render in Step 3. That step will not be separate, but combined with Step 5, which will only require a commit, not a push.",
    "crumbs": [
      "Quarto workflows"
    ]
  },
  {
    "objectID": "quarto-workflows/index.html#authoring",
    "href": "quarto-workflows/index.html#authoring",
    "title": "Quarto workflows",
    "section": "Authoring",
    "text": "Authoring\nAs an author, you have a lot of options of how your text will be formatted, arranged, and interlinked. You will be writing in Markdown, which is a lightweight text formatting language. The Quarto documentation about authoring introduces markdown-basics that will get you started. Also see Mine Çetinkaya-Rundel’s A Quarto tip a day.\nEach page of our site has a similar first few lines - this YAML, like we saw in our _quarto.yml and it is indicated by two sets of 3 dashes --- :\n---\ntitle: My title\n---\nYou’re able to add more features to individual pages by including it in the YAML, which for the most part here only includes a title. See Quarto excecution options for more information of what you can include in the YAML.",
    "crumbs": [
      "Quarto workflows"
    ]
  },
  {
    "objectID": "quarto-workflows/index.html#update-_quarto.yml",
    "href": "quarto-workflows/index.html#update-_quarto.yml",
    "title": "Quarto workflows",
    "section": "Update _quarto.yml",
    "text": "Update _quarto.yml\nLet’s have a closer look at the _quarto.yml file.\nThis type of file (.yml or .yaml) is written in YAML (“Yet Another Markup Language”). You’ll be able to shift the arrangement of webpages by reordering/adding/deleting them in the _quarto.yml file following the patterns you see in this example.\n\n\n\n_quarto.yml and website side-by-side\n\n\nNotice that there are multiple ways in the _quarto.yml for you to include a file in your website. For example, in the above image, the “First Observations” we see in the left sidebar of the published website (right image) is represented in _quarto.yml (left image) over two lines, with line 36 indicating the file reference and line 37 indicating the text to show up in the left sidebar. However, “From RStudio” is only represented in one line of _quarto.yml, on line 43. This represents two strategies for including a file in your website. By default, the title of a specified file will show up in the website’s sidebar, which is what is happening with the “From RStudio” example. If you would like more control over what is written in the sidebar vs the title of your files, then the approach we took with “First Observations” is what you’ll want to do: you’ll see that only “First Observations” shows up in the sidebar as we specified in _quarto.yml, but the page’s title says “First Observations & Setup” (which in our preference was too long for the sidebar).\n\n\n\n\n\n\nNote\n\n\n\nAs you modify _quarto.yml, the most important thing to know is that spacing matters. Pay attention to whether text is indented by one, two, four, or other spaces, and make sure you follow it; if your site is not looking as expected it is likely a silent error in your YAML. Some text editors like RStudio provide debugging support for YAML and are highly recommended to save you time and heartache.",
    "crumbs": [
      "Quarto workflows"
    ]
  },
  {
    "objectID": "quarto-workflows/index.html#install-quarto",
    "href": "quarto-workflows/index.html#install-quarto",
    "title": "Quarto workflows",
    "section": "Install Quarto",
    "text": "Install Quarto\nhttps://quarto.org/docs/get-started/ describes how to install Quarto, which will depend on your operating system. We’ll walk through installation for each tool in the next chapters.",
    "crumbs": [
      "Quarto workflows"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html",
    "href": "quarto-workflows/jupyter.html",
    "title": "From Jupyter",
    "section": "",
    "text": "You can interact with Quarto through JupyterLab or JupyterHub. Your Jupyter setup will involve .ipynb notebooks and the command line. Quarto’s JupyterLab tutorials has great instructions on getting started with JupyterLab, including computations and authoring.\nHere we will demonstrate how to work with this Quarto tutorial site in JupyterHub and add a Jupyter Notebook (.ipynb file). This example uses the NASA-Openscapes JupyterHub that already has all python environments as well as Quarto installed.",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#setup",
    "href": "quarto-workflows/jupyter.html#setup",
    "title": "From Jupyter",
    "section": "Setup",
    "text": "Setup\n\nJupyterHub\nOur JupyterHub is already setup with python environments as well as Quarto (through nasa-openscapes/corn), so there is no further installation required.\n\n\nClone your repo\nYou’ll start by cloning your repository into JupyterHub. Do this by opening a terminal (File &gt; New &gt; Terminal). In the Terminal, git clone your repository and cd into it:\ngit clone https://github.com/openscapes/quarto-website-tutorial\ncd quarto-website-tutorial\n\n\nInstall Quarto\nNot needed - Quarto is already installed on the NASA-Openscapes JupyterHub! But to install elsewhere you would do so from https://quarto.org/docs/get-started/.\nQuarto is a Command Line Interface (CLI), like git. Once download is complete, follow the installation prompts on your computer like you do for other software. You won’t see an application to click on when it is installed.\nNote for Mac users: If you do not have administrative privileges, please select “Install for me only” during the Destination Selection installation step (you will first click on “Change Install Location” at the Installation Type step).\nYou can check to confirm that Quarto is installed properly from the command line:\nquarto check install\n\n\n\n\n\n\nAdditional checks\n\n\n\n\n\nYou can also run:\n\nquarto check knitr to locate R, verify we have the rmarkdown package, and do a basic render\nquarto check jupyter to locate Python, verify we have Jupyter, and do a basic render\nquarto check to run all of these checks together\n\n\n\n\n\n\n\n\n\n\nHistorical aside: Install Quarto in a docker container\n\n\n\n\n\nIn Summer 2021 some NASA Mentors trying to install quarto locally was not an option, but they were able to install it inside a container using the following Dockerfile:\n#| fold: true\n#| summary: \"Show the Dockerfile\"\n\n##############################\n# This Dockerfile installs quarto and then runs quarto serve against the\n# internal /home/quarto/to_serve.\n#\n# BUILD\n# -----\n# To build this container, run\n#\n#     docker build -t quarto_serve .\n#\n# Add the --no-cache option to force docker to build fresh and get the most\n# recent version of quarto.\n#\n#\n# RUN\n# ---\n# 1. Find the directory you want quarto to serve. Let's call this /PATH/TO/earthdata-cloud-cookbook.\n# 2. Run docker:\n#\n#     docker run --rm -it -p 4848:4848 -v /PATH/TO/earthdata-cloud-cookbook:/home/quarto/to_serve quarto_serve\n#\n# 3. Open your browser and go to http://127.0.0.1:4848/\n#\n##############################\n\nFROM ubuntu:hirsute\n\n######\n# Install some command line tools we'll need\n######\nRUN apt-get update\nRUN apt-get -y install wget\nRUN apt-get -y install gdebi-core\nRUN apt-get -y install git\n\n\n######\n# Install quarto (https://quarto.org/)\n######\n\n# This is a quick and dirty way of getting the newest version number from\n# https://github.com/quarto-dev/quarto-cli/releases/latest. What's happening is\n# we're pulling the version number out of the redirect URL. This will end up\n# with QVER set to something like 0.2.11.\nRUN QVER=`wget --max-redirect 0 https://github.com/quarto-dev/quarto-cli/releases/latest 2&gt;&1 | grep \"Location\" | sed 's/L.*tag\\/v//' | sed 's/ .*//'` \\\n    && wget -O quarto.deb \"https://github.com/quarto-dev/quarto-cli/releases/download/v$QVER/quarto-$QVER-amd64.deb\"\nRUN gdebi -n quarto.deb\n\n# Run this to make sure quarto installed correctly\nRUN quarto check install\n\n\n######\n# Create a non-root user called quarto\n######\nRUN useradd -ms /bin/bash quarto\nUSER quarto\nRUN mkdir /home/quarto/to_serve\nWORKDIR /home/quarto/to_serve\n\n\n######\n# Start quarto serve\n######\n\nCMD quarto serve --no-browse --host 0.0.0.0 --port 4848",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#quarto-preview",
    "href": "quarto-workflows/jupyter.html#quarto-preview",
    "title": "From Jupyter",
    "section": "Quarto preview",
    "text": "Quarto preview\nLet’s start off by previewing our quarto site locally. In Terminal, type quarto preview, which will provide a URL with a preview of our site!\nquarto preview\n# Preparing to preview\n# Watching files for changes\n# Browse at https://openscapes.2i2c.cloud/user/jules32/proxy/4593/\nCopy this URL into another browser window; and arrange them so you can see them both. I make a bit more space in Jupyter by collapsing the left file menu by clicking on the file icon at the top of the left sidebar.\n\n\n\n\n\n\nMake a small change and preview it\nNow we’ll be able to see live changes in the preview as we edit in our .md files. Let’s try it: Change the date in index.md by opening it from the file directory. Change to today’s date, and save. Your preview window will refresh automatically! If it does not, you can also refresh the page manually. The refreshed previewed site will now display your changes!",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#create-a-new-.ipynb-page",
    "href": "quarto-workflows/jupyter.html#create-a-new-.ipynb-page",
    "title": "From Jupyter",
    "section": "Create a new .ipynb page",
    "text": "Create a new .ipynb page\nLet’s add a new page to our site. Instead of an .md file like the others, let’s add a .ipynb file.\nFile &gt; New &gt; Notebook. Accept the default kernel by clicking Select.\n\nFirst chunk: raw yaml\nBy default, this Notebook will give us a first chunk that is code. Let’s change it to raw so that we can write our yaml at the top.\n\n\n\n\n\nIn our Raw code chunk, let’s write the title of this document. We need three dashes --- on separate lines preceding and following the title:, which you can name as you’d like.\n---\ntitle: Python Example\n---\n\n\nSecond chunk: Markdown\nLet’s add a new chunk that is Markdown so we can write some description of what this page will be.\nClick the + symbol at the top of the document, and this will add a new chunk, which by default again is a Code chunk. Change it to a Markdown Chunk following the steps we did above when switching to Raw.\nHere, write a little bit of text in Markdown. Since your title is effectively a level-1 header, avoid using level-1 headers in the rest of your document. Here is some example text I wrote:\n## Introduction\n\nThis example has some Python code that will be a part of our Quarto site.\n\n\nThird chunk: Code\nNow let’s create a new chunk with the default Code setting.\nPaste the following code (or write some of your own to test):\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\nNow, go ahead and execute this code chunk like you normally would, by clicking the cursor in a code block and clicking the sideways “play” triangle to run the selected cells (and advance to the next cell). This code produces a plot.\nNote that the code runs as it normally would; the code options in the comments are just comments.\n\n\nSave your file\nSave your document - I’ll call mine python-example.ipynb in the main repository.",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#update-_quarto.yml",
    "href": "quarto-workflows/jupyter.html#update-_quarto.yml",
    "title": "From Jupyter",
    "section": "Update _quarto.yml",
    "text": "Update _quarto.yml\nNow we’ll add python-example.ipynb to our _quarto.yml file; this is where we register of all files to include in our site. Let’s add it after the section called “Basic Workflows”.\nOpen _quarto.yml by clicking on it from the file directory.\nScroll down to review the current contents in the sidebar: section. It’s there we see all the file arrangement that we see in the previewed site.\nAdd - python-example.ipynb to line 46, making sure that your indentation aligns with the other pages.\n\n\n\n\n\nYou’ll see that our new page shows up in our Preview, and the code is executed since we did that in the Jupyter Notebook itself. By default, Quarto will not execute code chunks since your computations will likely become more complex and you will want to control when they are executed (or “run”).\nSince Quarto is still previewing our website and the python-example.ipynb, the plot also displays in the notebook after the code is run and the file is saved, as shown below.\n\n\n\n\n\nSo, your normal workflow for creating and running code blocks in your Jupyter Notebook is the same one you’ll use as Quarto displays the preview.",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#quarto-render",
    "href": "quarto-workflows/jupyter.html#quarto-render",
    "title": "From Jupyter",
    "section": "Quarto render",
    "text": "Quarto render\nSo far we have used Quarto preview to view our website as we develop it. Quarto render will build the html elements of the website that we can see when we preview. Rendering will format the markdown text and code nicely as a website (or however is indicated in the _quarto.yml).\nBy default, Quarto render does not execute code in a Jupyter notebook. It will never run .ipynb files unless you tell it to.\n\nRender whole notebook\nIf you would like it to specifically execute code in a Jupyter notebook, you can do so in Terminal.\nOur Terminal is still busy previewing our website, so let’s open a new Terminal.\nFile &gt; New &gt; Terminal. Then type:\ncd quarto-website-tutorial\nquarto render python-example.ipynb --execute",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#authoring-tips",
    "href": "quarto-workflows/jupyter.html#authoring-tips",
    "title": "From Jupyter",
    "section": "Authoring tips",
    "text": "Authoring tips\nQuarto.org has details about authoring, including specific instructions about authoring in Jupyter: quarto.org/docs/reference/cells/cells-jupyter.",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#commit-and-push",
    "href": "quarto-workflows/jupyter.html#commit-and-push",
    "title": "From Jupyter",
    "section": "Commit and push!",
    "text": "Commit and push!\nCommitting and pushing will make the changes you see locally live on your website (using the GitHub Action we set up earlier).",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#troubleshooting",
    "href": "quarto-workflows/jupyter.html#troubleshooting",
    "title": "From Jupyter",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nMy changes don’t show up in preview\nMake sure you’ve saved your file! There might be a slight delay depending on your JupyterHub/Lab setup.\n\n\nQuarto render hangs / does not complete\nCheck the specific notebook, are there any `—` throughout to denote line breaks rather than yaml? They might be causing the issue; consider deleting those.\nAlso check how long the first raw cell is. Are there level-1 headers (#)? Try removing them.",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html",
    "href": "quarto-workflows/rstudio.html",
    "title": "From RStudio",
    "section": "",
    "text": "The RStudio software (called an IDE, integrated development environment) is an excellent way to edit files and interface with GitHub. Plus, as it is made by the same folks who make Quarto, it has many integrated features for streamlining your workflow with Quarto, including how it previews your edits and provides debugging support for yaml! Quarto's RStudio tutorials has great instructions on getting started with RStudio, including computations and authoring.\nHere is what you’ll need to do to set up and use RStudio with Quarto.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#setup",
    "href": "quarto-workflows/rstudio.html#setup",
    "title": "From RStudio",
    "section": "Setup",
    "text": "Setup\n\nRStudio and GitHub\nFor a workflow with RStudio and GitHub on your local computer, you will need four things:\n\nR\nRStudio\nGit\nGitHub\n\nFollow the UCSB MEDS Installation Guide for detailed instructions on how to create accounts, download, install, and configure on Mac and Windows. This takes about 20 minutes. (For an even more detailed walk-through, see Allison Horst’s ESM 206 Google Doc).\n\n\nClone your repo\nYou’ll start by cloning your repository into RStudio.\nFile &gt; New Project &gt; Version Control &gt; Git &gt; paste your repository name.\nR for Excel Users: Clone your repository using RStudio has detailed instructions and screenshots of these steps.\n\n\nInstall Quarto\nNext, you’ll install Quarto: https://quarto.org/docs/get-started/. After downloading, follow the installation wizard on your computer. When it is complete, you won’t see an application or any new software, but it is now available to RStudio (as well as all other applications on your computer, including the command line).\n\n\nRStudio orientation\nNow let’s take a moment to get oriented. This is an RStudio project, which is indicated in the top-right. The bottom right pane shows all the files in your project; everything we’ve cloned from GitHub. We can open any RStudio project by opening its .Rproj file, or from RStudio File &gt; Open Project ….\n\n\n\nRStudio IDE highlighting the project name and files pane\n\n\n\n\nVisual Editor\nThe RStudio Visual Editor is quite new and has features that improve your writing experience. Working in the Visual Editor feels a bit like working in a Google Doc.\nHere’s an example showing the same file in the original Source Editor with content in markdown format and in the Visual Editor with content that looks more like it will appear in a live site. You can switch freely between these modes.\n\n\n\n\n\n\n\n\n\nRStudio IDE highlighting the Source Editor\n\n\n\n\n\n\n\nRStudio IDE highlighting the Visual Editor\n\n\n\n\n\nAlready have some content formatted in a Google Doc? You can copy-paste it into the Visual Editor and most formatting will be retained.\nThe editing bar provides familiar point and click access to text formatting options like bulleted or numbered lists.\n\n\n\nRStudio IDE highlighting the point and click editing bar\n\n\n\nKeyboard shortcuts\nThe Visual Editor also lets you use many keyboard shortcuts that might be familiar for adding boldface (command-b), italics (command-i), or headers. On a Mac, option-command-2 will make a level 2 header. Try it with option-command-1, or option-command-0 for normal text!\n\n\nInsert an image or figure\nTo insert an image (called a figure in Quarto), click the image icon. This brings up a window in which we can select the image, set its alignment, give it a caption and alt text, hyperlink it, or edit other metadata.\n\n\n\nInsert image or figure using the Visual Editor\n\n\nOnce an image is added, clicking on that image gives us editing options. We can resize it dynamically by clicking in the image and dragging a corner or side to resize. When an image is selected, its dimensions are displayed for editing. Clicking on the gray ellipsis to the right of the dimensions opens the pop-up window to access more metadata edits.\n\n\nInsert a table\nSimilar to adding an image, to insert a table, we click the Table dropdown.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#quarto-render",
    "href": "quarto-workflows/rstudio.html#quarto-render",
    "title": "From RStudio",
    "section": "Quarto render",
    "text": "Quarto render\nIn the Build tab in the top-right pane, click “Render Website”. This will build the .html files and preview your website. It’s equivalent to “knitting” in RMarkdown.\nNote that you can also click “Preview Website”. With “Render Website” in RStudio, Quarto is able to render and preview in one step.\nIf you’d ever like to stop the preview, in the bottom-left, click on the Jobs tab and then the red Stop button.\n\nMake a small change and render it\nClick on index.md. This will open this markdown file in a fourth pane; the editor pane. Make a small change, for example change to today’s date on Line 4. Then, save your file; there is a disc icon at the top of the file.\nThen, render this file: press “Render” which is to the right of the disc icon that saves the file. This will render only this single file, as opposed to rerendering the whole website like when we clicked “Render Website” in the top right pane. Checking Render on Save (between the disc icon and the Render button) is a great strategy for doing this in one step.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#create-a-new-.rmd-page",
    "href": "quarto-workflows/rstudio.html#create-a-new-.rmd-page",
    "title": "From RStudio",
    "section": "Create a new .Rmd page",
    "text": "Create a new .Rmd page\nNew &gt; RMarkdown document &gt; OK\nThe starter RMarkdown document has some R code inside: it displays a summary of the cars dataset that is pre-loaded into R (summary(cars)) and plots the pressure data that is also pre-loaded (plot(pressure)).\nSave this document as r-example.rmd.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#update-_quarto.yml",
    "href": "quarto-workflows/rstudio.html#update-_quarto.yml",
    "title": "From RStudio",
    "section": "Update _quarto.yml",
    "text": "Update _quarto.yml\nNow we’ll add r-example.rmd to our _quarto.yml file; this is where we register all files to include in our site. Let’s add it after the section called “Quarto Workflows”.\nOpen _quarto.yml by clicking on it from the file directory.\nScroll down to review the current contents in the sidebar: section under contents:. It’s there we see all the file arrangement that we see in the previewed site.\nAdd - r-example.rmd in its own line, making sure that your indentation aligns with the other pages.\nFrom the Build tab, clicking Preview Website will recreate your website!",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#authoring-tips",
    "href": "quarto-workflows/rstudio.html#authoring-tips",
    "title": "From RStudio",
    "section": "Authoring tips",
    "text": "Authoring tips\nChecking “Render on Save” is really helpful when iterating quickly on a document.\nIf the document is very code-heavy, consider using freeze that will not run the code each time.\nQuarto.org has details about authoring, including specific instructions about authoring in RStudio.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#commit-and-push",
    "href": "quarto-workflows/rstudio.html#commit-and-push",
    "title": "From RStudio",
    "section": "Commit and push!",
    "text": "Commit and push!\nCommitting and pushing will make the changes you see locally live on your website (using the GitHub Action we set up earlier).",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#troubleshooting",
    "href": "quarto-workflows/rstudio.html#troubleshooting",
    "title": "From RStudio",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf you have trouble rendering your website after for example changing the extenstion of a file from .md to .qmd, refreshing your RStudio often helps. Do this by clicking the project name at the upper right of the RStudio window (in this case, quarto-website-tutorial), and underneath the “close project” section, click the same name of your project: quarto-website-tutorial. This will relaunch your whole project afresh.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/browser.html",
    "href": "quarto-workflows/browser.html",
    "title": "From the Browser",
    "section": "",
    "text": "A workflow from the browser if good for getting started (since you do not need to install additional software) and for making small contributions, but is definitely limited. Once you feel comfortable here, you can move to a different setup.\nHere’s an example of editing content on an existing page.",
    "crumbs": [
      "Quarto workflows",
      "From the Browser"
    ]
  },
  {
    "objectID": "quarto-workflows/browser.html#edit-content-on-an-existing-page",
    "href": "quarto-workflows/browser.html#edit-content-on-an-existing-page",
    "title": "From the Browser",
    "section": "Edit content on an existing page",
    "text": "Edit content on an existing page\nLet’s change the date on the home page of this website.\nIn your repository, navigate to index.md. Then, click the pencil icon in the top right to edit directly.\n\n\n\n\n\nWe are now in the “Edit file” tab of the editor, where we can make modifications. Let’s change the date to today’s date. Click the “Preview” tab to see your changes. You can even check the “Show diff” box on the right side to see the changes you’ve made.\n\n\n\n\n\nWhile you’re here, see if there are additional changes to the text you’d like to make. Maybe changing the title or author at the top, or for the main text on the home page of the website.\nOur index.md file is written in Markdown, which enables you to make simple text formatting. As you go back and forth from “Edit file” to “Preview”, notice the patterns of how the Markdown text looks when it is as source (“Edit file”) and when it is formatted (“Preview”). For example, in Markdown, you can make text as a header with # symbols, bold or italic with * symbols, and hyperlinks with [](). Notice that spacing is important: for example, there are carriage returns (when you hit the “return” key) before any bullet points. You can learn the short list of Markdown rules here: https://quarto.org/docs/authoring/markdown-basics.",
    "crumbs": [
      "Quarto workflows",
      "From the Browser"
    ]
  },
  {
    "objectID": "quarto-workflows/browser.html#commit-and-publish",
    "href": "quarto-workflows/browser.html#commit-and-publish",
    "title": "From the Browser",
    "section": "Commit and publish",
    "text": "Commit and publish\nCommit your changes by scrolling to the bottom of the page and writing a commit message - a note to yourself and others about what changes you made. Write your commit message and then click the green “Commit changes” button.\n\n\n\n\n\nNow, click back to the main page of your GitHub repository. You should see the orange dot confirming your website is published. You’ll have to wait for the GitHub Action to tell quarto to build your site for you to see the update, but it will be there!",
    "crumbs": [
      "Quarto workflows",
      "From the Browser"
    ]
  },
  {
    "objectID": "quarto-workflows/browser.html#limitations",
    "href": "quarto-workflows/browser.html#limitations",
    "title": "From the Browser",
    "section": "Limitations",
    "text": "Limitations\nWhile awesome that we can edit using GitHub directly from the browser, there are obvious limitations. One is that to see your edits show up in your book, you have to publish using the GitHub Action. This is slow. Another limitation is that we can only work on one file at a time and commit them each separately, which also is slow. Using additional software can make things much better, as we explore in subsequent chapters.",
    "crumbs": [
      "Quarto workflows",
      "From the Browser"
    ]
  },
  {
    "objectID": "transition-from-rmarkdown.html",
    "href": "transition-from-rmarkdown.html",
    "title": "Transition from RMarkdown",
    "section": "",
    "text": "You may already have workflows in RMarkdown and are interested in transitioning to Quarto. There’s no hurry to migrate to Quarto. Keep using Rmarkdown and when you’re ready the migration will be fine.\nHere are some notes as we migrate RMarkdown sites and books.\nTODO: translating R code chunks",
    "crumbs": [
      "Transition from Rmd"
    ]
  },
  {
    "objectID": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "href": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Bookdown to Quarto",
    "text": "Bookdown to Quarto\nConverting a Bookdown book to Quarto is slightly more involved than converting a website. A book has chapters whose order must be defined, and likely has citations and cross-refs. Still, conversion is not that hard.\nWe got some practice converting from Bookdown to Quarto by helping Gavin Fay convert his lab’s fantastic onboarding documentation, the Faylab Lab Manual. Here’s the GitHub view before and after.\nOur best first reference material for this was Nick Tierney’s Notes on Changing from Rmarkdown/Bookdown to Quarto. Nick shares some scripts in that post to automate some changes. In our case, the book was small enough that we made all changes manually. Quarto documentation was indispensable.\n\nExperimenting in a low-risk environment\nWe forked a copy of the Faylab Lab manual to the Openscapes organization, and worked in a branch so we could make changes relatively risk-free. We could always fork a new copy of the original if we “broke” something. (Caution: the default when making a pull request from a fork is to push changes to the original upstream repo, not your fork and it does this without warning if you have write-access to that repo.) With local previews it’s easy to test / play with settings to see what they do. We tended to make a change, Preview, then compare the look and functionality of the book to the original. It was helpful to comment out some elements of the configuration file _output.yml after their counterparts had been added to the Quarto configuration file _quarto.yml, or to confirm they were no longer needed, before making the drastic move of deleting them.\n\n\nThe conversion\nHere are the main steps to convert the Faylab Lab manual from Bookdown to Quarto.\nCreate new empty file called _quarto.yml and add book metadata there. The screenshots below\nSet project type as book.\nMove metadata out of index.qmd and into _quarto.yml. Title, author, and publication date were in index.qmd with date set using date: \"Last updated:r Sys.Date()\". Now these are in _quarto.yml with date set using date: last-modified. Note that having R code would require you to adjust code chunk options in the Quarto style (#|). This tripped us up a bit; see GitHub Actions.\nMove chapters listing out of _bookdown.yml and into _quarto.yml.\nAdd page footer to _quarto.yml.\nHere’s what ours looked like when we finished the steps above (_quarto.yml).\n\n\n\n\n\n\n\n\n\n_quarto.yml contents\n\n\n\n\n\n\n\nFaylab Lab Manual\n\n\n\n\n\nChange insertion of images from html style to Quarto style. (Note Quarto calls them “figures”, not “images”.) The following snippet will insert the GitHub octocat logo in a page:\n![](https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png){fig-align=\"left\" width=\"35px\"}\nChange all filename extensions .Rmd -&gt; .qmd (you could Preview after this change and see that the book looks the same). Note that Quarto works with .Rmd files just as well as it does .qmd, so this change is not urgent. In fact, if you have a lot of R code in your .Rmds (unlike the Faylab Lab Manual), there will be additional tinkering needed to make the code chunks happy.\n\n\nCitations\nThe Faylab Lab Manual cited two papers, presenting us with an opportunity to see how easy it is to add references to a Quarto book. Briefly, in the Visual Editor, Insert &gt; Citation &gt; DOI. Pasting the DOI or its full URL, we can insert the citation. This automatically creates a references.bib file and adds the full citations at the bottom of the chapter page (watch demo). In July 2022, we had to manually add a ## References heading, but this may not be necessary in future Quarto updates.\n\n\n\n\n\n\n\n\n\nInsert citation via its DOI using RStudio Visual Editor\n\n\n\n\n\n\n\n\n\n\nPublishing notes\nIf the book’s output is strictly html, there’s no need to specify output-dir in _quarto.yml. The output directory default is _book/, which is what we’d like. If we wanted other types of output like like PDF or EPUB, etc. those single file outputs are also written to the output-dir (Quarto docs).\nIf you currently have a docs/ folder, delete it.\nUpdate .gitignore to ignore _book/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_book/\nOnce all is settled, delete _output.yml.\nOnce the Openscapes fork was fully reviewed, we made a pull request from that to the main branch of the book’s repo. Once that was merged, we set up GitHub Actions to render the book. (TODO: instructions for GitHub Actions)\n\n\nGitHub Actions\nThis book was mostly prose and screenshots without any R code. This made the conversion from RMarkdown to Quarto likely more straightforward than if you also needed to adjust code chunk options in the quarto style (#|). Our initial GitHub Action to render the converted Faylab Lab Manual failed because we had a piece of R code - even though the code was commented out! This was resolved when we deleted the line.",
    "crumbs": [
      "Transition from Rmd"
    ]
  },
  {
    "objectID": "transition-from-rmarkdown.html#distill-to-quarto",
    "href": "transition-from-rmarkdown.html#distill-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Distill to quarto",
    "text": "Distill to quarto\nWe transitioned our events site from distill to quarto in May 2022 (github view before and after). We followed excellent notes and examples from Nick Tierney and Danielle Navarro.\nAfter we had changed all the files, the Build tab in the RStudio IDE still showed “Build website” rather then “Render Website” and “Preview Website”, and would error when we pushed them (because that button was expecting a distill site, not a quarto site). To fix this, we updated the .Rproj file. Clicking on the .Rproj file in the RStudio IDE will open a dialog box where you can click things you want (you can also open these in a text editor or from the GitHub website to see the actual text). To fix this situation with the Build tab: Project Options &gt; Build Tools &gt; Project Build Tools &gt; None.\nLooking at files /posts/_metadata.yml and _quarto.yml helps see where things are defined. For example, to make event post citations appear, we added citation: true to /posts/_metadata.yml and in _quarto.yml under the website key we set site-url: https://openscapes.github.io/events. We deleted footer.html used with distill because footer is now defined in quarto.yml.\n\nPublishing notes\n\nBackground: Our distill site had been set up to output to a docs folder, and had GitHub Settings &gt; Pages set to look there rather gh-pages branch. (Julie note: this was a new-to-me capability when we set up the events distill site in Spring 2021 so I had forgotten that was an option). We’ve inititally kept this same set-up for now with our events page in _quarto.yml: output-dir: docs. However, this is sub-optimal; better to not have to commit and push these files but to instead have a GitHub Action generate them upon a commit. So the following is what we did -\n\nDon’t specify output-dir in _quarto.yml. The output directory default is _site/, which is what we’d like.\nIf you currently have a docs/ folder (like we did as we were experimenting), delete it.\nUpdate .gitignore to ignore _site/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_site/\nPush these changes, merge into main.\nOn GitHub.com, in your repo, set up GitHub publishing\nFollow instructions from the explore and setup chapter.",
    "crumbs": [
      "Transition from Rmd"
    ]
  },
  {
    "objectID": "transition-from-rmarkdown.html#troubleshooting",
    "href": "transition-from-rmarkdown.html#troubleshooting",
    "title": "Transition from RMarkdown",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nGitHub Action fails, says you need RMarkdown but you don’t have R code!\nAnd you changed all .Rmds to .qmds!\nYou likely have a few setup code chunks from RMarkdown, that look like this:\n{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nYou can find them by opening each of your files and having a look, or use GitHub’s search for the keyword knitr",
    "crumbs": [
      "Transition from Rmd"
    ]
  },
  {
    "objectID": "machine-learning/ReinforcementLearningConcept.html",
    "href": "machine-learning/ReinforcementLearningConcept.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning (RL) là một lĩnh vực của trí tuệ nhân tạo mà trong đó một “agent” học cách tương tác với một “environment” để đạt được mục tiêu thông qua việc thực hiện “actions”. Agent học từ các phản hồi gọi là “rewards” từ environment và dần dần điều chỉnh hành vi để tối đa hóa tổng số rewards.",
    "crumbs": [
      "Machine learning",
      "Reinforcement Learning"
    ]
  },
  {
    "objectID": "machine-learning/ReinforcementLearningConcept.html#reinforcement-learning-là-gì",
    "href": "machine-learning/ReinforcementLearningConcept.html#reinforcement-learning-là-gì",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning (RL) là một lĩnh vực của trí tuệ nhân tạo mà trong đó một “agent” học cách tương tác với một “environment” để đạt được mục tiêu thông qua việc thực hiện “actions”. Agent học từ các phản hồi gọi là “rewards” từ environment và dần dần điều chỉnh hành vi để tối đa hóa tổng số rewards.",
    "crumbs": [
      "Machine learning",
      "Reinforcement Learning"
    ]
  },
  {
    "objectID": "machine-learning/ReinforcementLearningConcept.html#các-thuật-ngữ-thông-dụng",
    "href": "machine-learning/ReinforcementLearningConcept.html#các-thuật-ngữ-thông-dụng",
    "title": "Reinforcement Learning",
    "section": "Các thuật ngữ thông dụng",
    "text": "Các thuật ngữ thông dụng\nTrong reinforcement learning có rất nhiều các thuật ngữ khác nhau. Sau đây chúng ta cùng liệt kê các thuật ngữ thông dụng và tìm hiểu ý nghĩa của từng thuật ngữ\n\nAgent: Là thực thể thực hiện các hành động trong môi trường để đạt được mục tiêu.\nEnvironment: Là môi trường mà agent tương tác và học từ đó. Nó bao gồm tất cả các yếu tố ảnh hưởng đến agent và có thể thay đổi dựa trên hành động của agent.\nAction: Là các hành động mà agent thực hiện trong môi trường để thay đổi trạng thái hiện tại của nó.\nObservation: Là thông tin mà agent thu thập từ môi trường sau khi thực hiện một hành động.\nState: Là biểu diễn trạng thái hiện tại của môi trường. Nó chứa thông tin cần thiết để quyết định tương lai của agent.\nPolicy: Là chiến lược hoặc kế hoạch mà agent sử dụng để chọn hành động dựa trên trạng thái hiện tại.\nReward: Là phản hồi từ môi trường sau mỗi hành động. Reward định rõ giá trị của hành động và giúp agent học cách tối đa hóa tổng số reward theo thời gian.\nKhai thác và khám phá (exploit or explore): Là quá trình cân bằng giữa việc sử dụng kiến thức hiện có để đạt được reward ngay lập tức (khai thác) và việc thử nghiệm các hành động mới để tìm hiểu thêm về môi trường (khám phá).",
    "crumbs": [
      "Machine learning",
      "Reinforcement Learning"
    ]
  },
  {
    "objectID": "machine-learning/ReinforcementLearningConcept.html#so-sánh-reinforcement-learning-rl-unsupervised-learning-and-supervised-learning",
    "href": "machine-learning/ReinforcementLearningConcept.html#so-sánh-reinforcement-learning-rl-unsupervised-learning-and-supervised-learning",
    "title": "Reinforcement Learning",
    "section": "So sánh Reinforcement Learning (RL), Unsupervised Learning, and Supervised Learning",
    "text": "So sánh Reinforcement Learning (RL), Unsupervised Learning, and Supervised Learning\nHọc tăng cường (RL), Học không giám sát và Học có giám sát là ba mô hình cơ bản trong học máy, mỗi mô hình phục vụ các mục đích khác nhau và giải quyết các loại vấn đề riêng biệt. Dưới đây là so sánh ba cách tiếp cận này:\n\nHọc tăng cường (RL):\n\nMục tiêu: Học cách đưa ra quyết định để tối đa hóa phần thưởng tích lũy trong một môi trường.\nBản chất: Liên quan đến việc tác nhân tương tác với môi trường bằng cách thực hiện hành động và nhận phần thưởng dựa trên hành động của tác nhân đó.\nDữ liệu đào tạo: RL học hỏi từ các tương tác và phản hồi thay vì dữ liệu được gắn nhãn.\nVí dụ: Chơi trò chơi, điều khiển robot, hệ thống đề xuất.\nThách thức: Đánh đổi giữa thăm dò và khai thác, phần thưởng bị trì hoãn, cân bằng giữa thăm dò và khai thác.\n\n\n\nHọc không giám sát:\n\nMục tiêu: Tìm các mẫu hoặc cấu trúc trong dữ liệu chưa được gắn nhãn.\nBản chất: Không có mục tiêu hoặc kết quả cụ thể để dự đoán. Mục tiêu là khám phá các mối quan hệ hoặc cụm ẩn trong dữ liệu.\nDữ liệu huấn luyện: Học từ dữ liệu đầu vào mà không có nhãn đầu ra rõ ràng.\nVí dụ: Phân cụm, giảm kích thước, tạo mô hình tổng quát (ví dụ: GAN).\nThách thức: Xác định số lượng cụm thích hợp, xử lý dữ liệu nhiều chiều.\n\n\n\nHọc tập có giám sát:\n\nMục tiêu: Học cách ánh xạ từ đầu vào đến đầu ra dựa trên dữ liệu huấn luyện được gắn nhãn.\nBản chất: Yêu cầu tập dữ liệu được gắn nhãn trong đó mô hình học cách dự đoán kết quả đầu ra chính xác từ các đầu vào nhất định.\nDữ liệu huấn luyện: Sử dụng các cặp đầu vào-đầu ra để huấn luyện.\nVí dụ: Phân loại, hồi quy.\nThách thức: Quá phù hợp, sai lệch, khái quát hóa đối với dữ liệu không nhìn thấy được.\n\nSự khác biệt chính: - RL tập trung vào việc học các hành động tối ưu để tối đa hóa phần thưởng, trong khi việc học có giám sát và không giám sát tập trung vào các mô hình học tập hoặc các mối quan hệ trong dữ liệu. - Học không giám sát thiếu dữ liệu được dán nhãn, trong khi học có giám sát yêu cầu dữ liệu được dán nhãn. - Học có giám sát phù hợp với các nhiệm vụ đã biết đầu ra mong muốn, trong khi RL phù hợp với các nhiệm vụ liên quan đến việc ra quyết định tuần tự. - RL thường liên quan đến việc khám phá và đánh đổi giữa phần thưởng trước mắt và lâu dài, trong khi học tập có giám sát và không giám sát là tìm kiếm các mô hình hoặc đưa ra dự đoán.",
    "crumbs": [
      "Machine learning",
      "Reinforcement Learning"
    ]
  },
  {
    "objectID": "machine-learning/ReinforcementLearningConcept.html#tham-khảo",
    "href": "machine-learning/ReinforcementLearningConcept.html#tham-khảo",
    "title": "Reinforcement Learning",
    "section": "Tham khảo",
    "text": "Tham khảo\n\ncodelearn\ntechvidvan\nmathworks",
    "crumbs": [
      "Machine learning",
      "Reinforcement Learning"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html",
    "href": "machine-learning/transformer.html",
    "title": "Transformer",
    "section": "",
    "text": "Kiến trúc Transformer là một mô hình học sâu phổ biến được giới thiệu trong bài báo “Attention is All You Need” của Vaswani và đồng nghiệp (2017). Nó đã cách mạng hóa nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và trở thành cơ sở cho các mô hình tiên tiến như BERT và GPT.\nKiến trúc Transformer dựa trên khái niệm self-attention, cho phép mô hình tập trung vào các phần khác nhau của chuỗi đầu vào khi xử lý từng phần tử. Dưới đây là một cái nhìn tổng quan về kiến trúc Transformer:\n\n\n\nKiến trúc Transformer\n\n\n\nNhúng Đầu Vào (Input Embeddings):\n\nChuỗi đầu vào được nhúng thành các biểu diễn liên tục được gọi là nhúng đầu vào.\nMỗi từ hoặc thành phần trong chuỗi đầu vào được biểu diễn bằng một vector mật độ.\n\nMã Hóa Vị Trí (Positional Encoding):\n\nMã hóa vị trí được thêm vào nhúng đầu vào để cung cấp thông tin về thứ tự hoặc vị trí của các từ trong chuỗi.\nĐiều này cho phép mô hình bắt chước thông tin tuần tự, vì kiến trúc Transformer không có kết nối lặp lại.\n\nBộ Mã Hóa (Encoder):\n\nTransformer bao gồm một cấu trúc mã hóa-giải mã (encoder-decoder), nhưng để đơn giản, chúng ta tập trung vào bộ mã hóa.\nBộ mã hóa bao gồm nhiều lớp giống nhau, mỗi lớp bao gồm hai lớp con:\n\nMulti-head Self-Attention:\n\nSelf-attention cho phép mô hình đánh trọng số sự quan trọng của các từ khác nhau trong chuỗi đầu vào khi xử lý một từ cụ thể.\nNó tính toán tổng trọng số của các nhúng từ tất cả các từ, trong đó trọng số được xác định bởi sự tương đồng giữa các từ.\n\nFeed-Forward Neural Network:\n\nA simple feed-forward neural network được áp dụng cho từng vị trí trong chuỗi độc lập.\nNó giúp bắt chước mối quan hệ phi tuyến giữa các từ trong chuỗi.\n\n\n\nBộ Giải Mã (Decoder):\n\nBộ giải mã tương tự như bộ mã hóa nhưng có cơ chế attention bổ sung.\nNó tạo ra chuỗi đầu ra bằng cách chú ý đến các biểu diễn đầu ra của bộ mã hóa và các từ đã được tạo ra trước đó.\n\nCơ Chế Che (Masking):\n\nTrong quá trình huấn luyện, cơ chế che được sử dụng để ngăn mô hình chú ý đến các vị trí trong tương lai.\nĐiều này đảm bảo rằng mỗi vị trí chỉ có thể chú ý đến các vị trí trước đó trong chuỗi đầu vào.\n\nTạo Ra Đầu Ra:\n\nLớp cuối cùng của bộ giải mã tạo ra xác suất đầu ra cho mỗi vị trí trong chuỗi đầu ra.\nCác xác suất này thường được tính bằng cách sử dụng hàm kích hoạt softmax.\n\n\nKiến trúc Transformer có một số lợi thế, như khả năng song song hóa, khả năng bắt chước các phụ thuộc xa, và khả năng tổng quát tốt hơn nhờ self-attention. Nó đã đạt được kết quả tiên tiến trên nhiều nhiệm vụ NLP, bao gồm dịch máy, tóm tắt văn bản và hiểu ngôn ngữ.\n\n\n\n\n\nSelf-attention, còn được gọi là intra-attention hay scaled dot-product attention (tích vô hướng giữa 2 vector), là một cơ chế tính toán trọng số chú ý cho các phần tử trong cùng một chuỗi đầu vào. Nó cho phép mô hình đánh giá mức độ quan trọng của mỗi phần tử trong chuỗi dựa trên mức liên quan của nó đối với các phần tử khác.\n\n\n\nTrong self-attention, mỗi từ trong chuỗi đầu vào được biểu diễn bằng một vector, thông thường được gọi là nhúng hay vector nhúng (embedding vector). Giả sử nhúng từ cho chuỗi đầu vào như sau:\nVí dụ câu như sau: “Con mèo ngồi trên chiếc thảm.”\nĐể áp dụng self-attention, chúng ta trước tiên biểu diễn mỗi từ trong câu thành một vector nhúng. Hãy giả sử chúng ta có các vector nhúng từ sau đây:\nNhúng từ:\n- \"Con\": [0.2, 0.5, -0.3]\n- \"mèo\": [-0.1, 0.7, 0.2]\n- \"ngồi\": [0.4, -0.2, 0.6]\n- \"trên\": [-0.5, 0.3, -0.1]\n- \"chiếc\": [0.2, 0.5, -0.3]\n- \"thảm\": [0.3, 0.1, 0.8]\n- \".\": [0.0, 0.0, 0.0]\nBây giờ, chúng ta tính toán trọng số self-attention cho mỗi từ trong câu. Trong self-attention, mỗi từ được so sánh với tất cả các từ khác để xác định độ quan trọng hoặc liên quan của nó. Quá trình so sánh được thực hiện bằng cách tính tích vô hướng giữa nhúng từ, sau đó áp dụng phép softmax để thu được trọng số chú ý.\nVí dụ, đối với từ “Con,” tích vô hướng được tính toán với nhúng từ của tất cả các từ khác:\nTrọng số Chú ý cho “Con”:\n- \"Con\" so sánh với \"Con\": 0.3\n- \"Con\" so sánh với \"mèo\": 0.1\n- \"Con\" so sánh với \"ngồi\": 0.2\n- \"Con\" so sánh với \"trên\": 0.05\n- \"Con\" so sánh với \"chiếc\": 0.35\n- \"Con\" so sánh với \"thảm\": 0.0\n- \"Con\" so sánh với \".\": 0.0\nCác trọng số chú ý phản ánh sự quan trọng của mỗi từ đối với “Con” dựa trên sự tương đồng ngữ nghĩa hoặc liên quan ngữ cảnh của chúng. Phép softmax đảm bảo rằng các trọng số tổng cộng bằng 1.\nChúng ta lặp lại quá trình này cho mỗi từ trong câu để thu được trọng số self-attention cho toàn bộ câu. Những trọng số chú ý này sau đó có thể được sử dụng để tổng hợp thông tin từ các từ khác nhau và bắt chước các mối quan hệ quan trọng trong câu để tiếp tục xử lý trong mô hình.\n\n\n\n\n\n\nMulti-Head Attention là một phần mở rộng của self-attention cho phép mô hình tập trung vào các khía cạnh khác nhau của chuỗi đầu vào cùng một lúc. Đó là việc áp dụng self-attention nhiều lần song song, mỗi attention head tập trung vào một biểu diễn khác nhau của đầu vào. Bằng cách sử dụng nhiều attention head, mô hình có thể nắm bắt các thông tin khác nhau và học các mẫu đa dạng, nâng cao khả năng hiểu và xử lý các chuỗi phức tạp một cách hiệu quả.\n\n\n\n\nXem xét một câu tiếng Anh: “I love cats.” -&gt; Và chúng ta muốn dịch nó sang tiếng Pháp: “J’adore les chats.”\nGiả sử cả hai câu tiếng Anh và tiếng Pháp đã được nhúng thành các vector như sau:\nNhúng từ tiếng Anh:\n\n“I”: [0.1, 0.2, 0.3]\n“love”: [0.4, 0.5, 0.6]\n“cats”: [0.7, 0.8, 0.9]\n\nNhúng từ tiếng Pháp:\n\n“J’adore”: [0.9, 0.8, 0.7]\n“les”: [0.6, 0.5, 0.4]\n“chats”: [0.3, 0.2, 0.1]\n\nTính toán trọng số\n\nGiả sử chúng ta đang sử dụng cơ chế multi-head attention với 2 attention head. Mỗi head sẽ tập trung vào chuỗi đầu vào tiếng Anh (nguồn) và chuỗi đầu vào tiếng Pháp (đích) riêng biệt.\nVới mỗi attention head, chúng ta sẽ tính toán trọng số attention cho cả chuỗi đầu vào tiếng Anh và tiếng Pháp. Các trọng số attention xác định mức độ quan trọng của mỗi từ trong chuỗi nguồn liên quan đến các từ trong chuỗi đích.\nVí dụ,\n\nVới attention head đầu tiên, chúng ta tính toán trọng số attention cho chuỗi đầu vào tiếng Anh:\nAttention_scores_source_head1 = softmax(dot_product(english_source_embeddings, [french_target_embedding1, french_target_embedding2, french_target_embedding3]))\nTương tự, chúng ta tính toán trọng số attention cho chuỗi đích tiếng Pháp:\nAttention_scores_target_head1 = softmax(dot_product(french_target_embeddings, [english_source_embedding1, english_source_embedding2, english_source_embedding3]))\nLặp lại các bước trên cho attention head tiếp theo, kết quả sẽ được các attention score cho cả chuỗi nguồn và chuỗi đích.\nSau khi có các trọng số attention, chúng ta áp dụng chúng vào nhúng tương ứng để có tổng có trọng số cho mỗi chuỗi.\nVí dụ, đối với chuỗi nguồn tiếng Anh và attention head đầu tiên:\nWeighted_sum_source_head1 = attention_score1 * english_source_embedding1 + attention_score2 * english_source_embedding2 + attention_score3 * english_source_embedding3\nTương tự, đối với chuỗi đích tiếng Pháp và attention head đầu tiên:\nWeighted_sum_target_head1 = attention_score1 * french_target_embedding1 + attention_score2 * french_target_embedding2 + attention_score3 * french_target_embedding3\nChúng ta lặp lại các bước trên cho attention head thứ hai.\nCuối cùng, chúng ta nối các đầu ra từ mỗi attention head và thông qua một phép biến đổi tuyến tính để có đầu ra multi-head attention cuối cùng.\nNhư vậy, cơ chế multi-head attention cho phép mô hình nắm bắt các thông tin và mối quan hệ khác nhau giữa chuỗi nguồn và chuỗi đích, tăng khả năng dịch giữa các ngôn ngữ một cách chính xác.\n\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.query_projection = nn.Linear(embed_dim, embed_dim)\n        self.key_projection = nn.Linear(embed_dim, embed_dim)\n        self.value_projection = nn.Linear(embed_dim, embed_dim)\n        self.output_projection = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n        \n        # Apply linear projections for query, key, and value\n        query = self.query_projection(query)\n        key = self.key_projection(key)\n        value = self.value_projection(value)\n        \n        # Reshape query, key, and value to split heads\n        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Compute attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n        attention_weights = torch.softmax(scores, dim=-1)\n        \n        # Apply attention weights to value\n        attended_values = torch.matmul(attention_weights, value)\n        \n        # Reshape and concatenate attended values\n        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n        \n        # Apply linear projection for output\n        output = self.output_projection(attended_values)\n        \n        return output\n\n\n\n\n\n\n\nPhép mã hóa vị trí (Positional Encoding) là một kỹ thuật được sử dụng trong các mô hình dựa trên chuỗi, đặc biệt là trong kiến trúc Transformer, để đưa thông tin về vị trí tương đối của các yếu tố trong chuỗi đầu vào. Nó giải quyết vấn đề rằng tự chú ý (self-attention) và các cơ chế chú ý khác trong các mô hình này không ngầm định mã hóa thông tin về vị trí.\nTrong các nhiệm vụ xử lý ngôn ngữ tự nhiên, thứ tự của các từ trong một câu mang ý nghĩa và ngữ cảnh quan trọng. Tuy nhiên, mô hình Transformer xử lý chuỗi đầu vào theo cách song song, không rõ ràng ghi nhận thứ tự tuần tự. Phép mã hóa vị trí giúp mô hình phân biệt giữa các yếu tố ở các vị trí khác nhau trong chuỗi.\nMã hóa vị trí được thêm vào nhúng đầu vào của chuỗi trước khi đưa chúng vào mô hình Transformer. Thông thường, điều này được thực hiện bằng cách kết hợp các hàm sin-cos với các nhúng đầu vào. Mỗi chiều của mã hóa vị trí đại diện cho một vị trí cụ thể trong chuỗi.\nSự lựa chọn các hàm sin-cos cho phép mô hình dễ dàng mở rộng cho các độ dài chuỗi dài hơn so với những gì mô hình đã được huấn luyện. Tần số của các hàm sin-cos tuân theo một cấp số học, cho phép mô hình học để chú ý đến các vị trí khác nhau với các tỷ lệ khác nhau.\nBằng cách thêm mã hóa vị trí vào nhúng đầu vào, mô hình Transformer có thể hiệu quả nắm bắt thứ tự và vị trí tương đối của các yếu tố trong chuỗi. Điều này giúp mô hình hiểu các phụ thuộc và mối quan hệ giữa các vị trí khác nhau và cho phép thực hiện các nhiệm vụ yêu cầu xử lý thông tin tuần tự, chẳng hạn như dịch máy hoặc hiểu ngôn ngữ.\n\n\n\n\nGiả sử chúng ta có một chuỗi đầu vào gồm ba từ: [“Tôi”, “yêu”, “mèo”].\nMỗi từ sẽ được biểu diễn bằng một vector nhúng từ, nắm bắt ý nghĩa ngữ nghĩa của nó. Hãy giả định các vector nhúng từ như sau:\n\n“Tôi”: [0.2, 0.3, -0.1]\n“yêu”: [-0.5, 0.7, 0.2]\n“mèo”: [0.7, 0.8, 0.9]\n\nĐể tích hợp thông tin về vị trí, chúng ta sử dụng mã hóa vị trí. Trong ví dụ này, chúng ta sẽ sử dụng các hàm sin và cos để tạo các vector mã hóa vị trí.\nGiả sử chiều nhúng là 3, các vector mã hóa vị trí sẽ được tính như sau:\nCho vị trí 0 (từ đầu tiên): - PE(0) = [sin(0/10000^0), cos(0/10000^0), sin(0/10000^0)]\nCho vị trí 1 (từ thứ hai): - PE(1) = [sin(1/10000^1), cos(1/10000^1), sin(1/10000^1)]\nCho vị trí 2 (từ thứ ba): - PE(2) = [sin(2/10000^0), cos(2/10000^0), sin(2/10000^0)]\nVì sin(0) = 0, sin(1) = 0.8415, sin(2) = 0.9093, cos(0) = 1, cos(1) = 0.5403, cos(2) = -0.4161.\nNên, các vector mã hóa vị trí là:\nPE(0) = [0, 1, 0] PE(1) = [0.8415, 0.5403, 0.8415] PE(2) = [0.9093, -0.4161, 0.9093]\nĐể tích hợp mã hóa vị trí, chúng ta cộng vector mã hóa vị trí tương ứng vào mỗi vector nhúng từ:\n\nTừ “Tôi” với mã hóa vị trí: [0.2, 0.3, -0.1] + [0, 1, 0] = [0.2, 1.3, -0.1]\nTừ “yêu” với mã hóa vị trí: [-0.5, 0.7, 0.2] + [0.8415, 0.5403, 0.8415] =\n\n[0.3415, 1.2403, 1.0415] - Từ “mèo” với mã hóa vị trí: [0.7, 0.8, 0.9] + [0.9093, -0.4161, 0.9093] = [1.6093, 0.3839, 1.8093]\nBằng cách cộng các vector mã hóa vị trí vào các vector nhúng từ, chúng ta giới thiệu thông tin về vị trí tương đối của các từ trong chuỗi đầu vào. Điều này cho phép mô hình Transformer nắm bắt thứ tự tuần tự và các phụ thuộc giữa các từ, điều quan trọng cho các nhiệm vụ như dịch máy hoặc phân tích cảm xúc.\n\n\n\n\nLayer Normalization là một kỹ thuật được sử dụng trong các mô hình học sâu để chuẩn hóa các kích hoạt (kết quả chuyển đổi đầu vào thành đầu ra bằng hàm kích hoạt - tạm gọi là các kích hoạt) của các đơn vị trong một lớp. Nó giải quyết vấn đề của sự thay đổi phân phối đầu vào của lớp trong quá trình huấn luyện, gây khó khăn cho quá trình học.\nCông thức cho Layer Normalization có thể được biểu diễn như sau:\noutput = scale * (input - mean) / sqrt(variance + epsilon) + bias\n\nỞ đây, đầu vào đại diện cho các kích hoạt đầu vào của một lớp, trung bình và phương sai đại diện cho giá trị trung bình và phương sai được tính theo chiều đầu vào, epsilon là một hằng số nhỏ được thêm vào để đảm bảo tính ổn định số học, và scale và bias là các tham số có thể học giúp mô hình điều chỉnh và dịch chuyển các kích hoạt đã được chuẩn hóa.\nLayer Normalization được áp dụng độc lập cho mỗi lô dữ liệu (batch), điều này làm cho nó phù hợp cho các nhiệm vụ với kích thước lô nhỏ. Nó đã được chứng minh là hiệu quả trong ổn định quá trình huấn luyện, tăng tốc quá trình hội tụ và cải thiện hiệu suất tổng quát của các mô hình học sâu.\nTổng quát, Layer Normalization giúp giảm sự phụ thuộc vào tỷ lệ của các kích hoạt đầu vào, thúc đẩy việc truyền gradient ổn định và cải thiện động học huấn luyện tổng thể (overall training dynamics - gồm learning rate, gradient, loss rate) của mạng neural. Nó đã được sử dụng rộng rãi trong các lĩnh vực khác nhau, bao gồm xử lý ngôn ngữ tự nhiên, thị giác máy tính và nhận dạng giọng nói.\n\n\n\nGiả sử chúng ta có một câu: “Tôi thích chơi bóng đá.”\nTrong mô hình ngôn ngữ, mục tiêu là dự đoán từ tiếp theo trong ngữ cảnh đã cho. Tuy nhiên, trong quá trình huấn luyện, chúng ta thường không muốn cho mô hình truy cập vào các từ trong tương lai vì nó có thể dẫn đến rò rỉ dữ liệu và dự đoán không thực tế. Vì vậy, chế che được áp dụng.\nĐể áp dụng cơ chế che, thường được ký hiệu là [MASK], để thay thế một số từ trong chuỗi đầu vào trong quá trình huấn luyện. Trong trường hợp này, chúng ta có thể ngẫu nhiên che một số từ trong câu, dẫn đến:\nCâu Đầu Vào (có cơ chế che): “Tôi thích chơi [MASK] bóng đá.”\nBây giờ, trong quá trình huấn luyện, mục tiêu của mô hình là dự đoán từ bị che dựa trên ngữ cảnh xung quanh. Nó học cách hiểu cấu trúc câu, ý nghĩa và sự phụ thuộc giữa các từ để đưa ra dự đoán chính xác. Mô hình nhận chuỗi đầu vào đã được chỉnh sửa (thêm [MASK]) và cố gắng dự đoán bản gốc, không bị che, ở vị trí đã được che.\nTrong quá trình huấn luyện hoặc đánh giá, cơ chế che không được sử dụng và mô hình được cung cấp với toàn bộ chuỗi đầu vào. Nó có thể tạo ra dự đoán cho từ tiếp theo trong chuỗi dựa trên kiến thức đã học từ quá trình huấn luyện.\nCơ chế che giúp mô hình tổng quát tốt và xử lý các trường hợp mà nó gặp phải các từ chưa được nhìn thấy hoặc thiếu. Nó khuyến khích mô hình dựa vào ngữ cảnh có sẵn để đưa ra dự đoán chính xác và cải thiện khả năng hiểu cấu trúc câu.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#transformer-architecture",
    "href": "machine-learning/transformer.html#transformer-architecture",
    "title": "Transformer",
    "section": "",
    "text": "Kiến trúc Transformer là một mô hình học sâu phổ biến được giới thiệu trong bài báo “Attention is All You Need” của Vaswani và đồng nghiệp (2017). Nó đã cách mạng hóa nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và trở thành cơ sở cho các mô hình tiên tiến như BERT và GPT.\nKiến trúc Transformer dựa trên khái niệm self-attention, cho phép mô hình tập trung vào các phần khác nhau của chuỗi đầu vào khi xử lý từng phần tử. Dưới đây là một cái nhìn tổng quan về kiến trúc Transformer:\n\n\n\nKiến trúc Transformer\n\n\n\nNhúng Đầu Vào (Input Embeddings):\n\nChuỗi đầu vào được nhúng thành các biểu diễn liên tục được gọi là nhúng đầu vào.\nMỗi từ hoặc thành phần trong chuỗi đầu vào được biểu diễn bằng một vector mật độ.\n\nMã Hóa Vị Trí (Positional Encoding):\n\nMã hóa vị trí được thêm vào nhúng đầu vào để cung cấp thông tin về thứ tự hoặc vị trí của các từ trong chuỗi.\nĐiều này cho phép mô hình bắt chước thông tin tuần tự, vì kiến trúc Transformer không có kết nối lặp lại.\n\nBộ Mã Hóa (Encoder):\n\nTransformer bao gồm một cấu trúc mã hóa-giải mã (encoder-decoder), nhưng để đơn giản, chúng ta tập trung vào bộ mã hóa.\nBộ mã hóa bao gồm nhiều lớp giống nhau, mỗi lớp bao gồm hai lớp con:\n\nMulti-head Self-Attention:\n\nSelf-attention cho phép mô hình đánh trọng số sự quan trọng của các từ khác nhau trong chuỗi đầu vào khi xử lý một từ cụ thể.\nNó tính toán tổng trọng số của các nhúng từ tất cả các từ, trong đó trọng số được xác định bởi sự tương đồng giữa các từ.\n\nFeed-Forward Neural Network:\n\nA simple feed-forward neural network được áp dụng cho từng vị trí trong chuỗi độc lập.\nNó giúp bắt chước mối quan hệ phi tuyến giữa các từ trong chuỗi.\n\n\n\nBộ Giải Mã (Decoder):\n\nBộ giải mã tương tự như bộ mã hóa nhưng có cơ chế attention bổ sung.\nNó tạo ra chuỗi đầu ra bằng cách chú ý đến các biểu diễn đầu ra của bộ mã hóa và các từ đã được tạo ra trước đó.\n\nCơ Chế Che (Masking):\n\nTrong quá trình huấn luyện, cơ chế che được sử dụng để ngăn mô hình chú ý đến các vị trí trong tương lai.\nĐiều này đảm bảo rằng mỗi vị trí chỉ có thể chú ý đến các vị trí trước đó trong chuỗi đầu vào.\n\nTạo Ra Đầu Ra:\n\nLớp cuối cùng của bộ giải mã tạo ra xác suất đầu ra cho mỗi vị trí trong chuỗi đầu ra.\nCác xác suất này thường được tính bằng cách sử dụng hàm kích hoạt softmax.\n\n\nKiến trúc Transformer có một số lợi thế, như khả năng song song hóa, khả năng bắt chước các phụ thuộc xa, và khả năng tổng quát tốt hơn nhờ self-attention. Nó đã đạt được kết quả tiên tiến trên nhiều nhiệm vụ NLP, bao gồm dịch máy, tóm tắt văn bản và hiểu ngôn ngữ.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#self-attention",
    "href": "machine-learning/transformer.html#self-attention",
    "title": "Transformer",
    "section": "",
    "text": "Self-attention, còn được gọi là intra-attention hay scaled dot-product attention (tích vô hướng giữa 2 vector), là một cơ chế tính toán trọng số chú ý cho các phần tử trong cùng một chuỗi đầu vào. Nó cho phép mô hình đánh giá mức độ quan trọng của mỗi phần tử trong chuỗi dựa trên mức liên quan của nó đối với các phần tử khác.\n\n\n\nTrong self-attention, mỗi từ trong chuỗi đầu vào được biểu diễn bằng một vector, thông thường được gọi là nhúng hay vector nhúng (embedding vector). Giả sử nhúng từ cho chuỗi đầu vào như sau:\nVí dụ câu như sau: “Con mèo ngồi trên chiếc thảm.”\nĐể áp dụng self-attention, chúng ta trước tiên biểu diễn mỗi từ trong câu thành một vector nhúng. Hãy giả sử chúng ta có các vector nhúng từ sau đây:\nNhúng từ:\n- \"Con\": [0.2, 0.5, -0.3]\n- \"mèo\": [-0.1, 0.7, 0.2]\n- \"ngồi\": [0.4, -0.2, 0.6]\n- \"trên\": [-0.5, 0.3, -0.1]\n- \"chiếc\": [0.2, 0.5, -0.3]\n- \"thảm\": [0.3, 0.1, 0.8]\n- \".\": [0.0, 0.0, 0.0]\nBây giờ, chúng ta tính toán trọng số self-attention cho mỗi từ trong câu. Trong self-attention, mỗi từ được so sánh với tất cả các từ khác để xác định độ quan trọng hoặc liên quan của nó. Quá trình so sánh được thực hiện bằng cách tính tích vô hướng giữa nhúng từ, sau đó áp dụng phép softmax để thu được trọng số chú ý.\nVí dụ, đối với từ “Con,” tích vô hướng được tính toán với nhúng từ của tất cả các từ khác:\nTrọng số Chú ý cho “Con”:\n- \"Con\" so sánh với \"Con\": 0.3\n- \"Con\" so sánh với \"mèo\": 0.1\n- \"Con\" so sánh với \"ngồi\": 0.2\n- \"Con\" so sánh với \"trên\": 0.05\n- \"Con\" so sánh với \"chiếc\": 0.35\n- \"Con\" so sánh với \"thảm\": 0.0\n- \"Con\" so sánh với \".\": 0.0\nCác trọng số chú ý phản ánh sự quan trọng của mỗi từ đối với “Con” dựa trên sự tương đồng ngữ nghĩa hoặc liên quan ngữ cảnh của chúng. Phép softmax đảm bảo rằng các trọng số tổng cộng bằng 1.\nChúng ta lặp lại quá trình này cho mỗi từ trong câu để thu được trọng số self-attention cho toàn bộ câu. Những trọng số chú ý này sau đó có thể được sử dụng để tổng hợp thông tin từ các từ khác nhau và bắt chước các mối quan hệ quan trọng trong câu để tiếp tục xử lý trong mô hình.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#multi-head-attention",
    "href": "machine-learning/transformer.html#multi-head-attention",
    "title": "Transformer",
    "section": "",
    "text": "Multi-Head Attention là một phần mở rộng của self-attention cho phép mô hình tập trung vào các khía cạnh khác nhau của chuỗi đầu vào cùng một lúc. Đó là việc áp dụng self-attention nhiều lần song song, mỗi attention head tập trung vào một biểu diễn khác nhau của đầu vào. Bằng cách sử dụng nhiều attention head, mô hình có thể nắm bắt các thông tin khác nhau và học các mẫu đa dạng, nâng cao khả năng hiểu và xử lý các chuỗi phức tạp một cách hiệu quả.\n\n\n\n\nXem xét một câu tiếng Anh: “I love cats.” -&gt; Và chúng ta muốn dịch nó sang tiếng Pháp: “J’adore les chats.”\nGiả sử cả hai câu tiếng Anh và tiếng Pháp đã được nhúng thành các vector như sau:\nNhúng từ tiếng Anh:\n\n“I”: [0.1, 0.2, 0.3]\n“love”: [0.4, 0.5, 0.6]\n“cats”: [0.7, 0.8, 0.9]\n\nNhúng từ tiếng Pháp:\n\n“J’adore”: [0.9, 0.8, 0.7]\n“les”: [0.6, 0.5, 0.4]\n“chats”: [0.3, 0.2, 0.1]\n\nTính toán trọng số\n\nGiả sử chúng ta đang sử dụng cơ chế multi-head attention với 2 attention head. Mỗi head sẽ tập trung vào chuỗi đầu vào tiếng Anh (nguồn) và chuỗi đầu vào tiếng Pháp (đích) riêng biệt.\nVới mỗi attention head, chúng ta sẽ tính toán trọng số attention cho cả chuỗi đầu vào tiếng Anh và tiếng Pháp. Các trọng số attention xác định mức độ quan trọng của mỗi từ trong chuỗi nguồn liên quan đến các từ trong chuỗi đích.\nVí dụ,\n\nVới attention head đầu tiên, chúng ta tính toán trọng số attention cho chuỗi đầu vào tiếng Anh:\nAttention_scores_source_head1 = softmax(dot_product(english_source_embeddings, [french_target_embedding1, french_target_embedding2, french_target_embedding3]))\nTương tự, chúng ta tính toán trọng số attention cho chuỗi đích tiếng Pháp:\nAttention_scores_target_head1 = softmax(dot_product(french_target_embeddings, [english_source_embedding1, english_source_embedding2, english_source_embedding3]))\nLặp lại các bước trên cho attention head tiếp theo, kết quả sẽ được các attention score cho cả chuỗi nguồn và chuỗi đích.\nSau khi có các trọng số attention, chúng ta áp dụng chúng vào nhúng tương ứng để có tổng có trọng số cho mỗi chuỗi.\nVí dụ, đối với chuỗi nguồn tiếng Anh và attention head đầu tiên:\nWeighted_sum_source_head1 = attention_score1 * english_source_embedding1 + attention_score2 * english_source_embedding2 + attention_score3 * english_source_embedding3\nTương tự, đối với chuỗi đích tiếng Pháp và attention head đầu tiên:\nWeighted_sum_target_head1 = attention_score1 * french_target_embedding1 + attention_score2 * french_target_embedding2 + attention_score3 * french_target_embedding3\nChúng ta lặp lại các bước trên cho attention head thứ hai.\nCuối cùng, chúng ta nối các đầu ra từ mỗi attention head và thông qua một phép biến đổi tuyến tính để có đầu ra multi-head attention cuối cùng.\nNhư vậy, cơ chế multi-head attention cho phép mô hình nắm bắt các thông tin và mối quan hệ khác nhau giữa chuỗi nguồn và chuỗi đích, tăng khả năng dịch giữa các ngôn ngữ một cách chính xác.\n\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.query_projection = nn.Linear(embed_dim, embed_dim)\n        self.key_projection = nn.Linear(embed_dim, embed_dim)\n        self.value_projection = nn.Linear(embed_dim, embed_dim)\n        self.output_projection = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n        \n        # Apply linear projections for query, key, and value\n        query = self.query_projection(query)\n        key = self.key_projection(key)\n        value = self.value_projection(value)\n        \n        # Reshape query, key, and value to split heads\n        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Compute attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n        attention_weights = torch.softmax(scores, dim=-1)\n        \n        # Apply attention weights to value\n        attended_values = torch.matmul(attention_weights, value)\n        \n        # Reshape and concatenate attended values\n        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n        \n        # Apply linear projection for output\n        output = self.output_projection(attended_values)\n        \n        return output",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#positional-encoding",
    "href": "machine-learning/transformer.html#positional-encoding",
    "title": "Transformer",
    "section": "",
    "text": "Phép mã hóa vị trí (Positional Encoding) là một kỹ thuật được sử dụng trong các mô hình dựa trên chuỗi, đặc biệt là trong kiến trúc Transformer, để đưa thông tin về vị trí tương đối của các yếu tố trong chuỗi đầu vào. Nó giải quyết vấn đề rằng tự chú ý (self-attention) và các cơ chế chú ý khác trong các mô hình này không ngầm định mã hóa thông tin về vị trí.\nTrong các nhiệm vụ xử lý ngôn ngữ tự nhiên, thứ tự của các từ trong một câu mang ý nghĩa và ngữ cảnh quan trọng. Tuy nhiên, mô hình Transformer xử lý chuỗi đầu vào theo cách song song, không rõ ràng ghi nhận thứ tự tuần tự. Phép mã hóa vị trí giúp mô hình phân biệt giữa các yếu tố ở các vị trí khác nhau trong chuỗi.\nMã hóa vị trí được thêm vào nhúng đầu vào của chuỗi trước khi đưa chúng vào mô hình Transformer. Thông thường, điều này được thực hiện bằng cách kết hợp các hàm sin-cos với các nhúng đầu vào. Mỗi chiều của mã hóa vị trí đại diện cho một vị trí cụ thể trong chuỗi.\nSự lựa chọn các hàm sin-cos cho phép mô hình dễ dàng mở rộng cho các độ dài chuỗi dài hơn so với những gì mô hình đã được huấn luyện. Tần số của các hàm sin-cos tuân theo một cấp số học, cho phép mô hình học để chú ý đến các vị trí khác nhau với các tỷ lệ khác nhau.\nBằng cách thêm mã hóa vị trí vào nhúng đầu vào, mô hình Transformer có thể hiệu quả nắm bắt thứ tự và vị trí tương đối của các yếu tố trong chuỗi. Điều này giúp mô hình hiểu các phụ thuộc và mối quan hệ giữa các vị trí khác nhau và cho phép thực hiện các nhiệm vụ yêu cầu xử lý thông tin tuần tự, chẳng hạn như dịch máy hoặc hiểu ngôn ngữ.\n\n\n\n\nGiả sử chúng ta có một chuỗi đầu vào gồm ba từ: [“Tôi”, “yêu”, “mèo”].\nMỗi từ sẽ được biểu diễn bằng một vector nhúng từ, nắm bắt ý nghĩa ngữ nghĩa của nó. Hãy giả định các vector nhúng từ như sau:\n\n“Tôi”: [0.2, 0.3, -0.1]\n“yêu”: [-0.5, 0.7, 0.2]\n“mèo”: [0.7, 0.8, 0.9]\n\nĐể tích hợp thông tin về vị trí, chúng ta sử dụng mã hóa vị trí. Trong ví dụ này, chúng ta sẽ sử dụng các hàm sin và cos để tạo các vector mã hóa vị trí.\nGiả sử chiều nhúng là 3, các vector mã hóa vị trí sẽ được tính như sau:\nCho vị trí 0 (từ đầu tiên): - PE(0) = [sin(0/10000^0), cos(0/10000^0), sin(0/10000^0)]\nCho vị trí 1 (từ thứ hai): - PE(1) = [sin(1/10000^1), cos(1/10000^1), sin(1/10000^1)]\nCho vị trí 2 (từ thứ ba): - PE(2) = [sin(2/10000^0), cos(2/10000^0), sin(2/10000^0)]\nVì sin(0) = 0, sin(1) = 0.8415, sin(2) = 0.9093, cos(0) = 1, cos(1) = 0.5403, cos(2) = -0.4161.\nNên, các vector mã hóa vị trí là:\nPE(0) = [0, 1, 0] PE(1) = [0.8415, 0.5403, 0.8415] PE(2) = [0.9093, -0.4161, 0.9093]\nĐể tích hợp mã hóa vị trí, chúng ta cộng vector mã hóa vị trí tương ứng vào mỗi vector nhúng từ:\n\nTừ “Tôi” với mã hóa vị trí: [0.2, 0.3, -0.1] + [0, 1, 0] = [0.2, 1.3, -0.1]\nTừ “yêu” với mã hóa vị trí: [-0.5, 0.7, 0.2] + [0.8415, 0.5403, 0.8415] =\n\n[0.3415, 1.2403, 1.0415] - Từ “mèo” với mã hóa vị trí: [0.7, 0.8, 0.9] + [0.9093, -0.4161, 0.9093] = [1.6093, 0.3839, 1.8093]\nBằng cách cộng các vector mã hóa vị trí vào các vector nhúng từ, chúng ta giới thiệu thông tin về vị trí tương đối của các từ trong chuỗi đầu vào. Điều này cho phép mô hình Transformer nắm bắt thứ tự tuần tự và các phụ thuộc giữa các từ, điều quan trọng cho các nhiệm vụ như dịch máy hoặc phân tích cảm xúc.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#layer-normalization",
    "href": "machine-learning/transformer.html#layer-normalization",
    "title": "Transformer",
    "section": "",
    "text": "Layer Normalization là một kỹ thuật được sử dụng trong các mô hình học sâu để chuẩn hóa các kích hoạt (kết quả chuyển đổi đầu vào thành đầu ra bằng hàm kích hoạt - tạm gọi là các kích hoạt) của các đơn vị trong một lớp. Nó giải quyết vấn đề của sự thay đổi phân phối đầu vào của lớp trong quá trình huấn luyện, gây khó khăn cho quá trình học.\nCông thức cho Layer Normalization có thể được biểu diễn như sau:\noutput = scale * (input - mean) / sqrt(variance + epsilon) + bias\n\nỞ đây, đầu vào đại diện cho các kích hoạt đầu vào của một lớp, trung bình và phương sai đại diện cho giá trị trung bình và phương sai được tính theo chiều đầu vào, epsilon là một hằng số nhỏ được thêm vào để đảm bảo tính ổn định số học, và scale và bias là các tham số có thể học giúp mô hình điều chỉnh và dịch chuyển các kích hoạt đã được chuẩn hóa.\nLayer Normalization được áp dụng độc lập cho mỗi lô dữ liệu (batch), điều này làm cho nó phù hợp cho các nhiệm vụ với kích thước lô nhỏ. Nó đã được chứng minh là hiệu quả trong ổn định quá trình huấn luyện, tăng tốc quá trình hội tụ và cải thiện hiệu suất tổng quát của các mô hình học sâu.\nTổng quát, Layer Normalization giúp giảm sự phụ thuộc vào tỷ lệ của các kích hoạt đầu vào, thúc đẩy việc truyền gradient ổn định và cải thiện động học huấn luyện tổng thể (overall training dynamics - gồm learning rate, gradient, loss rate) của mạng neural. Nó đã được sử dụng rộng rãi trong các lĩnh vực khác nhau, bao gồm xử lý ngôn ngữ tự nhiên, thị giác máy tính và nhận dạng giọng nói.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#masking",
    "href": "machine-learning/transformer.html#masking",
    "title": "Transformer",
    "section": "",
    "text": "Giả sử chúng ta có một câu: “Tôi thích chơi bóng đá.”\nTrong mô hình ngôn ngữ, mục tiêu là dự đoán từ tiếp theo trong ngữ cảnh đã cho. Tuy nhiên, trong quá trình huấn luyện, chúng ta thường không muốn cho mô hình truy cập vào các từ trong tương lai vì nó có thể dẫn đến rò rỉ dữ liệu và dự đoán không thực tế. Vì vậy, chế che được áp dụng.\nĐể áp dụng cơ chế che, thường được ký hiệu là [MASK], để thay thế một số từ trong chuỗi đầu vào trong quá trình huấn luyện. Trong trường hợp này, chúng ta có thể ngẫu nhiên che một số từ trong câu, dẫn đến:\nCâu Đầu Vào (có cơ chế che): “Tôi thích chơi [MASK] bóng đá.”\nBây giờ, trong quá trình huấn luyện, mục tiêu của mô hình là dự đoán từ bị che dựa trên ngữ cảnh xung quanh. Nó học cách hiểu cấu trúc câu, ý nghĩa và sự phụ thuộc giữa các từ để đưa ra dự đoán chính xác. Mô hình nhận chuỗi đầu vào đã được chỉnh sửa (thêm [MASK]) và cố gắng dự đoán bản gốc, không bị che, ở vị trí đã được che.\nTrong quá trình huấn luyện hoặc đánh giá, cơ chế che không được sử dụng và mô hình được cung cấp với toàn bộ chuỗi đầu vào. Nó có thể tạo ra dự đoán cho từ tiếp theo trong chuỗi dựa trên kiến thức đã học từ quá trình huấn luyện.\nCơ chế che giúp mô hình tổng quát tốt và xử lý các trường hợp mà nó gặp phải các từ chưa được nhìn thấy hoặc thiếu. Nó khuyến khích mô hình dựa vào ngữ cảnh có sẵn để đưa ra dự đoán chính xác và cải thiện khả năng hiểu cấu trúc câu.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html",
    "href": "machine-learning/delong_test.html",
    "title": "Delong method for AUC interval and AUC test",
    "section": "",
    "text": "AUC là viết tắt của “Area Under the ROC Curve” trong tiếng Anh, dịch sang tiếng Việt là “Diện tích dưới đường cong ROC”. Đây là một phép đo quan trọng trong đánh giá hiệu suất của mô hình phân loại (classifier) trong trường hợp binary classification (phân loại nhị phân).\nĐể hiểu AUC, hãy cùng nhau xem xét đến đường cong ROC (Receiver Operating Characteristic curve). ROC curve biểu diễn sự tương quan giữa tỷ lệ True Positive Rate (TPR) và tỷ lệ False Positive Rate (FPR) của mô hình phân loại, khi ngưỡng phân loại thay đổi từ 0 đến 1.\n\nTrue Positive Rate (TPR), còn gọi là Sensitivity hoặc Recall, đo lường tỷ lệ các trường hợp positive mà mô hình dự đoán chính xác.\nFalse Positive Rate (FPR) đo lường tỷ lệ các trường hợp negative bị phân loại sai (tức là dự đoán nhầm thành positive).\n\nAUC là diện tích nằm dưới đường cong ROC và nó thể hiện khả năng của mô hình phân loại phân biệt giữa hai lớp positive và negative. AUC càng lớn thì mô hình càng có khả năng phân loại tốt hơn, với AUC = 1 thể hiện mô hình hoàn hảo, trong khi AUC = 0.5 chỉ ra mô hình không khác biệt so với một dự đoán ngẫu nhiên.\nAUC là một phép đo định lượng và phổ biến trong đánh giá hiệu suất của các mô hình phân loại như Logistic Regression, Support Vector Machines (SVM), Random Forest, Neural Networks, và nhiều mô hình khác.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\naSAH = pd.read_excel(\"aSAH.xlsx\")\n\ny_true_str = aSAH['outcome']\ny_pred = aSAH['s100b']\n\n# Create a dictionary to map string labels to binary labels\nlabel_mapping = {\"Poor\": 1, \"Good\": 0}\n\n# Map true labels to binary labels\ny_true = [label_mapping[label] for label in y_true_str]\n\n# Tính đường cong ROC và AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_pred)\nroc_auc = auc(fpr, tpr)\n\n# Vẽ biểu đồ AUC\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#auc",
    "href": "machine-learning/delong_test.html#auc",
    "title": "Delong method for AUC interval and AUC test",
    "section": "",
    "text": "AUC là viết tắt của “Area Under the ROC Curve” trong tiếng Anh, dịch sang tiếng Việt là “Diện tích dưới đường cong ROC”. Đây là một phép đo quan trọng trong đánh giá hiệu suất của mô hình phân loại (classifier) trong trường hợp binary classification (phân loại nhị phân).\nĐể hiểu AUC, hãy cùng nhau xem xét đến đường cong ROC (Receiver Operating Characteristic curve). ROC curve biểu diễn sự tương quan giữa tỷ lệ True Positive Rate (TPR) và tỷ lệ False Positive Rate (FPR) của mô hình phân loại, khi ngưỡng phân loại thay đổi từ 0 đến 1.\n\nTrue Positive Rate (TPR), còn gọi là Sensitivity hoặc Recall, đo lường tỷ lệ các trường hợp positive mà mô hình dự đoán chính xác.\nFalse Positive Rate (FPR) đo lường tỷ lệ các trường hợp negative bị phân loại sai (tức là dự đoán nhầm thành positive).\n\nAUC là diện tích nằm dưới đường cong ROC và nó thể hiện khả năng của mô hình phân loại phân biệt giữa hai lớp positive và negative. AUC càng lớn thì mô hình càng có khả năng phân loại tốt hơn, với AUC = 1 thể hiện mô hình hoàn hảo, trong khi AUC = 0.5 chỉ ra mô hình không khác biệt so với một dự đoán ngẫu nhiên.\nAUC là một phép đo định lượng và phổ biến trong đánh giá hiệu suất của các mô hình phân loại như Logistic Regression, Support Vector Machines (SVM), Random Forest, Neural Networks, và nhiều mô hình khác.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\naSAH = pd.read_excel(\"aSAH.xlsx\")\n\ny_true_str = aSAH['outcome']\ny_pred = aSAH['s100b']\n\n# Create a dictionary to map string labels to binary labels\nlabel_mapping = {\"Poor\": 1, \"Good\": 0}\n\n# Map true labels to binary labels\ny_true = [label_mapping[label] for label in y_true_str]\n\n# Tính đường cong ROC và AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_pred)\nroc_auc = auc(fpr, tpr)\n\n# Vẽ biểu đồ AUC\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#liên-hệ-auc-và-thống-kê-mann-whitney",
    "href": "machine-learning/delong_test.html#liên-hệ-auc-và-thống-kê-mann-whitney",
    "title": "Delong method for AUC interval and AUC test",
    "section": "Liên hệ AUC và thống kê Mann-Whitney",
    "text": "Liên hệ AUC và thống kê Mann-Whitney\n\nThống kê Mann-Whitney (U-test):\nGiả sử bạn có hai nhóm dữ liệu độc lập, \\(X\\) và \\(Y\\), với kích thước lần lượt là \\(n_x\\) và \\(n_y\\). Để tính U-statistic, bạn cần xếp hạng dữ liệu trong mỗi nhóm và tính tổng các xếp hạng trong nhóm \\(X\\), ký hiệu là \\(R_X\\). Sau đó, U-statistic được tính bằng công thức:\n\\[ U = n_x \\cdot n_y + \\frac{n_x \\cdot (n_x + 1)}{2} - R_X \\]\nNếu \\(U\\) là giá trị U-statistic (thống kê Mann-Whitney), thì AUC được tính như sau:\n\\[ AUC = \\frac{U - \\frac{n_x \\cdot (n_x + 1)}{2}}{n_x \\cdot n_y} \\]\nDeLong’s test:\nGiả sử bạn có hai mô hình phân loại và đã tính được các xác suất dự đoán của chúng trên tập dữ liệu kiểm tra. Để thực hiện kiểm định DeLong, bạn cần tính U-statistic và phương sai của U-statistic (Var(U)) như đã giải thích trước đó.\nTiếp theo, để tính Z-score, bạn cần có AUC của hai mô hình (\\(AUC_1\\) và \\(AUC_2\\)) và Var(U) như sau:\n\\[ Z = \\frac{AUC_1 - AUC_2}{\\sqrt{\\text{Var}(U)}} \\]\nSau đó, sử dụng Z-score để tính p-value, và nếu p-value nhỏ hơn mức ý nghĩa đã chọn (thường là 0.05), bạn có thể kết luận rằng có sự khác biệt đáng kể giữa AUC của hai mô hình.\nCông thức tính khoảng tin cậy (confidence interval) cho AUC dựa trên phương pháp DeLong như sau:\nGiả sử bạn đã tính được U-statistic (U) và Var(U) từ kiểm định DeLong, thì khoảng tin cậy 95% cho AUC sẽ được tính bằng công thức sau:\n\\[ \\text{CI}_{\\text{lower}} = AUC - z_{\\alpha/2} \\cdot \\sqrt{\\text{Var}(U)} \\]\n\\[ \\text{CI}_{\\text{upper}} = AUC + z_{\\alpha/2} \\cdot \\sqrt{\\text{Var}(U)} \\]\nTrong đó:\n\nAUC là diện tích dưới đường cong ROC đã tính từ mô hình phân loại và tập dữ liệu.\nVar(U) là phương sai của U-statistic, tính từ kiểm định DeLong.\n\\(z_{\\alpha/2}\\) là giá trị thống kê từ phân phối chuẩn tương ứng với mức đáng tin cậy 95%. Với mức đáng tin cậy 95%, \\(\\alpha = 0.05\\), nên \\(z_{\\alpha/2}\\) tương ứng với giá trị thống kê 1.96.",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#thuật-toán-sun-và-xu",
    "href": "machine-learning/delong_test.html#thuật-toán-sun-và-xu",
    "title": "Delong method for AUC interval and AUC test",
    "section": "Thuật toán Sun và Xu",
    "text": "Thuật toán Sun và Xu\nThuật toán được đề xuất bởi Sun và Xu trong bài báo “Fast Implementation of DeLong’s Algorithm for Comparing the Areas Under Correlated Receiver Operating Characteristic Curves” cung cấp một cách tính toán hiệu suất và nhanh chóng để tính phương sai của sự khác biệt giữa diện tích dưới đường cong ROC (AUC) bằng phương pháp DeLong cho các đường cong ROC tương quan. Thuật toán của họ giảm đáng kể độ phức tạp tính toán so với phương pháp DeLong gốc, làm cho nó phù hợp cho các tập dữ liệu lớn.\nDưới đây là tổng quan về các bước của thuật toán:\n\nAUC được tính theo công thức\n\\[AUC = \\hat{\\theta} = \\frac{\\sum_{i=1}^{m} R_i - \\frac{m \\cdot (m + 1)}{2}}{m \\cdot n} = \\frac{1}{mn} \\sum_{i=1}^{m} \\mathbb{T}_{Z}(X_{i}) - \\frac{m+1}{2n}\\]\nTrong đó:\n\nm, n lần lượt là số lượng nhãn 1, 0\n\\(R_i={T}_{Z}(X_{i})\\) là giá trị score hoặc PD được sắp xếp theo thứ tự giảm dần (trường hợp bad = 1, good = 0). \\({T}_{Z}(X_{i})\\) được tính theo hàm Heaviside\nHàm Heaviside \\(\\mathcal{H}(t)\\):\n\n\\[ \\mathcal{H}(t) = \\begin{cases}1 & t &gt; 0 \\\\ \\frac{1}{2} & t = 0 \\\\ 0 & t &lt; 0\\end{cases} \\]\n\nMối liên hệ giữa Mid-Ranks và Hàm Heaviside:\n\n\\[ \\mathbb{T}_{\\mathcal{Z}}(\\mathcal{Z}_{i}) = \\sum_{j=1}^{M} \\mathcal{H}(\\mathcal{Z}_{i} - \\mathcal{Z}_{j}) + \\frac{1}{2} \\]\nTính Z-score và p-value: Tính Z-score bằng cách sử dụng công thức:\n\\[ z \\triangleq \\frac{\\hat{\\theta}^{(1)}-\\hat{\\theta}^{(2)}}{\\sqrt{\\mathbb{V}\\left[\\hat{\\theta}^{(1)}-\\hat{\\theta}^{(2)}\\right]}}=\\frac{\\hat{\\theta}^{(1)}-\\hat{\\theta}^{(2)}}{\\sqrt{\\mathbb{V}\\left[\\hat{\\theta}^{(1)}\\right]+\\mathbb{V}\\left[\\hat{\\theta}^{(2)}\\right]-2 \\mathbb{C}\\left[\\hat{\\theta}^{(1)}, \\hat{\\theta}^{(2)}\\right]}} \\]\nTrong đó:\n\n\\(\\hat{\\theta}^{(1)}\\) và \\(\\hat{\\theta}^{(2)}\\): Đây là hai ước tính của tham số \\(\\theta\\) hay AUC trong hai mẫu khác nhau mà đang so sánh.\n\\(\\mathbb{V}\\left[\\hat{\\theta}^{(1)}\\right]\\) và \\(\\mathbb{V}\\left[\\hat{\\theta}^{(2)}\\right]\\): Đây là phương sai của hai ước tính \\(\\hat{\\theta}^{(1)}\\) và \\(\\hat{\\theta}^{(2)}\\) tương ứng.\n\\(\\mathbb{C}\\left[\\hat{\\theta}^{(1)}, \\hat{\\theta}^{(2)}\\right]\\): Đây là hiệp phương sai giữa hai ước tính \\(\\hat{\\theta}^{(1)}\\) và \\(\\hat{\\theta}^{(2)}\\).\n\nSau đó, tính giá trị p-value từ giá trị phân phối chuẩn Z-score.\nKết quả: Dựa vào giá trị p-value tính toán được và một ngưỡng ý nghĩa đã chọn (ví dụ, 0.05), bạn có thể xác định xem sự khác biệt trong AUC giữa hai mô hình có ý nghĩa thống kê hay không.\n\n\ndef midrank(x):\n    J, Z = zip(*sorted(enumerate(x), key=lambda x:x[1]))\n    J = list(J)\n    Z = list(Z)\n    Z.append(Z[-1]+1)\n    N = len(x)\n    T = np.zeros(N)\n\n    i = 1\n    while i &lt;= N:\n        a = i\n        j = a\n        while Z[j-1] == Z[a-1]:\n            j = j + 1\n        b = j - 1\n        for k in range(a, b+1):\n            T[k-1] = (a + b) / 2\n        i = b + 1\n    T2 = np.zeros(N)\n    T2[J] = T\n\n    return T2\n\ndef fastDeLong(samples):\n    # %FASTDELONGCOV\n    # %The fast version of DeLong's method for computing the covariance of\n    # %unadjusted AUC.\n    # %% Reference:\n    # % @article{sun2014fast,\n    # %   title={Fast Implementation of DeLong's Algorithm for Comparing the Areas Under Correlated Receiver Operating Characteristic Curves},\n    # %   author={Xu Sun and Weichao Xu},\n    # %   journal={IEEE Signal Processing Letters},\n    # %   volume={21},\n    # %   number={11},\n    # %   pages={1389--1393},\n    # %   year={2014},\n    # %   publisher={IEEE}\n    # % }\n    # %% [aucs, delongcov] = fastDeLong(samples)\n    # %%\n    # % Edited by Xu Sun.\n    # % Homepage: https://pamixsun.github.io\n    # % Version: 2014/12\n    # %%\n\n    if np.sum(samples.spsizes) != samples.ratings.shape[1] or len(samples.spsizes) != 2:\n        assert False, 'Argument mismatch error'\n\n    z = samples.ratings    \n    m, n = samples.spsizes\n    x = z[:, :m]    \n    y = z[:, m:]\n    k = z.shape[0]\n\n    tx = np.zeros([k, m])\n    ty = np.zeros([k, n])\n    tz = np.zeros([k, m + n])\n    for r in range(k):\n        tx[r, :] = midrank(x[r, :])\n        ty[r, :] = midrank(y[r, :])\n        tz[r, :] = midrank(z[r, :])\n\n    aucs = np.sum(tz[:, :m], axis=1) / m / n - float(m + 1.0) / 2.0 / n\n    v01 = (tz[:, :m] - tx[:, :]) / n\n    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n    sx = np.cov(v01)\n    sy = np.cov(v10)\n    delongcov = sx / m + sy / n\n\n    return aucs, delongcov, v01, v10\n\n\nclass Samples:\n    def __init__(self, y_true, y_score1, y_score2):         \n        n_samples = len(y_true)\n        m = int(sum(y_true))\n        n = n_samples - m\n\n        # Zip the lists together\n        zipped_data = list(zip(y_true, y_score1, y_score2))\n\n        # Sort the zipped data based on labels in decreasing order\n        sorted_data = sorted(zipped_data, key=lambda x: x[0], reverse=True)\n\n        # Unzip the sorted data to get separate sorted values and labels\n        sorted_y_true, sorted_y_score1, sorted_y_score2 = zip(*sorted_data)\n    \n        # Stack the sorted scores into a ratings matrix\n        ratings = np.vstack((sorted_y_score1, sorted_y_score2))\n        \n        # Create a tuple with the sample sizes\n        spsizes = (m, n)\n        \n        self.ratings = ratings\n        self.spsizes = spsizes\n        \nimport numpy as np\nimport pandas as pd\naSAH = pd.read_excel(\"aSAH.xlsx\")\n\ny_true_str = aSAH['outcome']\ny_pred1 = aSAH['s100b']\ny_pred2 = aSAH['wfns']\n\n# Create a dictionary to map string labels to binary labels\nlabel_mapping = {\"Poor\": 1, \"Good\": 0}\n\n# Map true labels to binary labels\ny_true = [label_mapping[label] for label in y_true_str]\n\nsamples = Samples(y_true, y_pred1, y_pred2)\n\naucs, delongcov, v01, v10 = fastDeLong(samples)\n# Print the results\nprint(\"AUCs:\", aucs)\nprint(\"DeLong Covariance:\", delongcov)\n\nAUCs: [0.73136856 0.82367886]\nDeLong Covariance: [[0.00266868 0.00119616]\n [0.00119616 0.00146991]]\n\n\n\nKiểm định sự khác biệt AUC của 2 mô hình theo Delong\n\nimport scipy\ndef calpvalue(aucs, sigma):\n    l = np.array([[1, -1]])\n    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n    pvalue = 2 * (1 - scipy.stats.norm.cdf(z, loc=0, scale=1))\n    return pvalue\n\nAUC_DS, C, _, _ = fastDeLong(samples)\np_value = calpvalue(AUC_DS, C)[0][0]\n\nprint(f\"AUC Model 1: {AUC_DS[0]:.3f}\")\nprint(f\"AUC Model 2: {AUC_DS[1]:.3f}\")\nprint(\"P-value for DeLong test:\", p_value)\n# So sánh AUC của hai mô hình\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Có sự khác biệt đáng kể giữa AUC của hai mô hình.\")\nelse:\n    print(\"Không có sự khác biệt đáng kể giữa AUC của hai mô hình.\")\n\nAUC Model 1: 0.731\nAUC Model 2: 0.824\nP-value for DeLong test: 0.02717578222918804\nCó sự khác biệt đáng kể giữa AUC của hai mô hình.",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#so-sánh-auc-của-hai-mô-hình-bằng-bootstrap",
    "href": "machine-learning/delong_test.html#so-sánh-auc-của-hai-mô-hình-bằng-bootstrap",
    "title": "Delong method for AUC interval and AUC test",
    "section": "So sánh AUC của hai mô hình bằng bootstrap",
    "text": "So sánh AUC của hai mô hình bằng bootstrap\n\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\n# Tạo dữ liệu mẫu với độ dài chuỗi y_true là 1000\n# Tính AUC cho hai mô hình\nauc_model1 = roc_auc_score(y_true, predictions_model1)\nauc_model2 = roc_auc_score(y_true, predictions_model2)\n\n# Thực hiện bootstrap để tính khoảng tin cậy 95% cho AUC của cả hai mô hình\nn_iterations = 1000  # Số lượng lần lấy mẫu bootstrap\nn_size = len(y_true)  # Số lượng phần tử trong chuỗi y_true\n\nauc_scores_model1 = []\nauc_scores_model2 = []\n\nfor _ in range(n_iterations):\n    # Lấy ngẫu nhiên mẫu từ dữ liệu với replacement\n    indices = np.random.randint(0, n_size, n_size)\n    y_true_bootstrap = np.array(y_true)[indices]\n    y_pred_bootstrap_model1 = predictions_model1[indices]\n    y_pred_bootstrap_model2 = predictions_model2[indices]\n    \n    # Tính AUC cho mẫu lấy ra từ dữ liệu bootstrap cho hai mô hình\n    auc_score_model1 = roc_auc_score(y_true_bootstrap, y_pred_bootstrap_model1)\n    auc_score_model2 = roc_auc_score(y_true_bootstrap, y_pred_bootstrap_model2)\n    \n    auc_scores_model1.append(auc_score_model1)\n    auc_scores_model2.append(auc_score_model2)\n\n# Tính khoảng tin cậy 95% cho AUC cho cả hai mô hình\nlower_bound_model1 = np.percentile(auc_scores_model1, 2.5)\nupper_bound_model1 = np.percentile(auc_scores_model1, 97.5)\n\nlower_bound_model2 = np.percentile(auc_scores_model2, 2.5)\nupper_bound_model2 = np.percentile(auc_scores_model2, 97.5)\n\nprint(f\"AUC Model 1: {auc_model1:.3f} (95% CI: [{lower_bound_model1:.3f}, {upper_bound_model1:.3f}])\")\nprint(f\"AUC Model 2: {auc_model2:.3f} (95% CI: [{lower_bound_model2:.3f}, {upper_bound_model2:.3f}])\")\n\n# So sánh khoảng tin cậy của AUC của hai mô hình\nif lower_bound_model1 &gt; upper_bound_model2 or lower_bound_model2 &gt; upper_bound_model1:\n    print(\"Có sự khác biệt đáng kể giữa AUC của hai mô hình.\")\nelse:\n    print(\"Không có sự khác biệt đáng kể giữa AUC của hai mô hình.\")\n\nAUC Model 1: 0.523 (95% CI: [0.403, 0.638])\nAUC Model 2: 0.506 (95% CI: [0.383, 0.613])\nKhông có sự khác biệt đáng kể giữa AUC của hai mô hình.",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#tính-khoảng-tin-cậy-auc-sử-dụng-phương-pháp-delong",
    "href": "machine-learning/delong_test.html#tính-khoảng-tin-cậy-auc-sử-dụng-phương-pháp-delong",
    "title": "Delong method for AUC interval and AUC test",
    "section": "Tính khoảng tin cậy AUC sử dụng phương pháp Delong",
    "text": "Tính khoảng tin cậy AUC sử dụng phương pháp Delong\n\nfrom scipy.stats import norm\ndef calculate_auc_ci(y_true, y_score, alpha = 0.05):\n    samples = Samples(y_true, y_score, y_score)    \n\n    auc, auc_cov,_,_ = fastDeLong(samples)\n    auc = auc[0]\n    var_U = auc_cov[0,0]    \n    \n    z_score = norm.ppf(1 - alpha / 2)\n    lower_bound = auc - z_score * np.sqrt(var_U)\n    upper_bound = auc + z_score * np.sqrt(var_U)\n  \n    return auc, var_U, lower_bound, upper_bound\n\nauc_model, _, lower_bound, upper_bound = calculate_auc_ci(y_true, y_pred)\nprint(f\"AUC: {auc_model:.3f}\")\nprint(f\"95% Confidence Interval for AUC using bootstrap method: [{lower_bound:.3f}, {upper_bound:.3f}]\")\n\nAUC: 0.731\n95% Confidence Interval for AUC using bootstrap method: [0.630, 0.833]",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#tính-khoảng-tin-cậy-auc-sử-dụng-phương-pháp-bootstrap",
    "href": "machine-learning/delong_test.html#tính-khoảng-tin-cậy-auc-sử-dụng-phương-pháp-bootstrap",
    "title": "Delong method for AUC interval and AUC test",
    "section": "Tính khoảng tin cậy AUC sử dụng phương pháp Bootstrap",
    "text": "Tính khoảng tin cậy AUC sử dụng phương pháp Bootstrap\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n\n# Tính AUC cho mô hình ban đầu\nauc_model = roc_auc_score(y_true, y_pred)\n\n# Số lượng lần lấy mẫu (bootstrap iterations)\nn_iterations = 1000\n\n# Số lượng mẫu lấy ra từ dữ liệu ban đầu trong mỗi lần lấy mẫu\nn_size = len(y_true)\n\n# Tạo mảng để lưu kết quả AUC từ các lần lấy mẫu\nauc_scores = []\n\nfor _ in range(n_iterations):\n    # Lấy ngẫu nhiên mẫu từ dữ liệu với replacement\n    indices = np.random.randint(0, n_size, n_size)\n    y_true_bootstrap = np.array(y_true)[indices]\n    y_pred_bootstrap = y_pred[indices]\n    \n    # Tính AUC cho mẫu lấy ra từ dữ liệu bootstrap\n    auc_score = roc_auc_score(y_true_bootstrap, y_pred_bootstrap)\n    auc_scores.append(auc_score)\n\n# Tính khoảng tin cậy 95% cho AUC\nlower_bound = np.percentile(auc_scores, 2.5)\nupper_bound = np.percentile(auc_scores, 97.5)\n\nprint(f\"AUC: {auc_model:.3f}\")\nprint(f\"95% Confidence Interval for AUC using bootstrap method: [{lower_bound:.3f}, {upper_bound:.3f}]\")\n\nAUC: 0.731\n95% Confidence Interval for AUC using bootstrap method: [0.624, 0.824]",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/CohortFitDistribution.html",
    "href": "machine-learning/CohortFitDistribution.html",
    "title": "Fit Distribution",
    "section": "",
    "text": "Phân phối bao gồm: Cauchy, Gamma, Exponential, Log-Logistic, Log-Normal Hàm mât đô xác suất (PDF) của phân phối Cauchy:\n\\[\nf\\left(x ; x_{0} ; \\gamma\\right)=\\frac{1}{\\pi \\gamma\\left[1+\\left(\\frac{x-x_{0}}{\\gamma}\\right)^{2}\\right]}=\\frac{1}{\\pi \\gamma}\\left[\\frac{\\gamma^{2}}{\\left(x-x_{0}\\right)^{2}+\\gamma^{2}}\\right]\n\\]\nTrong đó:\n\n\\(x_{0}\\) : là thông số vị trí, chỉ định vị trí đỉnh của phân phối\n\\(\\gamma\\) : là thông số tỷ lệ chỉ định nửa chiều rộng ở nửa tối đa (HWHM), \\(2 \\gamma\\) là toàn bộ chiều rộng ở mức tối đa một nửa (FWHM). \\(\\gamma\\) là một nửa phạm vi liên phần và đôi khi được gọi là lỗi có thể xảy ra\n\nHàm mât đô xác suất (PDF) của phân phối Gamma:\n\\[\nf(x ; k ; \\theta)=\\frac{x^{k-1} e^{-\\frac{x}{\\theta}}}{\\theta^{k} \\Gamma(k)}, \\text { for } x&gt;0 \\text { and } k, \\theta&gt;0\n\\]\n\n\\(k\\) : là tham số hình dạng\n\\(\\theta\\) : là tham số tỷ lệ\n\\(x\\) : là biến ngẫu nhiên\n\\(\\Gamma(k)\\) : là hàm gamma được đánh giá tại \\(\\mathrm{k}\\)\n\nHàm mật đô xác suất (PDF) của phân phối Exponential:\n\\[\nf(x ; \\lambda)=\\left\\{\\begin{array}{rr}\n0, & x&lt;0 \\\\\n\\lambda e^{-\\lambda x}, & x \\geq 0\n\\end{array}\\right.\n\\]\n\n\\(\\lambda\\) : là tham số của phân phối, thường được gọi là tham số tỷ lệ.\n\\(x\\) : là biến ngẫu nhiên\n\nHàm mật độ xác suất (PDF) của phân phối Log-Logistic:\n\\[\nf(x ; \\alpha ; \\beta)=\\frac{\\left(\\frac{\\beta}{\\alpha}\\right)\\left(\\frac{x}{\\alpha}\\right)^{\\beta-1}}{\\left(1+\\left(\\frac{x}{\\alpha}\\right)^{\\beta}\\right)^{2}}, \\text { where } x&gt;0, \\alpha&gt;0, \\beta&gt;0\n\\]\n\n\\(\\alpha\\) : là tham số tỷ lệ và là giá trị trung bình của phân phối\n\\(\\beta\\) : là tham số hình dạng\n\nHàm mât độ xác suất (PDF) của phân phối Lognormal \\(\\left(\\mu, \\sigma^{2}\\right)\\) :\n\\[\nf_{x}(x)=\\frac{d}{d x} \\operatorname{Pr}(X \\leq x)=\\frac{1}{x \\sigma \\sqrt{2 \\pi}} e^{\\left(-\\frac{(\\ln x-\\mu)^{2}}{2 \\sigma^{2}}\\right)}, \\text { where } x&gt;0, \\sigma&gt;0\n\\]\n\n\\(x\\) là biến ngẫu nhiên\nMột biến ngẫu nhiên \\(X\\) tuân theo phân phối Log-normal \\(\\left(X \\sim \\operatorname{Lognormal}\\left(\\mu_{x}, \\sigma_{x}^{2}\\right)\\right)\\) nếu \\(\\ln (X)\\) tuân theo phân phối chuẩn với giá trị trung bình là \\(\\mu\\) và phương sai \\(\\sigma^{2}\\)\n\nĐể lựa chọn phân phối phù hợp cho từng phân nhóm, Nhóm mô hình lựa chọn mô hình có SSE (Sum of square error - tổng của sai số bình phương) nhỏ nhất với SSE được tính theo công thức sau:\n\\[\n\\left.S S E=\\sum \\text { (Giá trị thực tế - Giá trị được tính ra từ mô hình phân phối }\\right)^{2}\n\\]\nCuối cùng, từ mô hình phân phối vừa được lựa chọn (là mô hình có SSE nhỏ nhất).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import cauchy, gamma, expon, lognorm, loglaplace\n\n# Define the x range for the plot\nx = np.linspace(0, 15, 1000)\n\n# Create a single row with 5 subplots\nfig, axs = plt.subplots(1, 5, figsize=(10, 3))\n\n# Cauchy Distribution\ncauchy_pdf = cauchy.pdf(x)\naxs[0].plot(x, cauchy_pdf)\naxs[0].set_title(\"Cauchy\")\n\n# Gamma Distribution\ngamma_pdf = gamma.pdf(x, a=2)\naxs[1].plot(x, gamma_pdf)\naxs[1].set_title(\"Gamma\")\n\n# Exponential Distribution\nexponential_pdf = expon.pdf(x)\naxs[2].plot(x, exponential_pdf)\naxs[2].set_title(\"Exponential\")\n\n# Log Normal Distribution\nlog_normal_pdf = lognorm.pdf(x, s=0.7)\naxs[3].plot(x, log_normal_pdf)\naxs[3].set_title(\"Log Normal\")\n\n# Log Logistic Distribution\nlog_logistic_pdf = loglaplace.pdf(x, c=0.5)\naxs[4].plot(x, log_logistic_pdf)\naxs[4].set_title(\"Log Logistic\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Fit Distribution"
    ]
  },
  {
    "objectID": "machine-learning/CohortFitDistribution.html#dạng-hàm-phân-phối",
    "href": "machine-learning/CohortFitDistribution.html#dạng-hàm-phân-phối",
    "title": "Fit Distribution",
    "section": "",
    "text": "Phân phối bao gồm: Cauchy, Gamma, Exponential, Log-Logistic, Log-Normal Hàm mât đô xác suất (PDF) của phân phối Cauchy:\n\\[\nf\\left(x ; x_{0} ; \\gamma\\right)=\\frac{1}{\\pi \\gamma\\left[1+\\left(\\frac{x-x_{0}}{\\gamma}\\right)^{2}\\right]}=\\frac{1}{\\pi \\gamma}\\left[\\frac{\\gamma^{2}}{\\left(x-x_{0}\\right)^{2}+\\gamma^{2}}\\right]\n\\]\nTrong đó:\n\n\\(x_{0}\\) : là thông số vị trí, chỉ định vị trí đỉnh của phân phối\n\\(\\gamma\\) : là thông số tỷ lệ chỉ định nửa chiều rộng ở nửa tối đa (HWHM), \\(2 \\gamma\\) là toàn bộ chiều rộng ở mức tối đa một nửa (FWHM). \\(\\gamma\\) là một nửa phạm vi liên phần và đôi khi được gọi là lỗi có thể xảy ra\n\nHàm mât đô xác suất (PDF) của phân phối Gamma:\n\\[\nf(x ; k ; \\theta)=\\frac{x^{k-1} e^{-\\frac{x}{\\theta}}}{\\theta^{k} \\Gamma(k)}, \\text { for } x&gt;0 \\text { and } k, \\theta&gt;0\n\\]\n\n\\(k\\) : là tham số hình dạng\n\\(\\theta\\) : là tham số tỷ lệ\n\\(x\\) : là biến ngẫu nhiên\n\\(\\Gamma(k)\\) : là hàm gamma được đánh giá tại \\(\\mathrm{k}\\)\n\nHàm mật đô xác suất (PDF) của phân phối Exponential:\n\\[\nf(x ; \\lambda)=\\left\\{\\begin{array}{rr}\n0, & x&lt;0 \\\\\n\\lambda e^{-\\lambda x}, & x \\geq 0\n\\end{array}\\right.\n\\]\n\n\\(\\lambda\\) : là tham số của phân phối, thường được gọi là tham số tỷ lệ.\n\\(x\\) : là biến ngẫu nhiên\n\nHàm mật độ xác suất (PDF) của phân phối Log-Logistic:\n\\[\nf(x ; \\alpha ; \\beta)=\\frac{\\left(\\frac{\\beta}{\\alpha}\\right)\\left(\\frac{x}{\\alpha}\\right)^{\\beta-1}}{\\left(1+\\left(\\frac{x}{\\alpha}\\right)^{\\beta}\\right)^{2}}, \\text { where } x&gt;0, \\alpha&gt;0, \\beta&gt;0\n\\]\n\n\\(\\alpha\\) : là tham số tỷ lệ và là giá trị trung bình của phân phối\n\\(\\beta\\) : là tham số hình dạng\n\nHàm mât độ xác suất (PDF) của phân phối Lognormal \\(\\left(\\mu, \\sigma^{2}\\right)\\) :\n\\[\nf_{x}(x)=\\frac{d}{d x} \\operatorname{Pr}(X \\leq x)=\\frac{1}{x \\sigma \\sqrt{2 \\pi}} e^{\\left(-\\frac{(\\ln x-\\mu)^{2}}{2 \\sigma^{2}}\\right)}, \\text { where } x&gt;0, \\sigma&gt;0\n\\]\n\n\\(x\\) là biến ngẫu nhiên\nMột biến ngẫu nhiên \\(X\\) tuân theo phân phối Log-normal \\(\\left(X \\sim \\operatorname{Lognormal}\\left(\\mu_{x}, \\sigma_{x}^{2}\\right)\\right)\\) nếu \\(\\ln (X)\\) tuân theo phân phối chuẩn với giá trị trung bình là \\(\\mu\\) và phương sai \\(\\sigma^{2}\\)\n\nĐể lựa chọn phân phối phù hợp cho từng phân nhóm, Nhóm mô hình lựa chọn mô hình có SSE (Sum of square error - tổng của sai số bình phương) nhỏ nhất với SSE được tính theo công thức sau:\n\\[\n\\left.S S E=\\sum \\text { (Giá trị thực tế - Giá trị được tính ra từ mô hình phân phối }\\right)^{2}\n\\]\nCuối cùng, từ mô hình phân phối vừa được lựa chọn (là mô hình có SSE nhỏ nhất).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import cauchy, gamma, expon, lognorm, loglaplace\n\n# Define the x range for the plot\nx = np.linspace(0, 15, 1000)\n\n# Create a single row with 5 subplots\nfig, axs = plt.subplots(1, 5, figsize=(10, 3))\n\n# Cauchy Distribution\ncauchy_pdf = cauchy.pdf(x)\naxs[0].plot(x, cauchy_pdf)\naxs[0].set_title(\"Cauchy\")\n\n# Gamma Distribution\ngamma_pdf = gamma.pdf(x, a=2)\naxs[1].plot(x, gamma_pdf)\naxs[1].set_title(\"Gamma\")\n\n# Exponential Distribution\nexponential_pdf = expon.pdf(x)\naxs[2].plot(x, exponential_pdf)\naxs[2].set_title(\"Exponential\")\n\n# Log Normal Distribution\nlog_normal_pdf = lognorm.pdf(x, s=0.7)\naxs[3].plot(x, log_normal_pdf)\naxs[3].set_title(\"Log Normal\")\n\n# Log Logistic Distribution\nlog_logistic_pdf = loglaplace.pdf(x, c=0.5)\naxs[4].plot(x, log_logistic_pdf)\naxs[4].set_title(\"Log Logistic\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Fit Distribution"
    ]
  },
  {
    "objectID": "machine-learning/CohortFitDistribution.html#python-example",
    "href": "machine-learning/CohortFitDistribution.html#python-example",
    "title": "Fit Distribution",
    "section": "Python Example",
    "text": "Python Example\n\nimport numpy as np\nimport pandas as pd\nimport scipy  \nfrom scipy import stats \nimport scipy.optimize as optimize\n\n\n1. Các hàm liên quan\n\n1.1 Hàm phân phối gamma\n\nclass opt_gamma:\n\n    def __init__(self, actual_pd):\n        self.actual_pd = actual_pd\n        self.x_input = range(1, len(actual_pd) + 1)\n\n    def func(self, x, scale, a, b):        \n        predict = stats.gamma.pdf(x, a = a, scale = b) * scale\n        return predict\n\n    def sse(self, params, xobs, yobs):\n        ynew = self.func(xobs, *params)\n        mse = np.sum((ynew - yobs) ** 2)        \n        return mse\n    \n    def solver(self):\n        p0 = [1,1,1]\n        bounds = [(0.0001, 2), (0.0001, 10), (0.0001, 10)]\n        res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), bounds= bounds)       \n        # res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), method='Nelder-Mead')        \n        return res\n    \n    def predict(self, t=30):\n        res = self.solver()\n        ypred = self.func(range(1, t+1), *res.x)        \n        return ypred        \n\n\n\n1.2 Hàm Phân phối mũ Exponential\n\nclass opt_exponential:\n\n    def __init__(self, actual_pd):\n        self.actual_pd = actual_pd\n        self.x_input = range(1, len(actual_pd) + 1)\n\n    def func(self, x, scale, a):        \n        # change function here        \n        pred = stats.expon.pdf(x, scale = 1/a) * scale\n        return pred\n\n    def sse(self, params, xobs, yobs):\n        ynew = self.func(xobs, *params)\n        mse = np.sum((ynew - yobs) ** 2)        \n        return mse\n    \n    def solver(self):\n        # change initial guess here\n        p0 = [1,2]\n        bounds = [(0.0001, 2), (0.0001, 0.5)]           \n        res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), bounds = bounds)             \n        return res\n    \n    def predict(self, t=30):\n        res = self.solver()\n        ypred = self.func(range(1, t+1), *res.x)        \n        return ypred       \n\n\n\n1.3 Hàm phân phối Cauchy\n\nclass opt_cauchy:\n\n    def __init__(self, actual_pd):\n        self.actual_pd = actual_pd\n        self.x_input = range(1, len(actual_pd) + 1)\n\n    def func(self, x, scale, a, b):        \n        # change function here        \n        predict = stats.cauchy.pdf(x, loc = b, scale = a) * scale\n        return predict\n\n    def sse(self, params, xobs, yobs):\n        ynew = self.func(xobs, *params)\n        mse = np.sum((ynew - yobs) ** 2)        \n        return mse\n    \n    def solver(self):\n        # change initial guess here\n        p0 = [1,1,1]\n        bounds = [(0.0001, 2), (0.0001, 15), (-30, 30)]\n        res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), bounds = bounds)        \n        return res\n    \n    def predict(self, t=30):\n        res = self.solver()\n        ypred = self.func(range(1, t+1), *res.x)        \n        return ypred    \n\n\n\n1.4 Hàm phân phối log logistic\n\nclass opt_log_logistic:\n\n    def __init__(self, actual_pd):\n        self.actual_pd = actual_pd\n        self.x_input = range(1, len(actual_pd) + 1)\n\n    def func(self, x, scale, a, b):        \n        # change function here\n        predict = scale*(a/b)*(x/b)**(a-1)/(1+(x/b)**a)**2\n        return predict\n\n    def sse(self, params, xobs, yobs):\n        ynew = self.func(xobs, *params)\n        mse = np.sum((ynew - yobs) ** 2)        \n        return mse\n    \n    def solver(self):\n        # change initial guess here\n        p0 = [2,3,1]\n        bounds = [(0.0001, 2), (0.0001, 10), (0.0001, 10)]\n        res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), bounds=bounds)        \n        return res\n    \n    def predict(self, t=30):\n        res = self.solver()\n        ypred = self.func(range(1, t+1), *res.x)        \n        return ypred    \n\n\n\n1.5 Hàm phân phối log normal\n\nclass opt_log_normal:\n\n    def __init__(self, actual_pd):\n        self.actual_pd = actual_pd\n        self.x_input = range(1, len(actual_pd) + 1)\n\n    def func(self, x, scale, a, b):        \n        # change function here\n        predict = (np.exp(-(np.log(x) - a)**2 / (2 * b**2)) / (x * b * np.sqrt(2 * np.pi)))*scale\n        return predict\n\n    def sse(self, params, xobs, yobs):\n        ynew = self.func(xobs, *params)\n        mse = np.sum((ynew - yobs) ** 2)        \n        return mse\n    \n    def solver(self):\n        # change initial guess here\n        p0 = [1,1,1]\n        bounds = [(0.0001, 2), (0.0001, 0.5), (0.0001, 0.5)]\n        res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), bounds=bounds)        \n        return res\n    \n    def predict(self, t=30):\n        res = self.solver()\n        ypred = self.func(range(1, t+1), *res.x)        \n        return ypred",
    "crumbs": [
      "Machine learning",
      "Fit Distribution"
    ]
  },
  {
    "objectID": "machine-learning/CohortFitDistribution.html#fit-models",
    "href": "machine-learning/CohortFitDistribution.html#fit-models",
    "title": "Fit Distribution",
    "section": "2. Fit models",
    "text": "2. Fit models\n\n2.1 Đặt giá trị initial\n\nGamma = 1,1,1\nCauchy = 1,1,1\nExpo = 1,2\nLog Logistic = 2,3,1\nLog Normal = 1,1,1\n\n\n\n2.2 Đọc dữ liệu\n\ndata = pd.read_excel('results/Cohort Analysis.xlsb', engine='pyxlsb', sheet_name='1.2 Visualize', usecols = 'O:Z')\ndata.tail()",
    "crumbs": [
      "Machine learning",
      "Fit Distribution"
    ]
  },
  {
    "objectID": "machine-learning/CohortFitDistribution.html#fit-models-1",
    "href": "machine-learning/CohortFitDistribution.html#fit-models-1",
    "title": "Fit Distribution",
    "section": "2.3 Fit models",
    "text": "2.3 Fit models\n\ndef fn_export_by_segment(segment, period):\n    # segment = segment.upper()\n    actual_pd = data[data.Segment_Group.isin([segment])]\n    actual_pd = actual_pd.iloc[0, 2:]\n    actual_pd = actual_pd.values\n    \n    res_gamma = opt_gamma(actual_pd)\n    res_exponential = opt_exponential(actual_pd)\n    res_cauchy = opt_cauchy(actual_pd)\n    res_log_logistic = opt_log_logistic(actual_pd)\n    res_log_normal = opt_log_normal(actual_pd)\n    \n    params = pd.DataFrame({\n        'Segment': segment,\n        'Distribution': ['Gamma', 'Exponential', 'Cauchy', 'Log-Logistic', 'Log-Normal'],\n        'SSE': [res_gamma.solver().fun, res_exponential.solver().fun, res_cauchy.solver().fun, res_log_logistic.solver().fun, res_log_normal.solver().fun],\n        'params': [res_gamma.solver().x, res_exponential.solver().x, res_cauchy.solver().x, res_log_logistic.solver().x, res_log_normal.solver().x]\n    })    \n   \n    params[['scale', 'a', 'b']] = pd.DataFrame(params.params.to_list())    \n    del params['params']\n    \n    dict_predict = {\n            'Segment': segment,\n            'Period': range(1, period+1),\n            'Gamma': res_gamma.predict(period),\n            'Exponential': res_exponential.predict(period),\n            'Cauchy': res_cauchy.predict(period),\n            'Log-Logistic': res_log_logistic.predict(period),\n            'Log-Normal': res_log_normal.predict(period)\n        }\n    \n    best_distribution = params.sort_values('SSE').head(1)\n    best_distribution['Tag best distribution'] = 'Best distribution'    \n\n    print('Best distribution of ' + segment + ' is '+ best_distribution['Distribution'].item())\n\n    return params, best_distribution, pd.DataFrame(dict_predict)\n\n\npred_best_fit = []\nparams_frame = []\npred_all_curve = []\nfor seg in data.Segment_Group:\n    params, df_best_fit, df_all = fn_export_by_segment(seg, 30)\n    pred_best_fit.append(df_best_fit)\n    params_frame.append(params)\n    pred_all_curve.append(df_all)\n\npred_best_fit = pd.concat(pred_best_fit, axis=0)\nparams_frame = pd.concat(params_frame, axis=0)\npred_all_curve = pd.concat(pred_all_curve, axis=0)\n\n\npivot_pred_all_curve = pd.melt(pred_all_curve, \n        id_vars=['Segment', 'Period'], \n        value_vars=['Gamma', 'Exponential', 'Cauchy', 'Log-Logistic', 'Log-Normal'],\n        var_name = 'Distribution').pivot(\n                index = ['Segment', 'Distribution'], columns='Period'\n        )\npivot_pred_all_curve.reset_index(inplace=True, drop=False)\npivot_pred_all_curve.columns = ['Segment', 'Distribution', *range(1,31)]\npivot_pred_all_curve = params_frame.merge(pivot_pred_all_curve, how='inner', on=['Segment', 'Distribution']).merge(\n        pred_best_fit[['Segment', 'Distribution', 'Tag best distribution']], how='left', on=['Segment', 'Distribution']\n)",
    "crumbs": [
      "Machine learning",
      "Fit Distribution"
    ]
  },
  {
    "objectID": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html",
    "href": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html",
    "title": "Overestimate feature importance in Random Forest",
    "section": "",
    "text": "Tree-based models have a strong tendency to overestimate the importance of continuous numerical or high cardinality categorical features\nCác mô hình dựa trên cây như decision trees, random forests, and gradient boosting machines (GBMs) có thể thể hiện sự thiên vị trong việc đánh giá quá cao tầm quan trọng của các các biến continuous numerical hoặc high cardinality categorical features trong một số trường hợp nhất định. Hiện tượng này thường được gọi là “variable bias” hay “variable inflation”. Đây là lý do tại sao nó xảy ra:\nĐể giải quyết sự thiên vị hoặc đánh giá quá cao tầm quan trọng của tính năng, bạn có thể xem xét những điều sau:\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\n# Load the German Credit dataset (assuming it's available in a CSV file)\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = [\"existing_account\", \"duration_month\", \"credit_history\", \"purpose\", \"credit_amount\",\n                \"savings_account\", \"employment_since\", \"installment_rate\", \"personal_status_sex\", \"other_debtors\",\n                \"present_residence\", \"property\", \"age\", \"other_installment_plans\", \"housing\", \"existing_credits\",\n                \"job\", \"people_liable\", \"telephone\", \"foreign_worker\", \"credit_risk\"]\ndf_raw = pd.read_csv(url, sep=\" \", header=None, names=column_names)\ndf_raw.describe()\n\n\n\n\n\n\n\n\nduration_month\ncredit_amount\ninstallment_rate\npresent_residence\nage\nexisting_credits\npeople_liable\ncredit_risk\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n20.903000\n3271.258000\n2.973000\n2.845000\n35.546000\n1.407000\n1.155000\n1.300000\n\n\nstd\n12.058814\n2822.736876\n1.118715\n1.103718\n11.375469\n0.577654\n0.362086\n0.458487\n\n\nmin\n4.000000\n250.000000\n1.000000\n1.000000\n19.000000\n1.000000\n1.000000\n1.000000\n\n\n25%\n12.000000\n1365.500000\n2.000000\n2.000000\n27.000000\n1.000000\n1.000000\n1.000000\n\n\n50%\n18.000000\n2319.500000\n3.000000\n3.000000\n33.000000\n1.000000\n1.000000\n1.000000\n\n\n75%\n24.000000\n3972.250000\n4.000000\n4.000000\n42.000000\n2.000000\n1.000000\n2.000000\n\n\nmax\n72.000000\n18424.000000\n4.000000\n4.000000\n75.000000\n4.000000\n2.000000\n2.000000\ndf_raw.infer_objects()\n\n\n\n\n\n\n\n\nexisting_account\nduration_month\ncredit_history\npurpose\ncredit_amount\nsavings_account\nemployment_since\ninstallment_rate\npersonal_status_sex\nother_debtors\n...\nproperty\nage\nother_installment_plans\nhousing\nexisting_credits\njob\npeople_liable\ntelephone\nforeign_worker\ncredit_risk\n\n\n\n\n0\nA11\n6\nA34\nA43\n1169\nA65\nA75\n4\nA93\nA101\n...\nA121\n67\nA143\nA152\n2\nA173\n1\nA192\nA201\n1\n\n\n1\nA12\n48\nA32\nA43\n5951\nA61\nA73\n2\nA92\nA101\n...\nA121\n22\nA143\nA152\n1\nA173\n1\nA191\nA201\n2\n\n\n2\nA14\n12\nA34\nA46\n2096\nA61\nA74\n2\nA93\nA101\n...\nA121\n49\nA143\nA152\n1\nA172\n2\nA191\nA201\n1\n\n\n3\nA11\n42\nA32\nA42\n7882\nA61\nA74\n2\nA93\nA103\n...\nA122\n45\nA143\nA153\n1\nA173\n2\nA191\nA201\n1\n\n\n4\nA11\n24\nA33\nA40\n4870\nA61\nA73\n3\nA93\nA101\n...\nA124\n53\nA143\nA153\n2\nA173\n2\nA191\nA201\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\nA14\n12\nA32\nA42\n1736\nA61\nA74\n3\nA92\nA101\n...\nA121\n31\nA143\nA152\n1\nA172\n1\nA191\nA201\n1\n\n\n996\nA11\n30\nA32\nA41\n3857\nA61\nA73\n4\nA91\nA101\n...\nA122\n40\nA143\nA152\n1\nA174\n1\nA192\nA201\n1\n\n\n997\nA14\n12\nA32\nA43\n804\nA61\nA75\n4\nA93\nA101\n...\nA123\n38\nA143\nA152\n1\nA173\n1\nA191\nA201\n1\n\n\n998\nA11\n45\nA32\nA43\n1845\nA61\nA73\n4\nA93\nA101\n...\nA124\n23\nA143\nA153\n1\nA173\n1\nA192\nA201\n2\n\n\n999\nA12\n45\nA34\nA41\n4576\nA62\nA71\n3\nA93\nA101\n...\nA123\n27\nA143\nA152\n1\nA173\n1\nA191\nA201\n1\n\n\n\n\n1000 rows × 21 columns",
    "crumbs": [
      "Machine learning",
      "Overestimate feature importance in Random Forest"
    ]
  },
  {
    "objectID": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#lựa-chọn-dữ-liệu-tính-toán",
    "href": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#lựa-chọn-dữ-liệu-tính-toán",
    "title": "Overestimate feature importance in Random Forest",
    "section": "Lựa chọn dữ liệu tính toán",
    "text": "Lựa chọn dữ liệu tính toán\nChọn các biến có các đặc điểm khác nhau như: biến dạng double, int, char\n\ndf = df_raw[['credit_risk','credit_amount', 'age', 'duration_month', 'credit_history', 'other_installment_plans']]\n\n# Encode categorical variables (e.g., using one-hot encoding)\ndf = pd.get_dummies(df, columns=['credit_history', 'other_installment_plans'])\n\n# Define the target variable and features\nX = df.drop(columns=[\"credit_risk\"])\ny = df[\"credit_risk\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Random Forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)",
    "crumbs": [
      "Machine learning",
      "Overestimate feature importance in Random Forest"
    ]
  },
  {
    "objectID": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#tính-feature-importances-với-các-biến-đã-lựa-chọn-ban-đầu",
    "href": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#tính-feature-importances-với-các-biến-đã-lựa-chọn-ban-đầu",
    "title": "Overestimate feature importance in Random Forest",
    "section": "Tính feature importances với các biến đã lựa chọn ban đầu",
    "text": "Tính feature importances với các biến đã lựa chọn ban đầu\n\n# Compute feature importances using MDI\nmdi_feature_importances = clf.feature_importances_\n\n# Compute feature importances using permutation method\nperm_importance = permutation_importance(clf, X_test, y_test, n_repeats=30, random_state=42)\nperm_feature_importances = perm_importance.importances_mean\n\n# Create DataFrames to display feature importances\nmdi_importances_df = pd.DataFrame({\"Feature\": X.columns, \"MDI Importance\": mdi_feature_importances})\nmdi_importances_df = mdi_importances_df.sort_values(by=\"MDI Importance\", ascending=False)\n\nperm_importances_df = pd.DataFrame({\"Feature\": X.columns, \"Permutation Importance\": perm_feature_importances})\nperm_importances_df = perm_importances_df.sort_values(by=\"Permutation Importance\", ascending=False)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Create DataFrames to display feature importances from both methods...\n# (As shown in the previous code)\n\n# Create subplots for visualization\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Visualize MDI feature importances\nsns.barplot(x=\"MDI Importance\", y=\"Feature\", data=mdi_importances_df, ax=axes[0], color=\"lightblue\")\naxes[0].set_title(\"MDI Feature Importance\")\n\n# Visualize permutation feature importances\nsns.barplot(x=\"Permutation Importance\", y=\"Feature\", data=perm_importances_df, ax=axes[1], color=\"lightgreen\")\naxes[1].set_title(\"Permutation Feature Importance\")\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Overestimate feature importance in Random Forest"
    ]
  },
  {
    "objectID": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#thêm-biến-random-để-đánh-giá",
    "href": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#thêm-biến-random-để-đánh-giá",
    "title": "Overestimate feature importance in Random Forest",
    "section": "Thêm biến random để đánh giá",
    "text": "Thêm biến random để đánh giá\n\nThêm 2 loại biến: 1 loại dạng CONTINUOUS variable, 1 loại DISCRETE variable để đánh giá xem biến nào importance cao hơn và so với các biến khác trong dữ liệu thì như thế nào\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\n# Load and preprocess the German Credit dataset as before...\n\n# Add two new random columns (Continuous_Random and Discrete_Random)\nimport numpy as np\nnp.random.seed(42)  # For reproducibility\n\n# Create random continuous values\ndf['CONTINUOUS_RANDOM'] = np.random.uniform(0,200, size=len(df))\n\n# Create random discrete values\ndf['DISCRETE_RANDOM'] = np.random.randint(0, 5, size=len(df))\n\n# Create random discrete values\ndf['DISCRETE_RANDOM2'] = np.random.randint(0, 10, size=len(df))\n\n# Define the target variable and features\nX = df.drop(columns=[\"credit_risk\"])\ny = df[\"credit_risk\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Random Forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Compute feature importances using MDI\nmdi_feature_importances = clf.feature_importances_\n\n# Compute feature importances using permutation method\nperm_importance = permutation_importance(clf, X_test, y_test, n_repeats=30, random_state=42)\nperm_feature_importances = perm_importance.importances_mean\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef pImportanceWithHighlight(categories, values, highlight_categories, highlight_color='red', base_color='lightblue', title='Feature Importances'):          \n    data = {'Category': categories,\n            'Values': values}\n    df = pd.DataFrame(data)   \n    \n    # Sort the DataFrame in increasing order of the 'Values' column\n    df_sorted = df.sort_values(by='Values', ascending=False)\n    df_sorted.reset_index(inplace=True, drop=True)    \n    \n     # Determine the positions (indices) of the highlighted categories\n    highlight_indices = [idx for idx, cat in enumerate(df_sorted['Category']) if cat in highlight_categories]\n    \n    # Create a list of colors for each row, setting the highlight color for the specific rows\n    colors = [base_color if idx not in highlight_indices else highlight_color for idx in range(len(df_sorted))]\n\n    # Create a bar plot with the specified colors using Seaborn\n    sns.barplot(x='Values', y='Category', data=df_sorted, palette=colors)\n\n    # Add labels and a title\n    plt.xlabel('Feature')\n    plt.ylabel('Importance')\n    plt.title(title)\n\n# Create subplots for visualization\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Visualize MDI feature importances\nplt.sca(axes[0])  # Set the current axes to the first subplot\npImportanceWithHighlight(X.columns, mdi_feature_importances, ['CONTINUOUS_RANDOM', 'DISCRETE_RANDOM', 'DISCRETE_RANDOM2'], title=\"MDI Feature Importance\")\n\n# Visualize permutation feature importances\nplt.sca(axes[1])  # Set the current axes to the second subplot\npImportanceWithHighlight(X.columns, perm_feature_importances, ['CONTINUOUS_RANDOM', 'DISCRETE_RANDOM', 'DISCRETE_RANDOM2'], title=\"Permutation Feature Importance\", base_color='lightgreen')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Overestimate feature importance in Random Forest"
    ]
  },
  {
    "objectID": "statistic/StandardizedCoefficients.html",
    "href": "statistic/StandardizedCoefficients.html",
    "title": "Standardized Coefficients vs Unstandardized Coefficients",
    "section": "",
    "text": "Hệ số hồi quy có thể được chuẩn hóa hoặc không chuẩn hóa, và chúng có mục đích khác nhau trong phân tích thống kê. Dưới đây là sự khác biệt giữa hai loại hệ số này:\n\nHệ số không chuẩn hóa:\n\nHệ số không chuẩn hóa (Unstandardized Coefficients), còn được gọi là hệ số góc hoặc hệ số beta, biểu thị sự thay đổi trong biến phụ thuộc (Y) khi biến độc lập (X) thay đổi một đơn vị, trong khi giữ tất cả các biến khác cố định.\nChúng được biểu thị trong đơn vị đo lường gốc của các biến. Ví dụ, nếu bạn đang dự đoán mức lương (bằng đô la) dựa trên số năm kinh nghiệm, hệ số không chuẩn hóa có thể là, ví dụ, 5.000 đô la, có nghĩa là với mỗi năm kinh nghiệm bổ sung, mức lương dự kiến sẽ tăng 5.000 đô la.\nHệ số không chuẩn hóa hữu ích khi bạn muốn hiểu về tác động thực tế của biến độc lập lên biến phụ thuộc.\n\nHệ số chuẩn hóa:\n\nHệ số chuẩn hóa (Standardized Coefficients), còn được gọi là trọng số beta hoặc hệ số beta, là các hệ số đã được biến đổi sao cho có giá trị trung bình bằng 0 và độ lệch chuẩn bằng 1. Quá trình chuẩn hóa này cho phép so sánh trực tiếp sự quan trọng tương đối của các biến độc lập khác nhau trong một mô hình hồi quy.\nChúng đo lường về sự thay đổi của biến phụ thuộc theo bao nhiêu độ lệch chuẩn khi biến độc lập thay đổi một độ lệch chuẩn.\nHệ số chuẩn hóa đặc biệt hữu ích khi bạn muốn so sánh sự quan trọng tương đối của các biến dự đoán khác nhau trong một mô hình, đặc biệt là khi các biến được đo trên các thang đo khác nhau. Chúng giúp trả lời câu hỏi như “Biến dự đoán nào có tác động mạnh hơn đối với kết quả, bất kể đơn vị đo lường?”\n\n\nTóm lại, hệ số không chuẩn hóa hữu ích để hiểu về tác động thực tế của các biến độc lập lên biến phụ thuộc trong đơn vị gốc của chúng, trong khi hệ số chuẩn hóa hữu ích để so sánh sự quan trọng tương đối của các biến dự đoán khác nhau khi các biến đo trên các thang đo khác nhau. Sự lựa chọn giữa hai loại hệ số này phụ thuộc vào câu hỏi nghiên cứu cụ thể và mục tiêu của phân tích của bạn.",
    "crumbs": [
      "Statistic",
      "Standardized Coefficients vs Unstandardized Coefficients"
    ]
  },
  {
    "objectID": "statistic/StandardizedCoefficients.html#so-sánh",
    "href": "statistic/StandardizedCoefficients.html#so-sánh",
    "title": "Standardized Coefficients vs Unstandardized Coefficients",
    "section": "",
    "text": "Hệ số hồi quy có thể được chuẩn hóa hoặc không chuẩn hóa, và chúng có mục đích khác nhau trong phân tích thống kê. Dưới đây là sự khác biệt giữa hai loại hệ số này:\n\nHệ số không chuẩn hóa:\n\nHệ số không chuẩn hóa (Unstandardized Coefficients), còn được gọi là hệ số góc hoặc hệ số beta, biểu thị sự thay đổi trong biến phụ thuộc (Y) khi biến độc lập (X) thay đổi một đơn vị, trong khi giữ tất cả các biến khác cố định.\nChúng được biểu thị trong đơn vị đo lường gốc của các biến. Ví dụ, nếu bạn đang dự đoán mức lương (bằng đô la) dựa trên số năm kinh nghiệm, hệ số không chuẩn hóa có thể là, ví dụ, 5.000 đô la, có nghĩa là với mỗi năm kinh nghiệm bổ sung, mức lương dự kiến sẽ tăng 5.000 đô la.\nHệ số không chuẩn hóa hữu ích khi bạn muốn hiểu về tác động thực tế của biến độc lập lên biến phụ thuộc.\n\nHệ số chuẩn hóa:\n\nHệ số chuẩn hóa (Standardized Coefficients), còn được gọi là trọng số beta hoặc hệ số beta, là các hệ số đã được biến đổi sao cho có giá trị trung bình bằng 0 và độ lệch chuẩn bằng 1. Quá trình chuẩn hóa này cho phép so sánh trực tiếp sự quan trọng tương đối của các biến độc lập khác nhau trong một mô hình hồi quy.\nChúng đo lường về sự thay đổi của biến phụ thuộc theo bao nhiêu độ lệch chuẩn khi biến độc lập thay đổi một độ lệch chuẩn.\nHệ số chuẩn hóa đặc biệt hữu ích khi bạn muốn so sánh sự quan trọng tương đối của các biến dự đoán khác nhau trong một mô hình, đặc biệt là khi các biến được đo trên các thang đo khác nhau. Chúng giúp trả lời câu hỏi như “Biến dự đoán nào có tác động mạnh hơn đối với kết quả, bất kể đơn vị đo lường?”\n\n\nTóm lại, hệ số không chuẩn hóa hữu ích để hiểu về tác động thực tế của các biến độc lập lên biến phụ thuộc trong đơn vị gốc của chúng, trong khi hệ số chuẩn hóa hữu ích để so sánh sự quan trọng tương đối của các biến dự đoán khác nhau khi các biến đo trên các thang đo khác nhau. Sự lựa chọn giữa hai loại hệ số này phụ thuộc vào câu hỏi nghiên cứu cụ thể và mục tiêu của phân tích của bạn.",
    "crumbs": [
      "Statistic",
      "Standardized Coefficients vs Unstandardized Coefficients"
    ]
  },
  {
    "objectID": "statistic/StandardizedCoefficients.html#công-thức-tính-standardized-coefficients",
    "href": "statistic/StandardizedCoefficients.html#công-thức-tính-standardized-coefficients",
    "title": "Standardized Coefficients vs Unstandardized Coefficients",
    "section": "Công thức tính Standardized Coefficients",
    "text": "Công thức tính Standardized Coefficients\n\nTrong trường hợp hồi quy tuyến tính, công thức để tính hệ số beta chuẩn hóa ($ _{, i} $) cho biến độc lập $ X_i $ là:\n\\[ \\beta_{\\text{std}, i} = \\frac{\\beta_i}{\\sigma_i} \\]\n\n$ _{, i} $: Hệ số beta chuẩn hóa cho biến độc lập $ X_i $.\n$ _i $: Hệ số beta không chuẩn hóa cho biến độc lập $ X_i $.\n$ _i $: Độ lệch chuẩn (standard deviation) của biến độc lập $ X_i $.\n\nTrong trường hợp hồi quy hồi quy logistic, công thức để tính hệ số beta chuẩn hóa ($ _{, i} $) cho biến độc lập $ X_i $ là:\n\\[\\beta_{\\text{std}, i} = \\frac{\\beta_i}{\\sigma_i} \\times \\frac{\\sqrt{3}}{\\pi}\\]\nTrong đó:\n\n\\(\\beta_i\\) là hệ số (beta) của biến dự đoán \\(X_i\\) trong mô hình hồi quy logistic.\n\\(\\sigma_i\\) là độ lệch chuẩn của biến dự đoán \\(X_i\\) trong tập dữ liệu huấn luyện.\n\\(\\beta_{\\text{std}, i}\\) là hệ số beta chuẩn hóa cho biến dự đoán \\(X_i\\).",
    "crumbs": [
      "Statistic",
      "Standardized Coefficients vs Unstandardized Coefficients"
    ]
  },
  {
    "objectID": "statistic/Binomial_interval.html",
    "href": "statistic/Binomial_interval.html",
    "title": "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial",
    "section": "",
    "text": "Phân phối nhị thức (binomial distribution) là một dạng phân phối xác suất rời rạc, đặc trưng cho số lần thành công trong một số lần thử nghiệm độc lập và có xác suất thành công cố định trong mỗi lần thử nghiệm. Phân phối nhị thức thường được sử dụng trong các tình huống mà mỗi thử nghiệm chỉ có hai kết quả có thể xảy ra (thành công hoặc thất bại), và xác suất thành công là không đổi trong tất cả các lần thử nghiệm.\nHàm xác suất của phân phối nhị thức được mô tả bởi công thức:\n\\[P(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k}\\]\nỞ đây: - \\(X\\) là biến ngẫu nhiên biểu thị số lần thành công trong \\(n\\) lần thử nghiệm. - \\(k\\) là một giá trị cụ thể của \\(X\\) (số lần thành công). - \\(n\\) là số lần thử nghiệm độc lập. - \\(p\\) là xác suất thành công trong mỗi lần thử nghiệm. - \\(\\binom{n}{k}\\) là số cách chọn \\(k\\) thành công từ \\(n\\) lần thử nghiệm, được gọi là hệ số nhị thức.\nPhân phối nhị thức thường được ký hiệu là \\(B(n, p)\\), trong đó \\(n\\) là số lần thử nghiệm và \\(p\\) là xác suất thành công. Phân phối này có giá trị kỳ vọng \\(np\\) và phương sai \\(np(1-p)\\).\nPhân phối nhị thức được ứng dụng rộng rãi trong thống kê, xã hội học, kinh tế học, và nhiều lĩnh vực khác để mô tả sự biến động trong số lần thành công trong một số thử nghiệm độc lập.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import binom\n\n# Các tham số của phân phối nhị thức\nn_values = [10, 20, 30]  # Số lần thử nghiệm\np = 0.5  # Xác suất thành công\n\n# Tạo mảng giá trị X từ 0 đến n_max cho mỗi n\nn_max = max(n_values)\nx_values = np.arange(0, n_max + 1)\n\n# Vẽ hình minh họa phân phối nhị thức cho các giá trị n khác nhau\nplt.figure(figsize=(10, 6))\n\nfor n in n_values:\n    pmf = binom.pmf(x_values, n, p)\n    plt.plot(x_values, pmf, label=f'n={n}')\n\nplt.title('Phân phối nhị thức với n khác nhau')\nplt.xlabel('Số lần thành công (X)')\nplt.ylabel('Xác suất')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Statistic",
      "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial"
    ]
  },
  {
    "objectID": "statistic/Binomial_interval.html#clopper-pearson-method-1",
    "href": "statistic/Binomial_interval.html#clopper-pearson-method-1",
    "title": "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial",
    "section": "Clopper-Pearson Method:",
    "text": "Clopper-Pearson Method:\n\nƯu điểm:\n\nBảo đảm độ tin cậy: Clopper-Pearson Method đảm bảo rằng khoảng tin cậy xây dựng sẽ bao gồm tỷ lệ thành công thực sự với độ tin cậy đã chọn.\nDễ hiểu và tính toán: Phương pháp này dễ hiểu và tính toán, thích hợp cho các ứng dụng với mẫu nhỏ.\n\n\n\nNhược điểm:\n\nRộng hóa khoảng tin cậy: Cho mẫu nhỏ, khoảng tin cậy Clopper-Pearson có thể rộng hơn so với các phương pháp khác, đặc biệt là khi tỷ lệ thành công gần 0 hoặc 1.",
    "crumbs": [
      "Statistic",
      "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial"
    ]
  },
  {
    "objectID": "statistic/Binomial_interval.html#jeffreys-method-1",
    "href": "statistic/Binomial_interval.html#jeffreys-method-1",
    "title": "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial",
    "section": "Jeffreys Method:",
    "text": "Jeffreys Method:\n\nƯu điểm:\nPhù hợp với mẫu lớn: Thích hợp cho các mẫu lớn, có thể tạo ra khoảng tin cậy ngắn hơn so với Clopper-Pearson.\n\n\nNhược điểm:\nTính phức tạp: Yêu cầu tính toán phức tạp hơn so với Clopper-Pearson, đặc biệt là khi sử dụng các phương pháp số để tính toán các giá trị của phân phối Beta.",
    "crumbs": [
      "Statistic",
      "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial"
    ]
  },
  {
    "objectID": "statistic/Binomial_interval.html#likelihood-ratio-method-1",
    "href": "statistic/Binomial_interval.html#likelihood-ratio-method-1",
    "title": "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial",
    "section": "Likelihood Ratio Method:",
    "text": "Likelihood Ratio Method:\n\nƯu điểm:\n\nPhù hợp với mẫu lớn: Tích hợp tốt với các mẫu lớn, có thể tạo ra khoảng tin cậy ngắn hơn.\nTính chất lý thuyết tỷ số kiểm định: Phương pháp này có liên quan chặt chẽ đến lý thuyết tỷ số kiểm định cơ bản, có ứng dụng rộng trong thống kê.\n\n\n\nNhược điểm:\n\nKhông đảm bảo chính xác độ tin cậy: Không đảm bảo chính xác độ tin cậy với mức ý nghĩa đã chọn, đặc biệt là với mẫu nhỏ.\nYêu cầu giả định: Đòi hỏi giả định về phân phối của ước lượng trong các trường hợp cụ thể.\n\n\nimport numpy as np\nfrom scipy.stats import beta\nimport matplotlib.pyplot as plt\n\ndef calculate_confidence_intervals(X, n, alpha=0.05, method='clopper-pearson'):\n    if method == 'clopper-pearson':\n        lower = beta.ppf(alpha/2, X, n - X + 1)\n        upper = beta.ppf(1 - alpha/2, X + 1, n - X)\n    elif method == 'jeffreys':\n        lower = beta.ppf(alpha/2, X + 0.5, n - X + 0.5)\n        upper = beta.ppf(1 - alpha/2, X + 0.5, n - X + 0.5)\n    elif method == 'likelihood-ratio':\n        p_hat = X/n\n        se_lr = np.sqrt(p_hat * (1 - p_hat) / n)\n        z_lr = 1.96  # Giá trị z tương ứng với mức ý nghĩa 0.025\n        lower = p_hat - z_lr * se_lr\n        upper = p_hat + z_lr * se_lr\n    else:\n        raise ValueError(\"Invalid method. Choose 'clopper-pearson', 'jeffreys', or 'likelihood-ratio'.\")\n    \n    return lower, upper\n\n# Dữ liệu mẫu\nX = 5\nn = 20\n\n# Tính khoảng tin cậy Clopper-Pearson\nlower_cp, upper_cp = calculate_confidence_intervals(X, n, method='clopper-pearson')\n\n# Tính khoảng tin cậy Jeffreys\nlower_j, upper_j = calculate_confidence_intervals(X, n, method='jeffreys')\n\n# Tính khoảng tin cậy Likelihood Ratio\nlower_lr, upper_lr = calculate_confidence_intervals(X, n, method='likelihood-ratio')\n\n# Hiển thị kết quả\nprint(f\"Clopper-Pearson CI: [{lower_cp}, {upper_cp}]\")\nprint(f\"Jeffreys CI: [{lower_j}, {upper_j}]\")\nprint(f\"Likelihood Ratio CI: [{lower_lr}, {upper_lr}]\")\n\n# Dữ liệu mẫu lớn hơn\nX_large = 500\nn_large = 10000\n\n# Tính khoảng tin cậy Clopper-Pearson\nlower_cp_large, upper_cp_large = calculate_confidence_intervals(X_large, n_large, method='clopper-pearson')\n\n# Tính khoảng tin cậy Jeffreys\nlower_j_large, upper_j_large = calculate_confidence_intervals(X_large, n_large, method='jeffreys')\n\n# Tính khoảng tin cậy Likelihood Ratio\nlower_lr_large, upper_lr_large = calculate_confidence_intervals(X_large, n_large, method='likelihood-ratio')\n\n# Hiển thị kết quả\nprint(f\"Clopper-Pearson CI (Large Sample): [{lower_cp_large}, {upper_cp_large}]\")\nprint(f\"Jeffreys CI (Large Sample): [{lower_j_large}, {upper_j_large}]\")\nprint(f\"Likelihood Ratio CI (Large Sample): [{lower_lr_large}, {upper_lr_large}]\")\n\nClopper-Pearson CI: [0.08657146910143461, 0.49104587170795744]\nJeffreys CI: [0.1023984856830451, 0.46419462924086813]\nLikelihood Ratio CI: [0.06022381603583657, 0.4397761839641634]\nClopper-Pearson CI (Large Sample): [0.0458099421987706, 0.05445452702835886]\nJeffreys CI (Large Sample): [0.04585790480962443, 0.054402519732842006]\nLikelihood Ratio CI (Large Sample): [0.045728279035330145, 0.05427172096466986]\n\n\nTrong trường hợp mẫu lớn thì kết quả ước lượng gần bằng nhau, nhưng trong trường hợp mẫu nhỏ thì phương pháp Clopper-Pearson thận trọng hơn\n\nlower_cp_large, upper_cp_large = calculate_confidence_intervals(243, 682, method='clopper-pearson')\nprint(lower_cp_large, upper_cp_large)\n\nlower_cp_large, upper_cp_large = calculate_confidence_intervals(243, 682, method='jeffreys')\nprint(lower_cp_large, upper_cp_large)\n\nlower_cp_large, upper_cp_large = calculate_confidence_intervals(243, 682, method='likelihood-ratio')\nprint(lower_cp_large, upper_cp_large)\n\n0.32031802030853457 0.3935415694585724\n0.32103156127821925 0.3927951336634465\n0.3203619370534926 0.39224803362099425\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\ndef calculate_intervals(X, n, alpha=0.05):\n    # Clopper-Pearson\n    cp_lower = beta.ppf(alpha/2, X, n - X + 1)\n    cp_upper = beta.ppf(1 - alpha/2, X + 1, n - X)\n\n    # Jeffreys\n    jeffreys_lower = beta.ppf(alpha/2, X + 0.5, n - X + 0.5)\n    jeffreys_upper = beta.ppf(1 - alpha/2, X + 0.5, n - X + 0.5)\n\n    # Likelihood Ratio\n    p_hat = X/n\n    se_lr = np.sqrt(p_hat * (1 - p_hat) / n)\n    z_lr = 1.96  # Giá trị z tương ứng với mức ý nghĩa 0.025\n    lr_lower = p_hat - z_lr * se_lr\n    lr_upper = p_hat + z_lr * se_lr\n\n    return cp_lower, cp_upper, jeffreys_lower, jeffreys_upper, lr_lower, lr_upper\n\ndef plot_distribution_and_intervals(X, n):\n    alpha = 0.05\n\n    # Calculate intervals\n    cp_lower, cp_upper, jeffreys_lower, jeffreys_upper, lr_lower, lr_upper = calculate_intervals(X, n, alpha)\n\n    # Vẽ đồ thị\n    x_values = np.linspace(0, 1, 1000)\n    pdf_values = beta.pdf(x_values, X, n - X + 1)\n\n    plt.plot(x_values, pdf_values, label=f'Beta({X}, {n - X + 1})', color='blue')\n    plt.axvline(cp_lower, color='red', linestyle='--', label='Clopper-Pearson (Lower)')\n    plt.axvline(jeffreys_lower, color='green', linestyle='--', label='Jeffreys (Lower)')\n    plt.axvline(cp_upper, color='red', linestyle='--', label='Clopper-Pearson (Upper)')\n    plt.axvline(jeffreys_upper, color='green', linestyle='--', label='Jeffreys (Upper)')\n\n    # Likelihood Ratio Interval\n    plt.axvline(lr_lower, color='purple', linestyle='--', label='Likelihood Ratio (Lower)')\n    plt.axvline(lr_upper, color='purple', linestyle='--', label='Likelihood Ratio (Upper)')\n\n    plt.title('Beta Distribution and Quantiles')\n    plt.xlabel('p')\n    plt.ylabel('Probability Density Function (PDF)')\n    plt.legend()\n    plt.show()\n\n# Example usage with small sample size\nplot_distribution_and_intervals(5, 20)\n\n# Example usage with large sample size\nplot_distribution_and_intervals(500, 2000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import binom\n\n# Example data\nsuccesses = 243\ntrials = 682\nconfidence_level = 0.95\n    \n# Calculate binomial confidence interval using the Clopper-Pearson method\nlower, upper = binom.interval(confidence_level, trials, successes/trials)\n\nprint(f\"Binomial Confidence Interval (Clopper-Pearson): [{lower}, {upper}]\")\n\nBinomial Confidence Interval (Clopper-Pearson): [219.0, 268.0]\n\n\n\n# Clopper–Pearson interval\nfrom scipy.stats import beta\nk = 243\nn = 682\nalpha = 0.05\np_u, p_o = beta.ppf([alpha/2, 1 - alpha/2], [k, k + 1], [n - k + 1, n - k])\nprint(f\"Binomial Confidence Interval (Clopper-Pearson): [{p_u}, {p_o}]\")\n\nBinomial Confidence Interval (Clopper-Pearson): [0.32031802030853457, 0.3935415694585724]",
    "crumbs": [
      "Statistic",
      "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial"
    ]
  },
  {
    "objectID": "statistic/MonteCarlo_Bootstrap.html",
    "href": "statistic/MonteCarlo_Bootstrap.html",
    "title": "So sánh giữa Monte Carlo Simulation và Bootstrap",
    "section": "",
    "text": "Phương pháp Monte Carlo Simulation xuất hiện đầu tiên vào cuối thập kỷ 1940, trong quá trình phát triển và nghiên cứu về vấn đề hạt nhân trong thời kỳ Chiến tranh thế giới thứ hai. Dự án Manhattan, một dự án nghiên cứu quy mô lớn do Mỹ thực hiện để phát triển bom nguyên tử, đặt ra nhiều thách thức về tính toán với những biến số không chắc chắn.\nJohn von Neumann, một nhà toán học và nhà vật lý người Mỹ-Hungari, cùng với nhóm nghiên cứu của ông, đã phát triển phương pháp Monte Carlo Simulation để giải quyết những vấn đề này. Tên gọi “Monte Carlo” xuất phát từ tên của thành phố Monaco nổi tiếng với sòng bạc và các trò chơi may rủi, thể hiện tính ngẫu nhiên trong quá trình mô phỏng.\nPhương pháp Monte Carlo Simulation được sử dụng rộng rãi trong các lĩnh vực như vật lý, tài chính, kỹ thuật, và nhiều ngành khác nữa. Đến nay, nó vẫn là một công cụ quan trọng trong mô hình hóa và dự đoán các biến số không chắc chắn, đặc biệt là khi không có phương trình toán học chính xác để mô tả các hệ thống.\n\n\n\nPhương pháp Bootstrap được giới thiệu bởi Bradley Efron vào cuối thập kỷ 1970, đặc biệt là trong bài báo mang tiêu đề “Bootstrap Methods: Another Look at the Jackknife” (1979). Trong bài báo này, Efron đề xuất phương pháp Bootstrap như một phương pháp thay thế và mở rộng cho phương pháp Jackknife, một phương pháp thống kê truyền thống.\nNgữ cảnh lịch sử: - Trước khi Bootstrap xuất hiện, các phương pháp thống kê thường phải dựa vào giả định về phân phối của dữ liệu hoặc kích thước mẫu lớn để áp dụng các ước lượng thống kê. - Efron nhận ra rằng, thay vì phụ thuộc vào giả định phân phối, chúng ta có thể sử dụng dữ liệu hiện có để đánh giá phân phối của một ước lượng thống kê thông qua việc tái chọn mẫu từ dữ liệu.\nCơ sở Lý Thuyết: - Bootstrap dựa trên ý tưởng rằng mẫu mà chúng ta có thể thu thập từ dữ liệu mẫu hiện có có thể đưa ra thông tin tốt về biến động của ước lượng thống kê. - Thay vì giả định về phân phối, Bootstrap sử dụng dữ liệu để xác định phân phối thông tin của ước lượng.\nỨng Dụng Đầu Tiên: - Phương pháp Bootstrap nhanh chóng được áp dụng rộng rãi trong thống kê và các lĩnh vực khác. Ban đầu, nó thường được sử dụng để đánh giá độ chắc chắn của các ước lượng thống kê như trung bình và phương sai. - Bootstrap cũng đã mở ra một hướng mới trong nghiên cứu thống kê, giúp làm thay đổi cách chúng ta xem xét và xử lý dữ liệu.\nPhương pháp Bootstrap đã trở thành một công cụ mạnh mẽ trong công việc thống kê và nghiên cứu khoa học, giúp làm giảm sự phụ thuộc vào giả định về phân phối và đồng thời cung cấp cách tiếp cận linh hoạt hơn trong đối mặt với dữ liệu thực tế.",
    "crumbs": [
      "Statistic",
      "So sánh giữa Monte Carlo Simulation và Bootstrap"
    ]
  },
  {
    "objectID": "statistic/MonteCarlo_Bootstrap.html#lịch-sử-ra-đời",
    "href": "statistic/MonteCarlo_Bootstrap.html#lịch-sử-ra-đời",
    "title": "So sánh giữa Monte Carlo Simulation và Bootstrap",
    "section": "",
    "text": "Phương pháp Monte Carlo Simulation xuất hiện đầu tiên vào cuối thập kỷ 1940, trong quá trình phát triển và nghiên cứu về vấn đề hạt nhân trong thời kỳ Chiến tranh thế giới thứ hai. Dự án Manhattan, một dự án nghiên cứu quy mô lớn do Mỹ thực hiện để phát triển bom nguyên tử, đặt ra nhiều thách thức về tính toán với những biến số không chắc chắn.\nJohn von Neumann, một nhà toán học và nhà vật lý người Mỹ-Hungari, cùng với nhóm nghiên cứu của ông, đã phát triển phương pháp Monte Carlo Simulation để giải quyết những vấn đề này. Tên gọi “Monte Carlo” xuất phát từ tên của thành phố Monaco nổi tiếng với sòng bạc và các trò chơi may rủi, thể hiện tính ngẫu nhiên trong quá trình mô phỏng.\nPhương pháp Monte Carlo Simulation được sử dụng rộng rãi trong các lĩnh vực như vật lý, tài chính, kỹ thuật, và nhiều ngành khác nữa. Đến nay, nó vẫn là một công cụ quan trọng trong mô hình hóa và dự đoán các biến số không chắc chắn, đặc biệt là khi không có phương trình toán học chính xác để mô tả các hệ thống.\n\n\n\nPhương pháp Bootstrap được giới thiệu bởi Bradley Efron vào cuối thập kỷ 1970, đặc biệt là trong bài báo mang tiêu đề “Bootstrap Methods: Another Look at the Jackknife” (1979). Trong bài báo này, Efron đề xuất phương pháp Bootstrap như một phương pháp thay thế và mở rộng cho phương pháp Jackknife, một phương pháp thống kê truyền thống.\nNgữ cảnh lịch sử: - Trước khi Bootstrap xuất hiện, các phương pháp thống kê thường phải dựa vào giả định về phân phối của dữ liệu hoặc kích thước mẫu lớn để áp dụng các ước lượng thống kê. - Efron nhận ra rằng, thay vì phụ thuộc vào giả định phân phối, chúng ta có thể sử dụng dữ liệu hiện có để đánh giá phân phối của một ước lượng thống kê thông qua việc tái chọn mẫu từ dữ liệu.\nCơ sở Lý Thuyết: - Bootstrap dựa trên ý tưởng rằng mẫu mà chúng ta có thể thu thập từ dữ liệu mẫu hiện có có thể đưa ra thông tin tốt về biến động của ước lượng thống kê. - Thay vì giả định về phân phối, Bootstrap sử dụng dữ liệu để xác định phân phối thông tin của ước lượng.\nỨng Dụng Đầu Tiên: - Phương pháp Bootstrap nhanh chóng được áp dụng rộng rãi trong thống kê và các lĩnh vực khác. Ban đầu, nó thường được sử dụng để đánh giá độ chắc chắn của các ước lượng thống kê như trung bình và phương sai. - Bootstrap cũng đã mở ra một hướng mới trong nghiên cứu thống kê, giúp làm thay đổi cách chúng ta xem xét và xử lý dữ liệu.\nPhương pháp Bootstrap đã trở thành một công cụ mạnh mẽ trong công việc thống kê và nghiên cứu khoa học, giúp làm giảm sự phụ thuộc vào giả định về phân phối và đồng thời cung cấp cách tiếp cận linh hoạt hơn trong đối mặt với dữ liệu thực tế.",
    "crumbs": [
      "Statistic",
      "So sánh giữa Monte Carlo Simulation và Bootstrap"
    ]
  },
  {
    "objectID": "statistic/MonteCarlo_Bootstrap.html#so-sánh-monte-carlo-simulation-và-bootstrap",
    "href": "statistic/MonteCarlo_Bootstrap.html#so-sánh-monte-carlo-simulation-và-bootstrap",
    "title": "So sánh giữa Monte Carlo Simulation và Bootstrap",
    "section": "So sánh Monte Carlo Simulation: và Bootstrap:",
    "text": "So sánh Monte Carlo Simulation: và Bootstrap:\n\nMục Tiêu Chính:\n\nMonte Carlo Simulation: Được sử dụng để mô phỏng hành vi của một hệ thống phức tạp thông qua việc tạo ra dữ liệu ngẫu nhiên và thực hiện phân tích.\nBootstrap: Sử dụng để đánh giá độ chắc chắn của ước lượng thống kê và xây dựng khoảng tin cậy thông qua việc tái chọn mẫu từ dữ liệu đã có.\n\nLoại Ứng Dụng:\n\nMonte Carlo Simulation: Thường được sử dụng trong mô hình hóa hệ thống phức tạp như tài chính, vật lý, và kỹ thuật.\nBootstrap: Thường được sử dụng trong thống kê để đánh giá độ biến động của ước lượng thống kê.\n\nDữ Liệu Sử Dụng:\n\nMonte Carlo Simulation: Thường sử dụng để tạo ra dữ liệu mới dựa trên mô hình toán học hoặc cảm nhận về hệ thống.\nBootstrap: Sử dụng dữ liệu hiện có và tái chọn mẫu từ nó để ước tính phân phối của một thống kê.\n\nMục Tiêu Ước Tính:\n\nMonte Carlo Simulation: Đưa ra ước lượng về hành vi của một biến số hay hệ thống thông qua việc lặp lại mô phỏng.\nBootstrap: Đưa ra ước lượng của phân phối của một thống kê cụ thể, chẳng hạn như trung bình hoặc phương sai.\n\nPhương Pháp Tạo Ngẫu Nhiên:\n\nMonte Carlo Simulation: Sử dụng phương pháp tạo số ngẫu nhiên để mô phỏng dữ liệu mới.\nBootstrap: Sử dụng phương pháp tái chọn mẫu từ dữ liệu hiện có để tạo ra các tập dữ liệu con.\n\nỨng Dụng Cụ Thể:\n\nMonte Carlo Simulation: Có thể được sử dụng để mô phỏng giá trị của tùy chọn tài chính, đánh giá rủi ro dự án, hoặc mô phỏng quá trình vật lý.\nBootstrap: Thường được sử dụng để đánh giá độ chắc chắn của ước lượng thống kê như trung bình, phương sai, hoặc hệ số tương quan.\n\nMức Độ Phức Tạp:\n\nMonte Carlo Simulation: Thường phức tạp hơn, đặc biệt là khi mô phỏng hệ thống phức tạp với nhiều yếu tố.\nBootstrap: Tương đối đơn giản và dễ triển khai.\n\nĐối Tượng Nghiên Cứu:\n\nMonte Carlo Simulation: Thường được ưa chuộng trong nghiên cứu lĩnh vực khoa học và kỹ thuật.\nBootstrap: Thường được sử dụng trong thống kê và nghiên cứu xã hội.\n\n\nTóm lại, Monte Carlo Simulation và Bootstrap là hai phương pháp mạnh mẽ được sử dụng trong các lĩnh vực khác nhau với mục tiêu và ứng dụng riêng biệt. Monte Carlo Simulation thường được sử dụng để mô phỏng và dự đoán hành vi của hệ thống phức tạp, trong khi Bootstrap tập trung vào đánh giá độ chắc chắn của ước lượng thống kê từ dữ liệu hiện có.",
    "crumbs": [
      "Statistic",
      "So sánh giữa Monte Carlo Simulation và Bootstrap"
    ]
  },
  {
    "objectID": "statistic/MonteCarlo_Bootstrap.html#ví-dụ-về-áp-dụng-phương-pháp-monte-carlo-simulation",
    "href": "statistic/MonteCarlo_Bootstrap.html#ví-dụ-về-áp-dụng-phương-pháp-monte-carlo-simulation",
    "title": "So sánh giữa Monte Carlo Simulation và Bootstrap",
    "section": "Ví dụ về áp dụng phương pháp Monte Carlo Simulation",
    "text": "Ví dụ về áp dụng phương pháp Monte Carlo Simulation\n\nVí dụ: Ước tính giá trị Pi\nGiả sử chúng ta muốn ước tính giá trị Pi bằng phương pháp Monte Carlo Simulation. Ý tưởng là vẽ một hình vuông có cạnh dài \\(2r\\) và nằm trong một hình tròn có bán kính \\(r\\). Nếu ta ngẫu nhiên chọn một điểm bất kỳ bên trong hình vuông, xác suất nó rơi vào hình tròn sẽ tỉ lệ với diện tích của hình tròn so với hình vuông.\n\nBước 1: Tạo điểm ngẫu nhiên:\n\nTạo một số lượng lớn \\(N\\) điểm ngẫu nhiên trong hình vuông.\n\nBước 2: Kiểm tra vị trí của điểm:\n\nĐối với mỗi điểm, kiểm tra xem nó có nằm trong hình tròn hay không bằng cách kiểm tra \\(x^2 + y^2 \\leq r^2\\).\n\nBước 3: Ước tính giá trị Pi:\n\nSử dụng tỉ lệ giữa số điểm nằm trong hình tròn và tổng số điểm để ước tính giá trị Pi: \\(\\pi \\approx \\frac{\\text{Số điểm trong hình tròn}}{\\text{Tổng số điểm}} \\times 4\\).\n\n\nDưới đây là một đoạn mã Python đơn giản để thực hiện ví dụ này:\n\nimport random\n\ndef estimate_pi(num_points):\n    points_inside_circle = 0\n\n    for _ in range(num_points):\n        x = random.uniform(-1, 1)\n        y = random.uniform(-1, 1)\n\n        if x**2 + y**2 &lt;= 1:\n            points_inside_circle += 1\n\n    pi_estimate = (points_inside_circle / num_points) * 4\n    return pi_estimate\n\n# Thử nghiệm với 1 triệu điểm\nnum_points = 1000000\npi_approximation = estimate_pi(num_points)\n\nprint(f\"Giá trị Pi ước tính với {num_points} điểm: {pi_approximation}\")\n\nGiá trị Pi ước tính với 1000000 điểm: 3.144\n\n\n\n\nVí dụ: Định giá tùy chọn (Option Pricing) trong Tài chính\nMonte Carlo Simulation được sử dụng rộng rãi trong tài chính để định giá các tùy chọn (options). Giả sử bạn muốn ước tính giá của một tùy chọn chứng khoán dựa trên mô hình Black-Scholes, một mô hình phổ biến trong tài chính.\n\nBước 1: Xác định các tham số của mô hình:\n\nSố liệu như giá chứng khoán hiện tại (\\(S\\)), giá thực hiện (\\(K\\)), thời gian đến hết hạn (\\(T\\)), biến động phần trăm hàng năm (\\(\\sigma\\)), và lãi suất không rủi ro (\\(r\\)).\n\nBước 2: Tạo ngẫu nhiên các biến đầu vào:\n\nSử dụng Monte Carlo để tạo ra một lượng lớn các biến ngẫu nhiên, chẳng hạn như biến động giá (\\(\\Delta S\\)) và thời gian đến hết hạn (\\(\\Delta t\\)).\n\nBước 3: Mô phỏng giá chứng khoán tương lai:\n\nSử dụng các biến ngẫu nhiên để mô phỏng giá chứng khoán tương lai theo mô hình Black-Scholes.\n\nBước 4: Định giá tùy chọn:\n\nSử dụng giá chứng khoán tương lai để định giá tùy chọn dựa trên điều kiện thị trường.\n\n\nDưới đây là một đoạn mã Python đơn giản để thực hiện ví dụ này: Trong ví dụ này, chúng ta sử dụng Monte Carlo để mô phỏng giá chứng khoán tương lai và sau đó định giá một tùy chọn dựa trên mô hình Black-Scholes. Monte Carlo giúp xác định phân phối xác suất của giá chứng khoán tương lai và từ đó đưa ra giá trị hiện tại của tùy chọn.\n\nimport numpy as np\nimport math\n\ndef black_scholes_simulation(S, K, T, r, sigma, num_simulations):\n    dt = T / 252  # Assume 252 trading days in a year\n    simulations = np.zeros(num_simulations)\n\n    for i in range(num_simulations):\n        path = [S]\n        for _ in range(252):\n            Z = np.random.normal(0, 1)\n            S_t = path[-1] * math.exp((r - 0.5 * sigma**2) * dt + sigma * math.sqrt(dt) * Z)\n            path.append(S_t)\n\n        simulations[i] = max(0, math.exp(-r * T) * (np.mean(path) - K))\n\n    option_price = np.mean(simulations)\n    return option_price\n\n# Thử nghiệm với các tham số\nstock_price = 100\nstrike_price = 100\nexpiry = 1  # 1 year\nrisk_free_rate = 0.05\nvolatility = 0.2\nnum_simulations = 100000\n\noption_price = black_scholes_simulation(stock_price, strike_price, expiry, risk_free_rate, volatility, num_simulations)\n\nprint(f\"Giá tùy chọn ước tính: {option_price}\")\n\nGiá tùy chọn ước tính: 5.74496815639896",
    "crumbs": [
      "Statistic",
      "So sánh giữa Monte Carlo Simulation và Bootstrap"
    ]
  },
  {
    "objectID": "statistic/MonteCarlo_Bootstrap.html#ví-dụ-sử-dụng-phương-pháp-bootstrap",
    "href": "statistic/MonteCarlo_Bootstrap.html#ví-dụ-sử-dụng-phương-pháp-bootstrap",
    "title": "So sánh giữa Monte Carlo Simulation và Bootstrap",
    "section": "Ví dụ Sử Dụng Phương Pháp Bootstrap",
    "text": "Ví dụ Sử Dụng Phương Pháp Bootstrap\nVí dụ Sử Dụng Phương Pháp Bootstrap để Ước Tính Trung Bình:\nGiả sử bạn có một tập dữ liệu đo chiều cao của một mẫu người và bạn muốn ước tính trung bình chiều cao của toàn bộ dân số. Thay vì dựa vào các giả định phân phối, bạn có thể sử dụng phương pháp Bootstrap để đánh giá độ chắc chắn của ước lượng trung bình.\nTrong ví dụ dưới, chúng ta sử dụng Bootstrap để tạo ra nhiều mẫu tái chọn từ dữ liệu chiều cao của mẫu người và tính toán trung bình của mỗi mẫu tái chọn. Kết quả là một phân phối mẫu trung bình được sử dụng để ước lượng giá trị trung bình và xây dựng khoảng tin cậy 95%.\nBootstrap giúp chúng ta đánh giá độ chắc chắn của ước lượng trung bình mà không cần phải làm giả định về phân phối của dữ liệu.\n\n\nimport numpy as np\n\n# Tạo một tập dữ liệu mẫu (giả sử đây là chiều cao của một mẫu người)\nnp.random.seed(42)\nsample_data = np.random.normal(loc=170, scale=5, size=100)\n\n# Hàm Bootstrap để ước lượng trung bình\ndef bootstrap_mean(data, num_samples=1000):\n    sample_means = np.zeros(num_samples)\n    \n    for i in range(num_samples):\n        bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n        sample_means[i] = np.mean(bootstrap_sample)\n    \n    return sample_means\n\n# Áp dụng Bootstrap để ước lượng trung bình và đánh giá độ chắc chắn\nbootstrap_means = bootstrap_mean(sample_data)\n\n# Tính toán khoảng tin cậy 95%\nconfidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n\n# Ước lượng trung bình thực sự từ dữ liệu mẫu\ntrue_mean = np.mean(sample_data)\n\nprint(f\"Trung bình ước lượng từ Bootstrap: {np.mean(bootstrap_means)}\")\nprint(f\"Khoảng tin cậy 95%: {confidence_interval}\")\nprint(f\"Trung bình thực sự từ dữ liệu mẫu: {true_mean}\")\n\n\n\nTrung bình ước lượng từ Bootstrap: 169.48841800757592\nKhoảng tin cậy 95%: [168.63831788 170.31425743]\nTrung bình thực sự từ dữ liệu mẫu: 169.48076741302958",
    "crumbs": [
      "Statistic",
      "So sánh giữa Monte Carlo Simulation và Bootstrap"
    ]
  },
  {
    "objectID": "statistic/KS-PSI_ENTROPY.html",
    "href": "statistic/KS-PSI_ENTROPY.html",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "",
    "text": "Chỉ số Ổn định tổng thể (PSI - Population Stability Index) là một phép đo thường được sử dụng trong ngành ngân hàng, đặc biệt trong việc xếp hạng tín dụng, để giám sát sự ổn định và hiệu suất của các mô hình xếp hạng theo thời gian. Đây là cách để định lượng sự thay đổi trong phân phối tổng thể, điều này có thể ảnh hưởng đến sức mạnh dự đoán của một mô hình.\n\nPSI được tính như sau:\n\\[ PSI = \\sum_{i=1}^{N} (Actual_{i} - Expected_{i}) \\log \\left( \\frac{Actual_{i}}{Expected_{i}} \\right) \\]\nTrong đó:\n\n\\(Actual_{i}\\) và \\(Expected_{i}\\) là tỷ lệ quan sát rơi vào bin (i) cho dữ liệu thực tế (mới hoặc kiểm tra) và dữ liệu mong đợi (cũ hoặc đào tạo), tương ứng. - (N) là tổng số bins.\n\nCách giải thích thông thường về các giá trị PSI như sau:\n\nPSI &lt; 0.1: Không có sự thay đổi đáng kể về tổng thể. Mô hình thường được coi là ổn định.\n0.1 ≤ PSI &lt; 0.25: Có một số thay đổi nhỏ trong tổng thể, có thể cần được điều tra thêm.\nPSI ≥ 0.25: Sự thay đổi đáng kể về tổng thể. Mô hình có thể không còn phù hợp và cần được cập nhật hoặc xây dựng lại.\n\nLưu ý rằng những ngưỡng này không phải là cố định và có thể thay đổi tùy thuộc vào đặc điểm cụ thể của tình huống và mức độ rủi ro bạn sẵn sàng chấp nhận.\n\nimport numpy as np\ndef calculate_psi(expected, actual, bins=10, categorical=False):\n    \"\"\"\n    Calculate the PSI (Population Stability Index) between expected and actual data.\n    \n    Args:\n    expected: numpy array of original values\n    actual: numpy array of new values, same size as expected\n    bins: number of bins to use in calculation, defaults to 10\n    categorical: boolean, if True indicates that the input variables are categorical\n    \n    # Example usage for categorical variables:\n    expected_categorical = np.random.choice(['A', 'B', 'C'], size=500, p=[0.4, 0.5, 0.1])\n    actual_categorical = np.random.choice(['A', 'B', 'C'], size=500, p=[0.42, 0.48, 0.1]) \n    \n    psi_value_categorical = calculate_psi(expected_categorical, actual_categorical, categorical=True)\n    psi_value_categorical\n    \n    Returns:\n    psi_value: calculated PSI value\n    \"\"\"\n\n    # Check if the variables are categorical\n    if categorical:\n        # Get unique categories\n        categories = np.unique(np.concatenate([expected, actual]))\n        \n        # Calculate the expected and actual proportions for each category\n        expected_probs = np.array([np.sum(expected == cat) for cat in categories]) / len(expected)\n        actual_probs = np.array([np.sum(actual == cat) for cat in categories]) / len(actual)\n    else:\n        # Define the bin edges for the histogram\n        bin_edges = np.histogram_bin_edges(expected, bins=bins)\n\n        # Calculate the expected and actual proportions for each bin\n        expected_probs, _ = np.histogram(expected, bins=bin_edges)\n        actual_probs, _ = np.histogram(actual, bins=bin_edges)\n\n        # Normalize to get proportions\n        expected_probs = expected_probs / len(expected)\n        actual_probs = actual_probs / len(actual)\n\n    # Initialize PSI\n    psi_value = 0\n\n    # Loop over each bin or category\n    for bin in range(len(expected_probs)):\n        # Avoid division by zero and log of zero\n        if expected_probs[bin] == 0 or actual_probs[bin] == 0:\n            continue\n        # Calculate the PSI for this bin or category\n        psi_value += (expected_probs[bin] - actual_probs[bin]) * np.log(expected_probs[bin] / actual_probs[bin])\n\n    return psi_value"
  },
  {
    "objectID": "statistic/KS-PSI_ENTROPY.html#psi",
    "href": "statistic/KS-PSI_ENTROPY.html#psi",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "",
    "text": "Chỉ số Ổn định tổng thể (PSI - Population Stability Index) là một phép đo thường được sử dụng trong ngành ngân hàng, đặc biệt trong việc xếp hạng tín dụng, để giám sát sự ổn định và hiệu suất của các mô hình xếp hạng theo thời gian. Đây là cách để định lượng sự thay đổi trong phân phối tổng thể, điều này có thể ảnh hưởng đến sức mạnh dự đoán của một mô hình.\n\nPSI được tính như sau:\n\\[ PSI = \\sum_{i=1}^{N} (Actual_{i} - Expected_{i}) \\log \\left( \\frac{Actual_{i}}{Expected_{i}} \\right) \\]\nTrong đó:\n\n\\(Actual_{i}\\) và \\(Expected_{i}\\) là tỷ lệ quan sát rơi vào bin (i) cho dữ liệu thực tế (mới hoặc kiểm tra) và dữ liệu mong đợi (cũ hoặc đào tạo), tương ứng. - (N) là tổng số bins.\n\nCách giải thích thông thường về các giá trị PSI như sau:\n\nPSI &lt; 0.1: Không có sự thay đổi đáng kể về tổng thể. Mô hình thường được coi là ổn định.\n0.1 ≤ PSI &lt; 0.25: Có một số thay đổi nhỏ trong tổng thể, có thể cần được điều tra thêm.\nPSI ≥ 0.25: Sự thay đổi đáng kể về tổng thể. Mô hình có thể không còn phù hợp và cần được cập nhật hoặc xây dựng lại.\n\nLưu ý rằng những ngưỡng này không phải là cố định và có thể thay đổi tùy thuộc vào đặc điểm cụ thể của tình huống và mức độ rủi ro bạn sẵn sàng chấp nhận.\n\nimport numpy as np\ndef calculate_psi(expected, actual, bins=10, categorical=False):\n    \"\"\"\n    Calculate the PSI (Population Stability Index) between expected and actual data.\n    \n    Args:\n    expected: numpy array of original values\n    actual: numpy array of new values, same size as expected\n    bins: number of bins to use in calculation, defaults to 10\n    categorical: boolean, if True indicates that the input variables are categorical\n    \n    # Example usage for categorical variables:\n    expected_categorical = np.random.choice(['A', 'B', 'C'], size=500, p=[0.4, 0.5, 0.1])\n    actual_categorical = np.random.choice(['A', 'B', 'C'], size=500, p=[0.42, 0.48, 0.1]) \n    \n    psi_value_categorical = calculate_psi(expected_categorical, actual_categorical, categorical=True)\n    psi_value_categorical\n    \n    Returns:\n    psi_value: calculated PSI value\n    \"\"\"\n\n    # Check if the variables are categorical\n    if categorical:\n        # Get unique categories\n        categories = np.unique(np.concatenate([expected, actual]))\n        \n        # Calculate the expected and actual proportions for each category\n        expected_probs = np.array([np.sum(expected == cat) for cat in categories]) / len(expected)\n        actual_probs = np.array([np.sum(actual == cat) for cat in categories]) / len(actual)\n    else:\n        # Define the bin edges for the histogram\n        bin_edges = np.histogram_bin_edges(expected, bins=bins)\n\n        # Calculate the expected and actual proportions for each bin\n        expected_probs, _ = np.histogram(expected, bins=bin_edges)\n        actual_probs, _ = np.histogram(actual, bins=bin_edges)\n\n        # Normalize to get proportions\n        expected_probs = expected_probs / len(expected)\n        actual_probs = actual_probs / len(actual)\n\n    # Initialize PSI\n    psi_value = 0\n\n    # Loop over each bin or category\n    for bin in range(len(expected_probs)):\n        # Avoid division by zero and log of zero\n        if expected_probs[bin] == 0 or actual_probs[bin] == 0:\n            continue\n        # Calculate the PSI for this bin or category\n        psi_value += (expected_probs[bin] - actual_probs[bin]) * np.log(expected_probs[bin] / actual_probs[bin])\n\n    return psi_value"
  },
  {
    "objectID": "statistic/KS-PSI_ENTROPY.html#ks-kolmogorov-smirnov",
    "href": "statistic/KS-PSI_ENTROPY.html#ks-kolmogorov-smirnov",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "KS (Kolmogorov-Smirnov)",
    "text": "KS (Kolmogorov-Smirnov)\n\nKiểm định KS (Kolmogorov-Smirnov) là một kiểm định thống kê phi tham số dùng để so sánh hai phân phối tích lũy (CDFs) hoặc một mẫu dữ liệu với một phân phối lý thuyết. Nó có hai ứng dụng chính:\n\nKiểm tra sự phù hợp của mẫu: Được sử dụng để kiểm tra xem một tập dữ liệu có tuân theo một phân phối lý thuyết cụ thể (như phân phối chuẩn, phân phối đều, v.v.) hay không.\nSo sánh hai mẫu dữ liệu: Được sử dụng để kiểm tra xem hai tập dữ liệu có xuất phát từ cùng một phân phối gốc hay không.\n\nCông thức tính toán cho chỉ số KS là:\n\\[ D = \\max |F_1(x) - F_2(x)| \\]\nTrong đó:\n\n$ F_1(x) $ và $F_2(x) $ là hai hàm phân phối tích lũy cần so sánh.\nD là giá trị lớn nhất của sự khác biệt tuyệt đối giữa hai hàm phân phối tích lũy trên toàn bộ phạm vi x.\n\nMột đặc điểm quan trọng của kiểm định KS là nó không yêu cầu giả định về dạng của phân phối, làm cho nó trở thành một công cụ mạnh mẽ và linh hoạt khi so sánh phân phối.\n\nfrom scipy.stats import ks_2samp\n\n# Generate two sample datasets\ndata1 = np.random.normal(0, 1, 1000)\ndata2 = np.random.normal(0.5, 1.5, 1000)\n\n# Compute the KS statistic and p-value\nks_statistic, p_value = ks_2samp(data1, data2)\n\nks_statistic, p_value\n\n(np.float64(0.22), np.float64(1.3152720028193915e-21))"
  },
  {
    "objectID": "statistic/KS-PSI_ENTROPY.html#divergence-test-cross-entropy",
    "href": "statistic/KS-PSI_ENTROPY.html#divergence-test-cross-entropy",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "Divergence Test (cross-entropy)",
    "text": "Divergence Test (cross-entropy)\n\nKiểm định Divergence, thường được gọi là Divergence Kullback-Leibler (KL), là một chỉ số đo sự khác biệt giữa một phân phối xác suất so với một phân phối xác suất thứ hai mong đợi. Nó được sử dụng để so sánh hai phân phối xác suất cho cùng một sự kiện.\nCho hai phân phối xác suất, P và Q, Divergence Kullback-Leibler của Q so với P được định nghĩa như sau:\n\\[ D_{KL}(P||Q) = \\sum_{i} P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)\\]\nTrong đó:\n\n\\(P(i)\\) là xác suất của sự kiện i theo phân phối P,\n\\(Q(i)\\) là xác suất của sự kiện i theo phân phối Q,\nTổng được tính trên tất cả các sự kiện i có thể xảy ra.\n\nMột số điểm quan trọng về KL Divergence:\n\nKhông đối xứng: \\(D_{KL}(P||Q)\\) không bằng \\(D_{KL}(Q||P)\\). Điều này có nghĩa là Divergence KL của Q so với P không giống như Divergence KL của P so với Q.\nKhông âm: Divergence KL luôn không âm, và nó bằng không chỉ khi P và Q là cùng một phân phối.\nĐơn vị: Divergence KL được đo bằng bit nếu logarithm có cơ số 2 (log2), hoặc bằng nats nếu logarithm có cơ số e (logarithm tự nhiên).\n\nTrên thực tế, KL Divergence có thể được sử dụng để đo sự khác biệt giữa phân phối thực tế và dự đoán, hoặc giữa một phân phối quan sát và một phân phối lý thuyết. Nó đặc biệt phổ biến trong các lĩnh vực như lý thuyết thông tin và học máy.\n\nimport numpy as np\n\ndef kl_divergence(p, q):\n    \"\"\"Compute KL divergence of two probability distributions.\"\"\"\n    return np.sum(p * np.log(p / q))\n\n# Example distributions\np = np.array([0.4, 0.5, 0.1])\nq = np.array([0.3, 0.4, 0.3])\n\n# Ensure the distributions are valid (i.e., sum to 1 and non-negative)\nassert np.all(p &gt;= 0) and np.all(q &gt;= 0)\nassert np.isclose(p.sum(), 1) and np.isclose(q.sum(), 1)\n\n# Calculate KL Divergence\ndivergence_value = kl_divergence(p, q)\nprint(f\"KL Divergence between p and q: {divergence_value:.4f}\")\n\nKL Divergence between p and q: 0.1168\n\n\n\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef kl_divergence(p, q):\n    \"\"\"Compute KL divergence of two probability distributions.\"\"\"\n    return entropy(p, q)\n\n# Example distributions\np = np.array([0.4, 0.5, 0.1])\nq = np.array([0.3, 0.4, 0.3])\n\n# Calculate KL Divergence from p to q\ndivergence_value = kl_divergence(p, q)\n\nprint(f\"Cross entropy: {divergence_value:.4f}\")\n\nCross entropy: 0.1168"
  },
  {
    "objectID": "statistic/KS-PSI_ENTROPY.html#so-sánh-ks-và-divergence",
    "href": "statistic/KS-PSI_ENTROPY.html#so-sánh-ks-và-divergence",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "So sánh KS và Divergence",
    "text": "So sánh KS và Divergence\n\nMục đích:\n\nKiểm định KS: Đây là một kiểm định phi tham số được sử dụng để xác định xem hai mẫu có xuất phát từ cùng một phân phối hay không. Thống kê KS đo sự khác biệt lớn nhất giữa các hàm phân phối tích lũy (CDFs) của hai mẫu.\nKiểm định Divergence (KL Divergence): Nó đo cách một phân phối xác suất khác biệt so với một phân phối xác suất thứ hai mong đợi. Nó thường được sử dụng trong lý thuyết thông tin để đo “khoảng cách” giữa hai phân phối.\n\nKết quả:\n\nKiểm định KS: Kết quả là một chỉ số (D) đại diện cho sự khác biệt lớn nhất giữa hai CDFs và một giá trị p kiểm tra giả thuyết rằng hai mẫu được rút ra từ cùng một phân phối.\nKiểm định Divergence (KL Divergence): Kết quả là một giá trị không âm, trong đó giá trị 0 chỉ ra rằng hai phân phối là giống nhau. Lưu ý rằng KL Divergence không đối xứng, tức là \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\).\n\nGiả định:\n\nKiểm định KS: Không giả định về phân phối của dữ liệu.\nKiểm định Divergence (KL Divergence): Giả định \\(Q(i) &gt; 0\\) cho bất kỳ i nào sao cho \\(P(i) &gt; 0\\), nếu không sự khác biệt sẽ vô cùng.\n\nỨng dụng:\n\nKiểm định KS: Thường được sử dụng trong kiểm định giả thuyết để xác định xem một mẫu dữ liệu có tuân theo một phân phối cụ thể hay không.\nKiểm định Divergence (KL Divergence): Rộng rãi được sử dụng trong lý thuyết thông tin, học máy và thống kê, đặc biệt khi so sánh một phân phối thực nghiệm với một phân phối lý thuyết.\n\nGiải thích:\n\nKiểm định KS: Một giá trị p nhỏ cho thấy rằng hai mẫu đến từ các phân phối khác nhau.\nKiểm định Divergence (KL Divergence): Một Divergence KL lớn hơn chỉ ra rằng hai phân phối khác biệt hơn so với nhau.\n\n\nTóm lại, mặc dù cả Kiểm định KS và KL Divergence đều được sử dụng để so sánh các phân phối, nhưng chúng có các phương pháp, giải thích và ứng dụng khác nhau. Sự lựa chọn giữa chúng phụ thuộc vào vấn đề cụ thể và bản chất của dữ liệu."
  },
  {
    "objectID": "irb/ModelGuideline.html",
    "href": "irb/ModelGuideline.html",
    "title": "Model guideline",
    "section": "",
    "text": "Nhìn tổng thể chân dung khách hàng",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#triết-lý-xây-dựng-mô-hình",
    "href": "irb/ModelGuideline.html#triết-lý-xây-dựng-mô-hình",
    "title": "Model guideline",
    "section": "",
    "text": "Nhìn tổng thể chân dung khách hàng",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#phạm-vi-mô-hình",
    "href": "irb/ModelGuideline.html#phạm-vi-mô-hình",
    "title": "Model guideline",
    "section": "Phạm vi mô hình",
    "text": "Phạm vi mô hình\n\nCần xác định rõ phạm vi của mô hình trước khi bắt đầu xây dựng & phát triển mô hình",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#single-factor-analysis",
    "href": "irb/ModelGuideline.html#single-factor-analysis",
    "title": "Model guideline",
    "section": "Single factor analysis",
    "text": "Single factor analysis\n\nWoE binning\n\nSố lượng nhóm phân chia dựa trên tỷ lệ bad rate tương đối, nếu 5 nhóm thì nhóm thứ 3 tỷ lệ này xấp xỉ 1\nSố lượng nhóm khoảng 5-7 nhóm (đẹp nhất là 5 nhóm)\nNên chọn số lượng nhóm số lẻ\nMissing tùy thuộc từng trường hợp sẽ nhóm vào nhóm có tỷ lệ bad rate cao nhất hoặc nhóm có tỷ lệ bad rate xấp xỉ\nTùy từng biến có trend sẽ coasre trend theo biến\n\n\n\nChuẩn hóa biến\n\nNormalising Score: Dùng để dễ so sánh coef của mô hình đa biến sau khi đã hồi quy. Normalised Log Odds thông thường có trung bình = 0, std = 50",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#multiple-factors-analysis",
    "href": "irb/ModelGuideline.html#multiple-factors-analysis",
    "title": "Model guideline",
    "section": "Multiple Factors Analysis",
    "text": "Multiple Factors Analysis\n\nTheo triết lý xây dựng mô hình, mỗi nhóm thông tin khách hàng có ít nhất 1 biến:\nVD: Nhóm thông tin chung có 1 biến, nhóm thông tin hành vi tiêu dùng 3 biến, … các nhóm thông tin còn lại có ít nhất 1 biến",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#chọn-mô-hình-cuối-cùng",
    "href": "irb/ModelGuideline.html#chọn-mô-hình-cuối-cùng",
    "title": "Model guideline",
    "section": "Chọn mô hình cuối cùng",
    "text": "Chọn mô hình cuối cùng\n\n30% &gt; std-coef &gt; 5%\nNếu có hệ số dưới 5%, điều chỉnh lại trọng số = 5%, rồi nhân ngược lại ra adjusted-coef\nCân nhắc trọng số của các nhóm thông tin: Ví dụ: Nhóm thông tin A có 2 biến tổng trọng số 10%, nhóm thông tin B có 3 biến tổng trọng số 50% (nhóm thông tin B này quá mạnh so với nhóm thông tin A)\nNên tăng hoặc giảm hệ số các biến trong cùng 1 nhóm thông tin",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#kiểm-định-mô-hình",
    "href": "irb/ModelGuideline.html#kiểm-định-mô-hình",
    "title": "Model guideline",
    "section": "Kiểm định mô hình",
    "text": "Kiểm định mô hình",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#giám-sát-mô-hình",
    "href": "irb/ModelGuideline.html#giám-sát-mô-hình",
    "title": "Model guideline",
    "section": "Giám sát mô hình",
    "text": "Giám sát mô hình\n\nGini đơn biến suy giảm quá 15%, hoặc 10% cho các biến nhân khẩu học điều chỉnh lại mô hình\nSau khi mô hình vi phạm các ngưỡng thì xem xét lại mô hình\nBáo cáo lại những yếu tố ảnh hưởng tới mô hình. VD: Tình hình cho vay, …\nFront-end monitoring: Xem sự dịch chuyển các nhóm hạng, danh mục\nBack-end monitoring: performance (test trên cả overall, sub-segment)của mô hình vẫn tốt, các factors vẫn phản ánh được kết quả của mô hình. Tùy thuộc dữ liệu có thể ấy dữ liệu từ 2 năm đến 5 năm dữ liệu\nThông thường các factors dễ thay đổi, có thể chạy mô hình trên dữ liệu mới\nPopulation thay đổi (PSI):\n\nFront-end\n\n\n\n\n\n\n\n\n\n\n\nDev\nOOT\nRemark\n\n\n\n\nPSI\nNA\n3.1%\nChỉ số ổn định tổng thể (PSI)\n- PSI &lt; 15%: chỉ số phân phối điểm ổn định.\n- PSI &gt;= 15% và &lt; 25%: chỉ số dịch chuyển vừa phải.\n- PSI &gt;=25%: thể hiện sự thay đổi đáng kể.\nNếu có sự thay đổi đáng kể, thẻ điểm có thể không áp dụng cho mẫu OOT.\n\n\n\nAHI\n5.1%\n4.0%\nChỉ số Herfindah được điều chỉnh (AHI) để đo độ tập trung trong dải điểm nhất định.\n- AHI &lt; 20%: không tập trung\n- AHI &gt;= 20%: có sự tập trung trong dải điểm nhất định. Cần điều tra để đảm bảo không có vấn đề gì với khả năng phân biệt rủi ro.\n\n\n\nBack-end\n\n\n\n\n\n\n\n\nOverall\nDev\nOOT\n\n\n\n\nGini\n\n\n\n\nNo of default\n\n\n\n\nOutcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSub-segment\nRisk Ranking\n\nDefault\n\nOutcome\n\n\n\n\n\n\nDev\nOOT\nDev\nOOT\nBaseline\n2020\n\n\nProduct type\n\n\n\n\n\n\n\n\nCredit card only\n\n\n\n\n\n\n\n\nPersonal Loan only\n\n\n\n\n\n\n\n\nĐơn vị kinh doanh\n\n\n\n\n\n\n\n\nHO\n\n\n\n\n\n\n\n\nCông ty tài chính\n\n\n\n\n\n\n\n\n\nFactor mới thêm vào có thể dùng phương pháp điều chỉnh weight",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#chuyển-đổi-điểm",
    "href": "irb/ModelGuideline.html#chuyển-đổi-điểm",
    "title": "Model guideline",
    "section": "Chuyển đổi điểm",
    "text": "Chuyển đổi điểm",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#apply-irb",
    "href": "irb/ModelGuideline.html#apply-irb",
    "title": "Model guideline",
    "section": "Apply IRB",
    "text": "Apply IRB\n\nQuy trình tín dụng\nQuy trình đánh giá khách hàng",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#pd-model-calibration",
    "href": "irb/ModelGuideline.html#pd-model-calibration",
    "title": "Model guideline",
    "section": "PD Model Calibration",
    "text": "PD Model Calibration\n\nHousing loan: với 6 tháng hành vi trả nợ chưa đủ mạnh, chưa có nhiều behavious, sử dụng MoB 12 để xác định khách hàng mới hoặc cũ\nThẻ tín dụng: chỉ cần 6 tháng đã có được hành vi tiêu dùng, trả nợ\nChú ý tính đồng nhất của danh mục\nNếu thay đổi chính sách thì mô hình phải phản ánh chính sách forward-looking\nNếu không có thông tin liên quan đến thị trường, thì phải có điều chỉnh downgrade PD\nA-score: Cyclicality ~ 40 -50%, B-score ~ 60-70%\nƯớc tính cyclicality: do 1 model thường không dùng qua 1 chu kỳ kinh tế, nếu back score thì có thẻ dữ liệu cũ không đủ. Những biến như age, gender thì không có cyclicality\nRetail có thể có master scale riêng với từng danh mục, nhưng có thể distribution không đẹp",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#xác-định-chu-kỳ-kinh-tế",
    "href": "irb/ModelGuideline.html#xác-định-chu-kỳ-kinh-tế",
    "title": "Model guideline",
    "section": "Xác định chu kỳ kinh tế",
    "text": "Xác định chu kỳ kinh tế\n\nThu thập dữ liệu càng dài càng tốt, tối thiểu 5 năm\nThị trường châu á, lấy mốc khủng hoảng kinh tế là 1998\nDùng portfolio có tính chất tương tự, có thể dùng portfolio tương tự của Central Bank NPL, chứng minh correlation sau đó extrapolate\nForward looking, chọn số trung bình giữa baseline và worst-case\nTrong giai đoạn covid, nếu có nhà nước có nhiều chính sách làm cho odr giảm, thì có thể bỏ những data point này ra khỏi mẫu trong thời kỳ crisis",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#chia-pool",
    "href": "irb/ModelGuideline.html#chia-pool",
    "title": "Model guideline",
    "section": "Chia pool",
    "text": "Chia pool\n\nDựa trên product types, pd, mob, bucket …",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#đánh-giá-lại-pd",
    "href": "irb/ModelGuideline.html#đánh-giá-lại-pd",
    "title": "Model guideline",
    "section": "Đánh giá lại PD",
    "text": "Đánh giá lại PD\n\ndựa vào CT, ODR, đánh giá implied pd còn phù hợp hay không",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#moc",
    "href": "irb/ModelGuideline.html#moc",
    "title": "Model guideline",
    "section": "MoC",
    "text": "MoC",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#thấu-chi",
    "href": "irb/ModelGuideline.html#thấu-chi",
    "title": "Model guideline",
    "section": "Thấu chi",
    "text": "Thấu chi\n\nCó tài sản đảm bảo\n\nNếu là KH housing loan vay thêm thấu chi, có thể dùng housing score card\n\n\n\nNếu tài sản là tiền gửi nhỏ, thì áp dụng standardise\n\nKhông có tài sản\n\nXếp chung vào nhóm credit card",
    "crumbs": [
      "Model guideline"
    ]
  }
]