[
  {
    "objectID": "transition-from-rmarkdown.html",
    "href": "transition-from-rmarkdown.html",
    "title": "Transition from RMarkdown",
    "section": "",
    "text": "You may already have workflows in RMarkdown and are interested in transitioning to Quarto. There’s no hurry to migrate to Quarto. Keep using Rmarkdown and when you’re ready the migration will be fine.\nHere are some notes as we migrate RMarkdown sites and books.\nTODO: translating R code chunks",
    "crumbs": [
      "Transition from Rmd"
    ]
  },
  {
    "objectID": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "href": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Bookdown to Quarto",
    "text": "Bookdown to Quarto\nConverting a Bookdown book to Quarto is slightly more involved than converting a website. A book has chapters whose order must be defined, and likely has citations and cross-refs. Still, conversion is not that hard.\nWe got some practice converting from Bookdown to Quarto by helping Gavin Fay convert his lab’s fantastic onboarding documentation, the Faylab Lab Manual. Here’s the GitHub view before and after.\nOur best first reference material for this was Nick Tierney’s Notes on Changing from Rmarkdown/Bookdown to Quarto. Nick shares some scripts in that post to automate some changes. In our case, the book was small enough that we made all changes manually. Quarto documentation was indispensable.\n\nExperimenting in a low-risk environment\nWe forked a copy of the Faylab Lab manual to the Openscapes organization, and worked in a branch so we could make changes relatively risk-free. We could always fork a new copy of the original if we “broke” something. (Caution: the default when making a pull request from a fork is to push changes to the original upstream repo, not your fork and it does this without warning if you have write-access to that repo.) With local previews it’s easy to test / play with settings to see what they do. We tended to make a change, Preview, then compare the look and functionality of the book to the original. It was helpful to comment out some elements of the configuration file _output.yml after their counterparts had been added to the Quarto configuration file _quarto.yml, or to confirm they were no longer needed, before making the drastic move of deleting them.\n\n\nThe conversion\nHere are the main steps to convert the Faylab Lab manual from Bookdown to Quarto.\nCreate new empty file called _quarto.yml and add book metadata there. The screenshots below\nSet project type as book.\nMove metadata out of index.qmd and into _quarto.yml. Title, author, and publication date were in index.qmd with date set using date: \"Last updated:r Sys.Date()\". Now these are in _quarto.yml with date set using date: last-modified. Note that having R code would require you to adjust code chunk options in the Quarto style (#|). This tripped us up a bit; see GitHub Actions.\nMove chapters listing out of _bookdown.yml and into _quarto.yml.\nAdd page footer to _quarto.yml.\nHere’s what ours looked like when we finished the steps above (_quarto.yml).\n\n\n\n\n\n\n_quarto.yml contents\n\n\n\n\n\n\n\nFaylab Lab Manual\n\n\n\n\n\nChange insertion of images from html style to Quarto style. (Note Quarto calls them “figures”, not “images”.) The following snippet will insert the GitHub octocat logo in a page:\n![](https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png){fig-align=\"left\" width=\"35px\"}\nChange all filename extensions .Rmd -&gt; .qmd (you could Preview after this change and see that the book looks the same). Note that Quarto works with .Rmd files just as well as it does .qmd, so this change is not urgent. In fact, if you have a lot of R code in your .Rmds (unlike the Faylab Lab Manual), there will be additional tinkering needed to make the code chunks happy.\n\n\nCitations\nThe Faylab Lab Manual cited two papers, presenting us with an opportunity to see how easy it is to add references to a Quarto book. Briefly, in the Visual Editor, Insert &gt; Citation &gt; DOI. Pasting the DOI or its full URL, we can insert the citation. This automatically creates a references.bib file and adds the full citations at the bottom of the chapter page (watch demo). In July 2022, we had to manually add a ## References heading, but this may not be necessary in future Quarto updates.\n\n\n\n\n\n\nInsert citation via its DOI using RStudio Visual Editor\n\n\n\n\n\n\n\n\n\n\nPublishing notes\nIf the book’s output is strictly html, there’s no need to specify output-dir in _quarto.yml. The output directory default is _book/, which is what we’d like. If we wanted other types of output like like PDF or EPUB, etc. those single file outputs are also written to the output-dir (Quarto docs).\nIf you currently have a docs/ folder, delete it.\nUpdate .gitignore to ignore _book/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_book/\nOnce all is settled, delete _output.yml.\nOnce the Openscapes fork was fully reviewed, we made a pull request from that to the main branch of the book’s repo. Once that was merged, we set up GitHub Actions to render the book. (TODO: instructions for GitHub Actions)\n\n\nGitHub Actions\nThis book was mostly prose and screenshots without any R code. This made the conversion from RMarkdown to Quarto likely more straightforward than if you also needed to adjust code chunk options in the quarto style (#|). Our initial GitHub Action to render the converted Faylab Lab Manual failed because we had a piece of R code - even though the code was commented out! This was resolved when we deleted the line.",
    "crumbs": [
      "Transition from Rmd"
    ]
  },
  {
    "objectID": "transition-from-rmarkdown.html#distill-to-quarto",
    "href": "transition-from-rmarkdown.html#distill-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Distill to quarto",
    "text": "Distill to quarto\nWe transitioned our events site from distill to quarto in May 2022 (github view before and after). We followed excellent notes and examples from Nick Tierney and Danielle Navarro.\nAfter we had changed all the files, the Build tab in the RStudio IDE still showed “Build website” rather then “Render Website” and “Preview Website”, and would error when we pushed them (because that button was expecting a distill site, not a quarto site). To fix this, we updated the .Rproj file. Clicking on the .Rproj file in the RStudio IDE will open a dialog box where you can click things you want (you can also open these in a text editor or from the GitHub website to see the actual text). To fix this situation with the Build tab: Project Options &gt; Build Tools &gt; Project Build Tools &gt; None.\nLooking at files /posts/_metadata.yml and _quarto.yml helps see where things are defined. For example, to make event post citations appear, we added citation: true to /posts/_metadata.yml and in _quarto.yml under the website key we set site-url: https://openscapes.github.io/events. We deleted footer.html used with distill because footer is now defined in quarto.yml.\n\nPublishing notes\n\nBackground: Our distill site had been set up to output to a docs folder, and had GitHub Settings &gt; Pages set to look there rather gh-pages branch. (Julie note: this was a new-to-me capability when we set up the events distill site in Spring 2021 so I had forgotten that was an option). We’ve inititally kept this same set-up for now with our events page in _quarto.yml: output-dir: docs. However, this is sub-optimal; better to not have to commit and push these files but to instead have a GitHub Action generate them upon a commit. So the following is what we did -\n\nDon’t specify output-dir in _quarto.yml. The output directory default is _site/, which is what we’d like.\nIf you currently have a docs/ folder (like we did as we were experimenting), delete it.\nUpdate .gitignore to ignore _site/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_site/\nPush these changes, merge into main.\nOn GitHub.com, in your repo, set up GitHub publishing\nFollow instructions from the explore and setup chapter.",
    "crumbs": [
      "Transition from Rmd"
    ]
  },
  {
    "objectID": "transition-from-rmarkdown.html#troubleshooting",
    "href": "transition-from-rmarkdown.html#troubleshooting",
    "title": "Transition from RMarkdown",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nGitHub Action fails, says you need RMarkdown but you don’t have R code!\nAnd you changed all .Rmds to .qmds!\nYou likely have a few setup code chunks from RMarkdown, that look like this:\n{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nYou can find them by opening each of your files and having a look, or use GitHub’s search for the keyword knitr",
    "crumbs": [
      "Transition from Rmd"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Giới thiệu",
    "section": "",
    "text": "Trong trang web này, tôi sẽ chia sẻ những dự án và tài liệu liên quan đến các chủ đề như:\n\nHọc máy (Machine Learning)\nThống kê (Statistics)\n\nTôi cũng rất quan tâm đến việc xây dựng các tài liệu dễ chia sẻ và có thể tái sử dụng, giúp cộng đồng dễ dàng áp dụng trong công việc của mình.\n\n\nDưới đây là các chủ đề mà tôi đã thực hiện và chia sẻ:\n\n\nTôi đã thực hiện nhiều nghiên cứu và dự án liên quan đến học máy, bao gồm các chủ đề như:\n\nFocal Loss: Cải thiện hiệu suất cho các mô hình phân loại mất cân bằng.\nDelong Test: Phương pháp so sánh đường cong ROC giữa các mô hình khác nhau.\nPermutation Feature Importance: Phân tích tầm quan trọng của các đặc trưng trong mô hình.\nSHAP: Giải thích các dự đoán của mô hình bằng phương pháp SHAP (SHapley Additive exPlanations).\nModel Calibration: Điều chỉnh xác suất dự đoán của mô hình.\nReinforcement Learning: Ứng dụng học tăng cường để tối ưu hóa mô hình.\n\nKhám phá thêm tại đây\n\n\n\nMột số nội dung tôi đã làm về thống kê bao gồm:\n\nConfidence Interval with Small Sample Size: Tính toán khoảng tin cậy cho mẫu dữ liệu nhỏ.\nPSI, KS, Entropy: Các phương pháp đánh giá sự khác biệt trong phân phối.\nDifference-in-Differences (DiD): Phương pháp để ước lượng ảnh hưởng của một sự can thiệp.\nMonte Carlo và Bootstrap: Kỹ thuật lấy mẫu để ước lượng thống kê.\n\nTìm hiểu thêm tại đây\n\n\n\n\nThông tin về các source code (github hoặc gitlab):\n\nTrang web cá nhân (same link for gitlab)\nGitHub (same link for gitlab)\nTwitter",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#các-nội-dung-chính",
    "href": "index.html#các-nội-dung-chính",
    "title": "Giới thiệu",
    "section": "",
    "text": "Dưới đây là các chủ đề mà tôi đã thực hiện và chia sẻ:\n\n\nTôi đã thực hiện nhiều nghiên cứu và dự án liên quan đến học máy, bao gồm các chủ đề như:\n\nFocal Loss: Cải thiện hiệu suất cho các mô hình phân loại mất cân bằng.\nDelong Test: Phương pháp so sánh đường cong ROC giữa các mô hình khác nhau.\nPermutation Feature Importance: Phân tích tầm quan trọng của các đặc trưng trong mô hình.\nSHAP: Giải thích các dự đoán của mô hình bằng phương pháp SHAP (SHapley Additive exPlanations).\nModel Calibration: Điều chỉnh xác suất dự đoán của mô hình.\nReinforcement Learning: Ứng dụng học tăng cường để tối ưu hóa mô hình.\n\nKhám phá thêm tại đây\n\n\n\nMột số nội dung tôi đã làm về thống kê bao gồm:\n\nConfidence Interval with Small Sample Size: Tính toán khoảng tin cậy cho mẫu dữ liệu nhỏ.\nPSI, KS, Entropy: Các phương pháp đánh giá sự khác biệt trong phân phối.\nDifference-in-Differences (DiD): Phương pháp để ước lượng ảnh hưởng của một sự can thiệp.\nMonte Carlo và Bootstrap: Kỹ thuật lấy mẫu để ước lượng thống kê.\n\nTìm hiểu thêm tại đây",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#thông-tin-thêm",
    "href": "index.html#thông-tin-thêm",
    "title": "Giới thiệu",
    "section": "",
    "text": "Thông tin về các source code (github hoặc gitlab):\n\nTrang web cá nhân (same link for gitlab)\nGitHub (same link for gitlab)\nTwitter",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "quarto-workflows/browser.html",
    "href": "quarto-workflows/browser.html",
    "title": "From the Browser",
    "section": "",
    "text": "A workflow from the browser if good for getting started (since you do not need to install additional software) and for making small contributions, but is definitely limited. Once you feel comfortable here, you can move to a different setup.\nHere’s an example of editing content on an existing page.",
    "crumbs": [
      "Quarto workflows",
      "From the Browser"
    ]
  },
  {
    "objectID": "quarto-workflows/browser.html#edit-content-on-an-existing-page",
    "href": "quarto-workflows/browser.html#edit-content-on-an-existing-page",
    "title": "From the Browser",
    "section": "Edit content on an existing page",
    "text": "Edit content on an existing page\nLet’s change the date on the home page of this website.\nIn your repository, navigate to index.md. Then, click the pencil icon in the top right to edit directly.\n\n\n\n\n\nWe are now in the “Edit file” tab of the editor, where we can make modifications. Let’s change the date to today’s date. Click the “Preview” tab to see your changes. You can even check the “Show diff” box on the right side to see the changes you’ve made.\n\n\n\n\n\nWhile you’re here, see if there are additional changes to the text you’d like to make. Maybe changing the title or author at the top, or for the main text on the home page of the website.\nOur index.md file is written in Markdown, which enables you to make simple text formatting. As you go back and forth from “Edit file” to “Preview”, notice the patterns of how the Markdown text looks when it is as source (“Edit file”) and when it is formatted (“Preview”). For example, in Markdown, you can make text as a header with # symbols, bold or italic with * symbols, and hyperlinks with [](). Notice that spacing is important: for example, there are carriage returns (when you hit the “return” key) before any bullet points. You can learn the short list of Markdown rules here: https://quarto.org/docs/authoring/markdown-basics.",
    "crumbs": [
      "Quarto workflows",
      "From the Browser"
    ]
  },
  {
    "objectID": "quarto-workflows/browser.html#commit-and-publish",
    "href": "quarto-workflows/browser.html#commit-and-publish",
    "title": "From the Browser",
    "section": "Commit and publish",
    "text": "Commit and publish\nCommit your changes by scrolling to the bottom of the page and writing a commit message - a note to yourself and others about what changes you made. Write your commit message and then click the green “Commit changes” button.\n\n\n\n\n\nNow, click back to the main page of your GitHub repository. You should see the orange dot confirming your website is published. You’ll have to wait for the GitHub Action to tell quarto to build your site for you to see the update, but it will be there!",
    "crumbs": [
      "Quarto workflows",
      "From the Browser"
    ]
  },
  {
    "objectID": "quarto-workflows/browser.html#limitations",
    "href": "quarto-workflows/browser.html#limitations",
    "title": "From the Browser",
    "section": "Limitations",
    "text": "Limitations\nWhile awesome that we can edit using GitHub directly from the browser, there are obvious limitations. One is that to see your edits show up in your book, you have to publish using the GitHub Action. This is slow. Another limitation is that we can only work on one file at a time and commit them each separately, which also is slow. Using additional software can make things much better, as we explore in subsequent chapters.",
    "crumbs": [
      "Quarto workflows",
      "From the Browser"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html",
    "href": "quarto-workflows/rstudio.html",
    "title": "From RStudio",
    "section": "",
    "text": "The RStudio software (called an IDE, integrated development environment) is an excellent way to edit files and interface with GitHub. Plus, as it is made by the same folks who make Quarto, it has many integrated features for streamlining your workflow with Quarto, including how it previews your edits and provides debugging support for yaml! Quarto's RStudio tutorials has great instructions on getting started with RStudio, including computations and authoring.\nHere is what you’ll need to do to set up and use RStudio with Quarto.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#setup",
    "href": "quarto-workflows/rstudio.html#setup",
    "title": "From RStudio",
    "section": "Setup",
    "text": "Setup\n\nRStudio and GitHub\nFor a workflow with RStudio and GitHub on your local computer, you will need four things:\n\nR\nRStudio\nGit\nGitHub\n\nFollow the UCSB MEDS Installation Guide for detailed instructions on how to create accounts, download, install, and configure on Mac and Windows. This takes about 20 minutes. (For an even more detailed walk-through, see Allison Horst’s ESM 206 Google Doc).\n\n\nClone your repo\nYou’ll start by cloning your repository into RStudio.\nFile &gt; New Project &gt; Version Control &gt; Git &gt; paste your repository name.\nR for Excel Users: Clone your repository using RStudio has detailed instructions and screenshots of these steps.\n\n\nInstall Quarto\nNext, you’ll install Quarto: https://quarto.org/docs/get-started/. After downloading, follow the installation wizard on your computer. When it is complete, you won’t see an application or any new software, but it is now available to RStudio (as well as all other applications on your computer, including the command line).\n\n\nRStudio orientation\nNow let’s take a moment to get oriented. This is an RStudio project, which is indicated in the top-right. The bottom right pane shows all the files in your project; everything we’ve cloned from GitHub. We can open any RStudio project by opening its .Rproj file, or from RStudio File &gt; Open Project ….\n\n\n\nRStudio IDE highlighting the project name and files pane\n\n\n\n\nVisual Editor\nThe RStudio Visual Editor is quite new and has features that improve your writing experience. Working in the Visual Editor feels a bit like working in a Google Doc.\nHere’s an example showing the same file in the original Source Editor with content in markdown format and in the Visual Editor with content that looks more like it will appear in a live site. You can switch freely between these modes.\n\n\n\n\n\n\nRStudio IDE highlighting the Source Editor\n\n\n\n\n\n\n\nRStudio IDE highlighting the Visual Editor\n\n\n\n\n\nAlready have some content formatted in a Google Doc? You can copy-paste it into the Visual Editor and most formatting will be retained.\nThe editing bar provides familiar point and click access to text formatting options like bulleted or numbered lists.\n\n\n\nRStudio IDE highlighting the point and click editing bar\n\n\n\nKeyboard shortcuts\nThe Visual Editor also lets you use many keyboard shortcuts that might be familiar for adding boldface (command-b), italics (command-i), or headers. On a Mac, option-command-2 will make a level 2 header. Try it with option-command-1, or option-command-0 for normal text!\n\n\nInsert an image or figure\nTo insert an image (called a figure in Quarto), click the image icon. This brings up a window in which we can select the image, set its alignment, give it a caption and alt text, hyperlink it, or edit other metadata.\n\n\n\nInsert image or figure using the Visual Editor\n\n\nOnce an image is added, clicking on that image gives us editing options. We can resize it dynamically by clicking in the image and dragging a corner or side to resize. When an image is selected, its dimensions are displayed for editing. Clicking on the gray ellipsis to the right of the dimensions opens the pop-up window to access more metadata edits.\n\n\nInsert a table\nSimilar to adding an image, to insert a table, we click the Table dropdown.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#quarto-render",
    "href": "quarto-workflows/rstudio.html#quarto-render",
    "title": "From RStudio",
    "section": "Quarto render",
    "text": "Quarto render\nIn the Build tab in the top-right pane, click “Render Website”. This will build the .html files and preview your website. It’s equivalent to “knitting” in RMarkdown.\nNote that you can also click “Preview Website”. With “Render Website” in RStudio, Quarto is able to render and preview in one step.\nIf you’d ever like to stop the preview, in the bottom-left, click on the Jobs tab and then the red Stop button.\n\nMake a small change and render it\nClick on index.md. This will open this markdown file in a fourth pane; the editor pane. Make a small change, for example change to today’s date on Line 4. Then, save your file; there is a disc icon at the top of the file.\nThen, render this file: press “Render” which is to the right of the disc icon that saves the file. This will render only this single file, as opposed to rerendering the whole website like when we clicked “Render Website” in the top right pane. Checking Render on Save (between the disc icon and the Render button) is a great strategy for doing this in one step.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#create-a-new-.rmd-page",
    "href": "quarto-workflows/rstudio.html#create-a-new-.rmd-page",
    "title": "From RStudio",
    "section": "Create a new .Rmd page",
    "text": "Create a new .Rmd page\nNew &gt; RMarkdown document &gt; OK\nThe starter RMarkdown document has some R code inside: it displays a summary of the cars dataset that is pre-loaded into R (summary(cars)) and plots the pressure data that is also pre-loaded (plot(pressure)).\nSave this document as r-example.rmd.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#update-_quarto.yml",
    "href": "quarto-workflows/rstudio.html#update-_quarto.yml",
    "title": "From RStudio",
    "section": "Update _quarto.yml",
    "text": "Update _quarto.yml\nNow we’ll add r-example.rmd to our _quarto.yml file; this is where we register all files to include in our site. Let’s add it after the section called “Quarto Workflows”.\nOpen _quarto.yml by clicking on it from the file directory.\nScroll down to review the current contents in the sidebar: section under contents:. It’s there we see all the file arrangement that we see in the previewed site.\nAdd - r-example.rmd in its own line, making sure that your indentation aligns with the other pages.\nFrom the Build tab, clicking Preview Website will recreate your website!",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#authoring-tips",
    "href": "quarto-workflows/rstudio.html#authoring-tips",
    "title": "From RStudio",
    "section": "Authoring tips",
    "text": "Authoring tips\nChecking “Render on Save” is really helpful when iterating quickly on a document.\nIf the document is very code-heavy, consider using freeze that will not run the code each time.\nQuarto.org has details about authoring, including specific instructions about authoring in RStudio.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#commit-and-push",
    "href": "quarto-workflows/rstudio.html#commit-and-push",
    "title": "From RStudio",
    "section": "Commit and push!",
    "text": "Commit and push!\nCommitting and pushing will make the changes you see locally live on your website (using the GitHub Action we set up earlier).",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "quarto-workflows/rstudio.html#troubleshooting",
    "href": "quarto-workflows/rstudio.html#troubleshooting",
    "title": "From RStudio",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf you have trouble rendering your website after for example changing the extenstion of a file from .md to .qmd, refreshing your RStudio often helps. Do this by clicking the project name at the upper right of the RStudio window (in this case, quarto-website-tutorial), and underneath the “close project” section, click the same name of your project: quarto-website-tutorial. This will relaunch your whole project afresh.",
    "crumbs": [
      "Quarto workflows",
      "From RStudio"
    ]
  },
  {
    "objectID": "statistic/MonteCarlo_Bootstrap.html",
    "href": "statistic/MonteCarlo_Bootstrap.html",
    "title": "So sánh giữa Monte Carlo Simulation và Bootstrap",
    "section": "",
    "text": "Phương pháp Monte Carlo Simulation xuất hiện đầu tiên vào cuối thập kỷ 1940, trong quá trình phát triển và nghiên cứu về vấn đề hạt nhân trong thời kỳ Chiến tranh thế giới thứ hai. Dự án Manhattan, một dự án nghiên cứu quy mô lớn do Mỹ thực hiện để phát triển bom nguyên tử, đặt ra nhiều thách thức về tính toán với những biến số không chắc chắn.\nJohn von Neumann, một nhà toán học và nhà vật lý người Mỹ-Hungari, cùng với nhóm nghiên cứu của ông, đã phát triển phương pháp Monte Carlo Simulation để giải quyết những vấn đề này. Tên gọi “Monte Carlo” xuất phát từ tên của thành phố Monaco nổi tiếng với sòng bạc và các trò chơi may rủi, thể hiện tính ngẫu nhiên trong quá trình mô phỏng.\nPhương pháp Monte Carlo Simulation được sử dụng rộng rãi trong các lĩnh vực như vật lý, tài chính, kỹ thuật, và nhiều ngành khác nữa. Đến nay, nó vẫn là một công cụ quan trọng trong mô hình hóa và dự đoán các biến số không chắc chắn, đặc biệt là khi không có phương trình toán học chính xác để mô tả các hệ thống.\n\n\n\nPhương pháp Bootstrap được giới thiệu bởi Bradley Efron vào cuối thập kỷ 1970, đặc biệt là trong bài báo mang tiêu đề “Bootstrap Methods: Another Look at the Jackknife” (1979). Trong bài báo này, Efron đề xuất phương pháp Bootstrap như một phương pháp thay thế và mở rộng cho phương pháp Jackknife, một phương pháp thống kê truyền thống.\nNgữ cảnh lịch sử: - Trước khi Bootstrap xuất hiện, các phương pháp thống kê thường phải dựa vào giả định về phân phối của dữ liệu hoặc kích thước mẫu lớn để áp dụng các ước lượng thống kê. - Efron nhận ra rằng, thay vì phụ thuộc vào giả định phân phối, chúng ta có thể sử dụng dữ liệu hiện có để đánh giá phân phối của một ước lượng thống kê thông qua việc tái chọn mẫu từ dữ liệu.\nCơ sở Lý Thuyết: - Bootstrap dựa trên ý tưởng rằng mẫu mà chúng ta có thể thu thập từ dữ liệu mẫu hiện có có thể đưa ra thông tin tốt về biến động của ước lượng thống kê. - Thay vì giả định về phân phối, Bootstrap sử dụng dữ liệu để xác định phân phối thông tin của ước lượng.\nỨng Dụng Đầu Tiên: - Phương pháp Bootstrap nhanh chóng được áp dụng rộng rãi trong thống kê và các lĩnh vực khác. Ban đầu, nó thường được sử dụng để đánh giá độ chắc chắn của các ước lượng thống kê như trung bình và phương sai. - Bootstrap cũng đã mở ra một hướng mới trong nghiên cứu thống kê, giúp làm thay đổi cách chúng ta xem xét và xử lý dữ liệu.\nPhương pháp Bootstrap đã trở thành một công cụ mạnh mẽ trong công việc thống kê và nghiên cứu khoa học, giúp làm giảm sự phụ thuộc vào giả định về phân phối và đồng thời cung cấp cách tiếp cận linh hoạt hơn trong đối mặt với dữ liệu thực tế.",
    "crumbs": [
      "Statistic",
      "So sánh giữa Monte Carlo Simulation và Bootstrap"
    ]
  },
  {
    "objectID": "statistic/MonteCarlo_Bootstrap.html#lịch-sử-ra-đời",
    "href": "statistic/MonteCarlo_Bootstrap.html#lịch-sử-ra-đời",
    "title": "So sánh giữa Monte Carlo Simulation và Bootstrap",
    "section": "",
    "text": "Phương pháp Monte Carlo Simulation xuất hiện đầu tiên vào cuối thập kỷ 1940, trong quá trình phát triển và nghiên cứu về vấn đề hạt nhân trong thời kỳ Chiến tranh thế giới thứ hai. Dự án Manhattan, một dự án nghiên cứu quy mô lớn do Mỹ thực hiện để phát triển bom nguyên tử, đặt ra nhiều thách thức về tính toán với những biến số không chắc chắn.\nJohn von Neumann, một nhà toán học và nhà vật lý người Mỹ-Hungari, cùng với nhóm nghiên cứu của ông, đã phát triển phương pháp Monte Carlo Simulation để giải quyết những vấn đề này. Tên gọi “Monte Carlo” xuất phát từ tên của thành phố Monaco nổi tiếng với sòng bạc và các trò chơi may rủi, thể hiện tính ngẫu nhiên trong quá trình mô phỏng.\nPhương pháp Monte Carlo Simulation được sử dụng rộng rãi trong các lĩnh vực như vật lý, tài chính, kỹ thuật, và nhiều ngành khác nữa. Đến nay, nó vẫn là một công cụ quan trọng trong mô hình hóa và dự đoán các biến số không chắc chắn, đặc biệt là khi không có phương trình toán học chính xác để mô tả các hệ thống.\n\n\n\nPhương pháp Bootstrap được giới thiệu bởi Bradley Efron vào cuối thập kỷ 1970, đặc biệt là trong bài báo mang tiêu đề “Bootstrap Methods: Another Look at the Jackknife” (1979). Trong bài báo này, Efron đề xuất phương pháp Bootstrap như một phương pháp thay thế và mở rộng cho phương pháp Jackknife, một phương pháp thống kê truyền thống.\nNgữ cảnh lịch sử: - Trước khi Bootstrap xuất hiện, các phương pháp thống kê thường phải dựa vào giả định về phân phối của dữ liệu hoặc kích thước mẫu lớn để áp dụng các ước lượng thống kê. - Efron nhận ra rằng, thay vì phụ thuộc vào giả định phân phối, chúng ta có thể sử dụng dữ liệu hiện có để đánh giá phân phối của một ước lượng thống kê thông qua việc tái chọn mẫu từ dữ liệu.\nCơ sở Lý Thuyết: - Bootstrap dựa trên ý tưởng rằng mẫu mà chúng ta có thể thu thập từ dữ liệu mẫu hiện có có thể đưa ra thông tin tốt về biến động của ước lượng thống kê. - Thay vì giả định về phân phối, Bootstrap sử dụng dữ liệu để xác định phân phối thông tin của ước lượng.\nỨng Dụng Đầu Tiên: - Phương pháp Bootstrap nhanh chóng được áp dụng rộng rãi trong thống kê và các lĩnh vực khác. Ban đầu, nó thường được sử dụng để đánh giá độ chắc chắn của các ước lượng thống kê như trung bình và phương sai. - Bootstrap cũng đã mở ra một hướng mới trong nghiên cứu thống kê, giúp làm thay đổi cách chúng ta xem xét và xử lý dữ liệu.\nPhương pháp Bootstrap đã trở thành một công cụ mạnh mẽ trong công việc thống kê và nghiên cứu khoa học, giúp làm giảm sự phụ thuộc vào giả định về phân phối và đồng thời cung cấp cách tiếp cận linh hoạt hơn trong đối mặt với dữ liệu thực tế.",
    "crumbs": [
      "Statistic",
      "So sánh giữa Monte Carlo Simulation và Bootstrap"
    ]
  },
  {
    "objectID": "statistic/MonteCarlo_Bootstrap.html#so-sánh-monte-carlo-simulation-và-bootstrap",
    "href": "statistic/MonteCarlo_Bootstrap.html#so-sánh-monte-carlo-simulation-và-bootstrap",
    "title": "So sánh giữa Monte Carlo Simulation và Bootstrap",
    "section": "So sánh Monte Carlo Simulation: và Bootstrap:",
    "text": "So sánh Monte Carlo Simulation: và Bootstrap:\n\nMục Tiêu Chính:\n\nMonte Carlo Simulation: Được sử dụng để mô phỏng hành vi của một hệ thống phức tạp thông qua việc tạo ra dữ liệu ngẫu nhiên và thực hiện phân tích.\nBootstrap: Sử dụng để đánh giá độ chắc chắn của ước lượng thống kê và xây dựng khoảng tin cậy thông qua việc tái chọn mẫu từ dữ liệu đã có.\n\nLoại Ứng Dụng:\n\nMonte Carlo Simulation: Thường được sử dụng trong mô hình hóa hệ thống phức tạp như tài chính, vật lý, và kỹ thuật.\nBootstrap: Thường được sử dụng trong thống kê để đánh giá độ biến động của ước lượng thống kê.\n\nDữ Liệu Sử Dụng:\n\nMonte Carlo Simulation: Thường sử dụng để tạo ra dữ liệu mới dựa trên mô hình toán học hoặc cảm nhận về hệ thống.\nBootstrap: Sử dụng dữ liệu hiện có và tái chọn mẫu từ nó để ước tính phân phối của một thống kê.\n\nMục Tiêu Ước Tính:\n\nMonte Carlo Simulation: Đưa ra ước lượng về hành vi của một biến số hay hệ thống thông qua việc lặp lại mô phỏng.\nBootstrap: Đưa ra ước lượng của phân phối của một thống kê cụ thể, chẳng hạn như trung bình hoặc phương sai.\n\nPhương Pháp Tạo Ngẫu Nhiên:\n\nMonte Carlo Simulation: Sử dụng phương pháp tạo số ngẫu nhiên để mô phỏng dữ liệu mới.\nBootstrap: Sử dụng phương pháp tái chọn mẫu từ dữ liệu hiện có để tạo ra các tập dữ liệu con.\n\nỨng Dụng Cụ Thể:\n\nMonte Carlo Simulation: Có thể được sử dụng để mô phỏng giá trị của tùy chọn tài chính, đánh giá rủi ro dự án, hoặc mô phỏng quá trình vật lý.\nBootstrap: Thường được sử dụng để đánh giá độ chắc chắn của ước lượng thống kê như trung bình, phương sai, hoặc hệ số tương quan.\n\nMức Độ Phức Tạp:\n\nMonte Carlo Simulation: Thường phức tạp hơn, đặc biệt là khi mô phỏng hệ thống phức tạp với nhiều yếu tố.\nBootstrap: Tương đối đơn giản và dễ triển khai.\n\nĐối Tượng Nghiên Cứu:\n\nMonte Carlo Simulation: Thường được ưa chuộng trong nghiên cứu lĩnh vực khoa học và kỹ thuật.\nBootstrap: Thường được sử dụng trong thống kê và nghiên cứu xã hội.\n\n\nTóm lại, Monte Carlo Simulation và Bootstrap là hai phương pháp mạnh mẽ được sử dụng trong các lĩnh vực khác nhau với mục tiêu và ứng dụng riêng biệt. Monte Carlo Simulation thường được sử dụng để mô phỏng và dự đoán hành vi của hệ thống phức tạp, trong khi Bootstrap tập trung vào đánh giá độ chắc chắn của ước lượng thống kê từ dữ liệu hiện có.",
    "crumbs": [
      "Statistic",
      "So sánh giữa Monte Carlo Simulation và Bootstrap"
    ]
  },
  {
    "objectID": "statistic/MonteCarlo_Bootstrap.html#ví-dụ-về-áp-dụng-phương-pháp-monte-carlo-simulation",
    "href": "statistic/MonteCarlo_Bootstrap.html#ví-dụ-về-áp-dụng-phương-pháp-monte-carlo-simulation",
    "title": "So sánh giữa Monte Carlo Simulation và Bootstrap",
    "section": "Ví dụ về áp dụng phương pháp Monte Carlo Simulation",
    "text": "Ví dụ về áp dụng phương pháp Monte Carlo Simulation\n\nVí dụ: Ước tính giá trị Pi\nGiả sử chúng ta muốn ước tính giá trị Pi bằng phương pháp Monte Carlo Simulation. Ý tưởng là vẽ một hình vuông có cạnh dài \\(2r\\) và nằm trong một hình tròn có bán kính \\(r\\). Nếu ta ngẫu nhiên chọn một điểm bất kỳ bên trong hình vuông, xác suất nó rơi vào hình tròn sẽ tỉ lệ với diện tích của hình tròn so với hình vuông.\n\nBước 1: Tạo điểm ngẫu nhiên:\n\nTạo một số lượng lớn \\(N\\) điểm ngẫu nhiên trong hình vuông.\n\nBước 2: Kiểm tra vị trí của điểm:\n\nĐối với mỗi điểm, kiểm tra xem nó có nằm trong hình tròn hay không bằng cách kiểm tra \\(x^2 + y^2 \\leq r^2\\).\n\nBước 3: Ước tính giá trị Pi:\n\nSử dụng tỉ lệ giữa số điểm nằm trong hình tròn và tổng số điểm để ước tính giá trị Pi: \\(\\pi \\approx \\frac{\\text{Số điểm trong hình tròn}}{\\text{Tổng số điểm}} \\times 4\\).\n\n\nDưới đây là một đoạn mã Python đơn giản để thực hiện ví dụ này:\n\nimport random\n\ndef estimate_pi(num_points):\n    points_inside_circle = 0\n\n    for _ in range(num_points):\n        x = random.uniform(-1, 1)\n        y = random.uniform(-1, 1)\n\n        if x**2 + y**2 &lt;= 1:\n            points_inside_circle += 1\n\n    pi_estimate = (points_inside_circle / num_points) * 4\n    return pi_estimate\n\n# Thử nghiệm với 1 triệu điểm\nnum_points = 1000000\npi_approximation = estimate_pi(num_points)\n\nprint(f\"Giá trị Pi ước tính với {num_points} điểm: {pi_approximation}\")\n\nGiá trị Pi ước tính với 1000000 điểm: 3.144\n\n\n\n\nVí dụ: Định giá tùy chọn (Option Pricing) trong Tài chính\nMonte Carlo Simulation được sử dụng rộng rãi trong tài chính để định giá các tùy chọn (options). Giả sử bạn muốn ước tính giá của một tùy chọn chứng khoán dựa trên mô hình Black-Scholes, một mô hình phổ biến trong tài chính.\n\nBước 1: Xác định các tham số của mô hình:\n\nSố liệu như giá chứng khoán hiện tại (\\(S\\)), giá thực hiện (\\(K\\)), thời gian đến hết hạn (\\(T\\)), biến động phần trăm hàng năm (\\(\\sigma\\)), và lãi suất không rủi ro (\\(r\\)).\n\nBước 2: Tạo ngẫu nhiên các biến đầu vào:\n\nSử dụng Monte Carlo để tạo ra một lượng lớn các biến ngẫu nhiên, chẳng hạn như biến động giá (\\(\\Delta S\\)) và thời gian đến hết hạn (\\(\\Delta t\\)).\n\nBước 3: Mô phỏng giá chứng khoán tương lai:\n\nSử dụng các biến ngẫu nhiên để mô phỏng giá chứng khoán tương lai theo mô hình Black-Scholes.\n\nBước 4: Định giá tùy chọn:\n\nSử dụng giá chứng khoán tương lai để định giá tùy chọn dựa trên điều kiện thị trường.\n\n\nDưới đây là một đoạn mã Python đơn giản để thực hiện ví dụ này: Trong ví dụ này, chúng ta sử dụng Monte Carlo để mô phỏng giá chứng khoán tương lai và sau đó định giá một tùy chọn dựa trên mô hình Black-Scholes. Monte Carlo giúp xác định phân phối xác suất của giá chứng khoán tương lai và từ đó đưa ra giá trị hiện tại của tùy chọn.\n\nimport numpy as np\nimport math\n\ndef black_scholes_simulation(S, K, T, r, sigma, num_simulations):\n    dt = T / 252  # Assume 252 trading days in a year\n    simulations = np.zeros(num_simulations)\n\n    for i in range(num_simulations):\n        path = [S]\n        for _ in range(252):\n            Z = np.random.normal(0, 1)\n            S_t = path[-1] * math.exp((r - 0.5 * sigma**2) * dt + sigma * math.sqrt(dt) * Z)\n            path.append(S_t)\n\n        simulations[i] = max(0, math.exp(-r * T) * (np.mean(path) - K))\n\n    option_price = np.mean(simulations)\n    return option_price\n\n# Thử nghiệm với các tham số\nstock_price = 100\nstrike_price = 100\nexpiry = 1  # 1 year\nrisk_free_rate = 0.05\nvolatility = 0.2\nnum_simulations = 100000\n\noption_price = black_scholes_simulation(stock_price, strike_price, expiry, risk_free_rate, volatility, num_simulations)\n\nprint(f\"Giá tùy chọn ước tính: {option_price}\")\n\nGiá tùy chọn ước tính: 5.74496815639896",
    "crumbs": [
      "Statistic",
      "So sánh giữa Monte Carlo Simulation và Bootstrap"
    ]
  },
  {
    "objectID": "statistic/MonteCarlo_Bootstrap.html#ví-dụ-sử-dụng-phương-pháp-bootstrap",
    "href": "statistic/MonteCarlo_Bootstrap.html#ví-dụ-sử-dụng-phương-pháp-bootstrap",
    "title": "So sánh giữa Monte Carlo Simulation và Bootstrap",
    "section": "Ví dụ Sử Dụng Phương Pháp Bootstrap",
    "text": "Ví dụ Sử Dụng Phương Pháp Bootstrap\nVí dụ Sử Dụng Phương Pháp Bootstrap để Ước Tính Trung Bình:\nGiả sử bạn có một tập dữ liệu đo chiều cao của một mẫu người và bạn muốn ước tính trung bình chiều cao của toàn bộ dân số. Thay vì dựa vào các giả định phân phối, bạn có thể sử dụng phương pháp Bootstrap để đánh giá độ chắc chắn của ước lượng trung bình.\nTrong ví dụ dưới, chúng ta sử dụng Bootstrap để tạo ra nhiều mẫu tái chọn từ dữ liệu chiều cao của mẫu người và tính toán trung bình của mỗi mẫu tái chọn. Kết quả là một phân phối mẫu trung bình được sử dụng để ước lượng giá trị trung bình và xây dựng khoảng tin cậy 95%.\nBootstrap giúp chúng ta đánh giá độ chắc chắn của ước lượng trung bình mà không cần phải làm giả định về phân phối của dữ liệu.\n\n\nimport numpy as np\n\n# Tạo một tập dữ liệu mẫu (giả sử đây là chiều cao của một mẫu người)\nnp.random.seed(42)\nsample_data = np.random.normal(loc=170, scale=5, size=100)\n\n# Hàm Bootstrap để ước lượng trung bình\ndef bootstrap_mean(data, num_samples=1000):\n    sample_means = np.zeros(num_samples)\n    \n    for i in range(num_samples):\n        bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n        sample_means[i] = np.mean(bootstrap_sample)\n    \n    return sample_means\n\n# Áp dụng Bootstrap để ước lượng trung bình và đánh giá độ chắc chắn\nbootstrap_means = bootstrap_mean(sample_data)\n\n# Tính toán khoảng tin cậy 95%\nconfidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n\n# Ước lượng trung bình thực sự từ dữ liệu mẫu\ntrue_mean = np.mean(sample_data)\n\nprint(f\"Trung bình ước lượng từ Bootstrap: {np.mean(bootstrap_means)}\")\nprint(f\"Khoảng tin cậy 95%: {confidence_interval}\")\nprint(f\"Trung bình thực sự từ dữ liệu mẫu: {true_mean}\")\n\n\n\nTrung bình ước lượng từ Bootstrap: 169.48841800757592\nKhoảng tin cậy 95%: [168.63831788 170.31425743]\nTrung bình thực sự từ dữ liệu mẫu: 169.48076741302958",
    "crumbs": [
      "Statistic",
      "So sánh giữa Monte Carlo Simulation và Bootstrap"
    ]
  },
  {
    "objectID": "statistic/Mixture_distribution.html",
    "href": "statistic/Mixture_distribution.html",
    "title": "Mixture distribution",
    "section": "",
    "text": "Định nghĩa: Hàm mật độ xác suất, ký hiệu thường là $ f(x) $, mô tả xác suất tương đối của một biến ngẫu nhiên liên tục tại một giá trị cụ thể. Nó không cho biết xác suất chính xác tại một điểm (vì với biến liên tục, xác suất tại một điểm bằng 0), mà cho biết “mật độ” xác suất trong một khoảng giá trị.\nÝ nghĩa: Xác suất để biến ngẫu nhiên $ X $ nằm trong khoảng \\([a, b]\\) được tính bằng tích phân của hàm mật độ xác suất trên khoảng đó:\n\\[ P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx \\]\nĐặc điểm:\n\n$ f(x) $ cho mọi $ x $.\nTổng tích phân trên toàn bộ miền xác định của $ f(x) $ bằng 1: $ _{-}^{} f(x) , dx = 1 $.\n\nVí dụ: Với phân phối chuẩn (Gaussian), hàm mật độ xác suất có dạng: \\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\] trong đó $ $ là giá trị trung bình, $ $ là độ lệch chuẩn.\n\n\n\n\n\nĐịnh nghĩa: Hàm phân phối xác suất, ký hiệu là $ F(x) $, cho biết xác suất để biến ngẫu nhiên $ X $ nhận giá trị nhỏ hơn hoặc bằng một giá trị cụ thể $ x \\(:\\)$ F(x) = P(X x) \\[\nVới biến ngẫu nhiên liên tục, $ F(x) $ là tích phân của hàm mật độ xác suất từ âm vô cực đến $ x $:\n\\] F(x) = _{-}^x f(t) , dt $$\nÝ nghĩa: Hàm $ F(x) $ cung cấp thông tin về xác suất tích lũy của biến ngẫu nhiên, thể hiện cách xác suất được phân phối trên các giá trị của $ X $.\nĐặc điểm:\n\n$ F(x) $ là hàm không giảm.\n$ {x -} F(x) = 0 $ và $ {x } F(x) = 1 $.\nNếu $ F(x) $ khả vi, thì đạo hàm của nó là hàm mật độ xác suất: $ F’(x) = f(x) $.\n\nVí dụ: Với phân phối chuẩn, hàm phân phối xác suất không có công thức đóng, nhưng được biểu diễn qua tích phân hoặc bảng tra cứu.\n\n\n\n\n\nHàm mật độ xác suất $ f(x) $ là đạo hàm của hàm phân phối xác suất $ F(x) $ (nếu $ F(x) $ khả vi).\nHàm phân phối xác suất $ F(x) $ là tích phân của hàm mật độ xác suất $ f(x) $.\n\n\n\n\n\nHàm mật độ xác suất (PDF): Mô tả “mật độ” xác suất tại một điểm, dùng để tính xác suất trong một khoảng.\nHàm phân phối xác suất (CDF): Mô tả xác suất tích lũy, cho biết xác suất để biến ngẫu nhiên nhỏ hơn hoặc bằng một giá trị.",
    "crumbs": [
      "Statistic",
      "Mixture distribution"
    ]
  },
  {
    "objectID": "statistic/Mixture_distribution.html#i.-khái-niệm-hàm-mật-độ-xác-suất-và-phân-phối-xác-suất",
    "href": "statistic/Mixture_distribution.html#i.-khái-niệm-hàm-mật-độ-xác-suất-và-phân-phối-xác-suất",
    "title": "Mixture distribution",
    "section": "",
    "text": "Định nghĩa: Hàm mật độ xác suất, ký hiệu thường là $ f(x) $, mô tả xác suất tương đối của một biến ngẫu nhiên liên tục tại một giá trị cụ thể. Nó không cho biết xác suất chính xác tại một điểm (vì với biến liên tục, xác suất tại một điểm bằng 0), mà cho biết “mật độ” xác suất trong một khoảng giá trị.\nÝ nghĩa: Xác suất để biến ngẫu nhiên $ X $ nằm trong khoảng \\([a, b]\\) được tính bằng tích phân của hàm mật độ xác suất trên khoảng đó:\n\\[ P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx \\]\nĐặc điểm:\n\n$ f(x) $ cho mọi $ x $.\nTổng tích phân trên toàn bộ miền xác định của $ f(x) $ bằng 1: $ _{-}^{} f(x) , dx = 1 $.\n\nVí dụ: Với phân phối chuẩn (Gaussian), hàm mật độ xác suất có dạng: \\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\] trong đó $ $ là giá trị trung bình, $ $ là độ lệch chuẩn.\n\n\n\n\n\nĐịnh nghĩa: Hàm phân phối xác suất, ký hiệu là $ F(x) $, cho biết xác suất để biến ngẫu nhiên $ X $ nhận giá trị nhỏ hơn hoặc bằng một giá trị cụ thể $ x \\(:\\)$ F(x) = P(X x) \\[\nVới biến ngẫu nhiên liên tục, $ F(x) $ là tích phân của hàm mật độ xác suất từ âm vô cực đến $ x $:\n\\] F(x) = _{-}^x f(t) , dt $$\nÝ nghĩa: Hàm $ F(x) $ cung cấp thông tin về xác suất tích lũy của biến ngẫu nhiên, thể hiện cách xác suất được phân phối trên các giá trị của $ X $.\nĐặc điểm:\n\n$ F(x) $ là hàm không giảm.\n$ {x -} F(x) = 0 $ và $ {x } F(x) = 1 $.\nNếu $ F(x) $ khả vi, thì đạo hàm của nó là hàm mật độ xác suất: $ F’(x) = f(x) $.\n\nVí dụ: Với phân phối chuẩn, hàm phân phối xác suất không có công thức đóng, nhưng được biểu diễn qua tích phân hoặc bảng tra cứu.\n\n\n\n\n\nHàm mật độ xác suất $ f(x) $ là đạo hàm của hàm phân phối xác suất $ F(x) $ (nếu $ F(x) $ khả vi).\nHàm phân phối xác suất $ F(x) $ là tích phân của hàm mật độ xác suất $ f(x) $.\n\n\n\n\n\nHàm mật độ xác suất (PDF): Mô tả “mật độ” xác suất tại một điểm, dùng để tính xác suất trong một khoảng.\nHàm phân phối xác suất (CDF): Mô tả xác suất tích lũy, cho biết xác suất để biến ngẫu nhiên nhỏ hơn hoặc bằng một giá trị.",
    "crumbs": [
      "Statistic",
      "Mixture distribution"
    ]
  },
  {
    "objectID": "statistic/Mixture_distribution.html#ii.-lựa-chọn-phân-phối",
    "href": "statistic/Mixture_distribution.html#ii.-lựa-chọn-phân-phối",
    "title": "Mixture distribution",
    "section": "II. Lựa chọn phân phối",
    "text": "II. Lựa chọn phân phối\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Dữ liệu đầu vào (giá trị x và f(x) dưới dạng %)\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n              21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n              39, 40, 41, 42, 43, 44, 45, 46, 47])\nfx = np.array([0.000038, 0.000067, 0.000136, 0.017609, 0.010485, 0.008596, 0.007793,\n               0.007277, 0.006820, 0.006466, 0.006208, 0.005735, 0.005297, 0.004883,\n               0.004514, 0.004133, 0.003786, 0.003436, 0.003127, 0.002820, 0.002527,\n               0.002262, 0.002005, 0.001804, 0.001616, 0.001447, 0.001285, 0.001141,\n               0.001015, 0.000897, 0.000805, 0.000714, 0.000633, 0.000560, 0.000497,\n               0.000438, 0.000363, 0.000294, 0.000226, 0.000155, 0.000078, 0.000050,\n               0.000028, 0.000015, 0.000006, 0.000003, 0.000001])  # Đơn vị %\n\n# Vẽ đồ thị\nplt.figure(figsize=(10, 6))\nplt.plot(x, fx, 'bo-', label='Raw Data PDF')\nplt.title('Raw Data Distribution')\nplt.xlabel('x')\nplt.ylabel('Probability Density (%)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nVới đồ thị ở trên, ta quan sát thấy một phân phối có hình dạng đặc trưng với: - Một đỉnh cao ở gần giá trị nhỏ (khoảng 1-3), sau đó giảm dần theo dạng đuôi dài về phía các giá trị lớn hơn (lên đến 47). - Trục tung có đơn vị phần trăm (% hoặc 0.00% đến 2.00%).\nDựa trên hình dạng này, ta cần chọn hai phân phối để kết hợp thành một phân phối hỗn hợp (mixture distribution) sao cho mô hình hóa tốt hành vi của dữ liệu. Dưới đây là phân tích và cách tiếp cận chi tiết:\n\n\n1. Phân tích hình dạng đồ thị và lựa chọn phân phối\n\nPhần đỉnh cao (gần 0-3): Phân phối có mật độ cao ở giá trị nhỏ, giảm nhanh sau đó, gợi ý một phân phối tập trung ở đầu (left-skewed hoặc có đỉnh rõ rệt).\nĐuôi dài (3-47): Phân phối giảm dần nhưng vẫn có giá trị đáng kể ở các giá trị lớn, cho thấy một đuôi phải (right-skewed) với khả năng mô hình hóa bằng phân phối có đuôi dài.\n\n\nLựa chọn phân phối:\n\nPhân phối Exponenial (mũ) hoặc Gamma cho phần đỉnh cao:\n\nPhân phối Exponenial phù hợp với dữ liệu giảm theo hàm mũ sau một điểm khởi đầu, thường xuất hiện trong các mô hình thời gian chờ hoặc sự kiện.\nPhân phối Gamma là lựa chọn linh hoạt hơn, có thể mô hình hóa dữ liệu với đỉnh và giảm dần, đặc biệt khi dữ liệu có biến thiên lớn.\nVì đồ thị giảm nhanh nhưng không hoàn toàn theo hàm mũ thuần túy (có xu hướng chậm lại ở đuôi), Gamma là lựa chọn hợp lý hơn.\n\nPhân phối Gamma hoặc Lognormal cho đuôi dài:\n\nPhân phối Gamma tiếp tục phù hợp vì nó có thể mô hình hóa cả đỉnh và đuôi dài với tham số hình dạng $ k $ và tỷ lệ $ $.\nPhân phối Lognormal cũng có đuôi phải dài, thường dùng cho dữ liệu tài chính hoặc thời gian, nhưng vì Gamma đã đủ linh hoạt và có thể điều chỉnh để phù hợp cả đỉnh lẫn đuôi, ta ưu tiên Gamma cho cả hai phần với các tham số khác nhau.\n\n\n\n\nKết luận về phân phối:\n\nKết hợp hai phân phối Gamma với các tham số khác nhau:\n\nMột Gamma với $ k $ nhỏ (hình dạng thấp) và $ $ lớn để mô hình hóa phần đỉnh cao.\nMột Gamma với $ k $ lớn hơn và $ $ nhỏ hơn để mô hình hóa phần đuôi dài.\n\nLý do: Gamma là phân phối linh hoạt, có thể điều chỉnh để khớp với cả đỉnh cao và đuôi dài mà không cần chuyển sang phân phối khác phức tạp hơn.\n\n\n\n\n\n2. Lý do chọn hai phân phối Gamma\n\nHình dạng khớp: Đồ thị cho thấy một đỉnh rõ rệt ở đầu và giảm dần, điều này phù hợp với đặc trưng của Gamma, đặc biệt khi thay đổi tham số $ k $ (hình dạng) và $ $ (tỷ lệ).\nLiên tục: Sử dụng hai Gamma cho phép tạo ra một phân phối hỗn hợp mượt mà, dễ dàng điều chỉnh tại điểm chuyển tiếp (ví dụ: khoảng 5-10).\nSo với Beta (trên đoạn hữu hạn) hoặc Exponenial (giảm nhanh quá mức), Gamma cung cấp sự linh hoạt hơn cho cả đỉnh và đuôi.\n\n\n\n\n3. Cách tiếp cận chi tiết\nĐể xây dựng phân phối hỗn hợp từ hai phân phối Gamma, ta thực hiện các bước sau:\n\nBước 1: Định nghĩa hai phân phối Gamma\n\nGamma 1 (phần đỉnh cao):\n\nTham số: $ k_1 $ nhỏ (ví dụ: $ k_1 = 1 $ hoặc $ 2 $) để tạo đỉnh cao ở đầu.\nTham số: $ _1 $ lớn (ví dụ: $ _1 = 5 $) để kiểm soát tốc độ giảm.\nMiền: $ x $ từ 0 đến một điểm chuyển tiếp (giả sử $ x_c $ hoặc 10).\nHàm mật độ xác suất (PDF): \\[\nf_1(x) = \\frac{1}{\\Gamma(k_1) \\theta_1^{k_1}} x^{k_1-1} e^{-x/\\theta_1}, \\quad 0 &lt; x &lt; x_c\n\\]\nHàm phân phối xác suất (CDF): \\[\nF_1(x) = \\frac{\\gamma(k_1, x/\\theta_1)}{\\Gamma(k_1)}\n\\]\n\nGamma 2 (phần đuôi dài):\n\nTham số: $ k_2 $ lớn hơn (ví dụ: $ k_2 = 3 $ hoặc $ 4 $) để tạo đuôi dài.\nTham số: $ _2 $ nhỏ hơn (ví dụ: $ _2 = 2 $) để kiểm soát độ dốc.\nMiền: $ x $ từ $ x_c $ đến $ $.\nHàm mật độ xác suất (PDF): \\[\nf_2(x) = \\frac{1}{\\Gamma(k_2) \\theta_2^{k_2}} x^{k_2-1} e^{-x/\\theta_2}, \\quad x \\geq x_c\n\\]\nHàm phân phối xác suất (CDF) được điều chỉnh từ $ x_c \\(:\\)$ F_2(x) = , x x_c $$ (để chuẩn hóa sao cho $ F_2(x_c) = 0 $ và $ F_2() = 1 $ trong phần còn lại).\n\n\n\n\nBước 2: Xác định trọng số\n\nGiới thiệu trọng số $ p $ (0 &lt; $ p $ &lt; 1):\n\n$ p $: Tỷ lệ xác suất thuộc Gamma 1 (phần đỉnh cao).\n$ 1 - p $: Tỷ lệ xác suất thuộc Gamma 2 (phần đuôi).\n\nƯớc lượng ban đầu:\n\nQuan sát đồ thị: Đỉnh cao chiếm phần lớn (khoảng 70-80% diện tích dưới đường cong), nên chọn $ p $ đến $ 0.8 $.\nĐiều chỉnh $ p $ dựa trên dữ liệu hoặc thử nghiệm để khớp với tổng xác suất bằng 1.\n\n\n\n\nBước 3: Xây dựng CDF tổng quát\n\nCDF của phân phối hỗn hợp: \\[\nF_X(x) =\n\\begin{cases}\np \\cdot F_1(x), & 0 &lt; x \\leq x_c \\\\\np + (1 - p) \\cdot F_2(x), & x &gt; x_c\n\\end{cases}\n\\]\nTại $ x = x_c $:\n\n$ F_X(x_c^-) = p F_1(x_c) $\n$ F_X(x_c^+) = p + (1 - p) F_2(x_c) = p + (1 - p) = p $\nĐể liên tục, cần chọn $ F_1(x_c) = 1 $ (tức là Gamma 1 được chuẩn hóa trên $ [0, x_c] $), nhưng thường ta điều chỉnh $ x_c $ sao cho $ F_1(x_c) p $ và $ F_2(x_c) = 0 $.\n\n\n\n\nBước 4: Ước lượng tham số\n\nSử dụng phương pháp hợp lý tối đa (Maximum Likelihood) hoặc EM để ước lượng:\n\n$ p $, $ k_1 $, $ _1 $ (cho Gamma 1).\n$ k_2 $, $ _2 $ (cho Gamma 2).\n\nHoặc thử nghiệm các giá trị ban đầu (ví dụ: $ k_1 = 2 $, $ _1 = 5 $, $ k_2 = 4 $, $ _2 = 2 $, $ p = 0.8 $) và tinh chỉnh dựa trên độ khớp với đồ thị.\n\n\n\nBước 5: Kiểm tra và điều chỉnh\n\nSo sánh PDF hoặc CDF của mô hình với đồ thị.\nĐiều chỉnh $ x_c $, $ p $, $ k_i $, $ _i $ để khớp tốt hơn (ví dụ: di chuyển $ x_c $ đến 5 hoặc 10 dựa trên điểm giảm dốc rõ rệt).\n\n\n\n\n\n4. Ví dụ cụ thể\nGiả sử: - $ x_c = 5 $ - Gamma 1: $ k_1 = 2 $, $ _1 = 3 $ - Gamma 2: $ k_2 = 4 $, $ _2 = 1.5 $ - $ p = 0.75 $\nCDF: \\[\nF_X(x) =\n\\begin{cases}\n0.75 \\cdot \\frac{\\gamma(2, x/3)}{\\Gamma(2)}, & 0 &lt; x \\leq 5 \\\\\n0.75 + 0.25 \\cdot \\frac{\\gamma(4, x/1.5) - \\gamma(4, 5/1.5)}{\\Gamma(4) - \\gamma(4, 5/1.5)}, & x &gt; 5\n\\end{cases}\n\\]\n\nKiểm tra tại $ x = 5 $:\n\n$ F_1(5) = $ (tính gần đúng).\n$ F_X(5^-) = 0.75 $\n$ F_X(5^+) = 0.75 + 0.25 $ (cần điều chỉnh $ x_c $ hoặc $ p $ để khớp).\n\n\nĐiều chỉnh $ x_c $ hoặc tham số để đảm bảo liên tục.\n\n\n\n5. Kết luận\n\nPhân phối phù hợp: Hai phân phối Gamma.\nLý do: Khớp với đỉnh cao và đuôi dài, linh hoạt với tham số.\nCách tiếp cận: Xây dựng hỗn hợp với trọng số $ p $, ước lượng tham số bằng dữ liệu hoặc thử nghiệm, và điều chỉnh để liên tục.",
    "crumbs": [
      "Statistic",
      "Mixture distribution"
    ]
  },
  {
    "objectID": "statistic/Mixture_distribution.html#iii.-ước-lượng-tham-số",
    "href": "statistic/Mixture_distribution.html#iii.-ước-lượng-tham-số",
    "title": "Mixture distribution",
    "section": "III. Ước lượng tham số",
    "text": "III. Ước lượng tham số\nĐể ước lượng các tham số $ p $, $ k_1 $, $ _1 $, $ k_2 $, $ _2 $ cho mô hình hỗn hợp hai phân phối Gamma dựa trên dữ liệu đã cung cấp, ta sẽ sử dụng phương pháp Maximum Likelihood Estimation (MLE). Vì đây là một mô hình hỗn hợp, ta có thể áp dụng thuật toán Expectation-Maximization (EM) để tối ưu hóa các tham số. Dưới đây là code Python sử dụng thư viện scipy và numpy để thực hiện việc này.\n\nGiả định\n\nDữ liệu là hàm mật độ xác suất (PDF) hoặc tần suất tương đối, cần chuẩn hóa.\nMiền được chia thành hai phần: $ 1 x $ (Gamma 1) và $ x &gt; 10 $ (Gamma 2).\nSử dụng EM để ước lượng các tham số.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gamma\nfrom scipy.special import gamma as gamma_func\nfrom scipy.optimize import minimize\n\n# Dữ liệu đầu vào (giá trị x và f(x) dưới dạng %)\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n              21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n              39, 40, 41, 42, 43, 44, 45, 46, 47])\nfx = np.array([0.000038, 0.000067, 0.000136, 0.017609, 0.010485, 0.008596, 0.007793,\n               0.007277, 0.006820, 0.006466, 0.006208, 0.005735, 0.005297, 0.004883,\n               0.004514, 0.004133, 0.003786, 0.003436, 0.003127, 0.002820, 0.002527,\n               0.002262, 0.002005, 0.001804, 0.001616, 0.001447, 0.001285, 0.001141,\n               0.001015, 0.000897, 0.000805, 0.000714, 0.000633, 0.000560, 0.000497,\n               0.000438, 0.000363, 0.000294, 0.000226, 0.000155, 0.000078, 0.000050,\n               0.000028, 0.000015, 0.000006, 0.000003, 0.000001])  # Chuyển % thành phần thập phân\n\n# Chuẩn hóa dữ liệu để tổng bằng 1\nfx_normalized = fx / np.sum(fx)\n\n# Hàm PDF của mô hình hỗn hợp Gamma\ndef mixture_pdf(x, p, k1, theta1, k2, theta2):\n    pdf1 = p * gamma.pdf(x, a=k1, scale=theta1) if x &lt;= 10 else 0\n    pdf2 = (1 - p) * gamma.pdf(x, a=k2, scale=theta2) if x &gt; 10 else 0\n    return pdf1 + pdf2\n\n# Hàm log-likelihood cho mô hình hỗn hợp Gamma\ndef log_likelihood(params, x, fx):\n    p, k1, theta1, k2, theta2 = params\n    p = min(max(p, 0.01), 0.99)  # Giới hạn p trong [0.01, 0.99] để tránh lỗi\n    n = len(x)\n    \n    ll = 0\n    for i in range(n):\n        total_pdf = mixture_pdf(x[i], p, k1, theta1, k2, theta2)\n        if total_pdf &gt; 0:\n            ll += fx[i] * np.log(total_pdf)\n    \n    return -ll  # Minimization, nên trả về giá trị âm\n\n# Giá trị ban đầu cho các tham số\ninitial_params = [0.75, 2.0, 2.0, 4.0, 1.0]  # [p, k1, theta1, k2, theta2]\n\n# Tối ưu hóa sử dụng minimize\nresult = minimize(log_likelihood, initial_params, args=(x, fx_normalized),\n                  method='Nelder-Mead', bounds=[(0.01, 0.99), (0.1, None), (0.1, None), (0.1, None), (0.1, None)])\n\n# Lấy kết quả tối ưu\np_opt, k1_opt, theta1_opt, k2_opt, theta2_opt = result.x\n\n# In kết quả\nprint(f\"Optimized parameters:\")\nprint(f\"p = {p_opt:.4f}\")\nprint(f\"k1 = {k1_opt:.4f}\")\nprint(f\"theta1 = {theta1_opt:.4f}\")\nprint(f\"k2 = {k2_opt:.4f}\")\nprint(f\"theta2 = {theta2_opt:.4f}\")\n\n# Tính và in log-likelihood tối ưu\noptimal_ll = -result.fun\nprint(f\"Optimal log-likelihood: {optimal_ll:.4f}\")\n\nOptimized parameters:\np = 0.5019\nk1 = 9.4693\ntheta1 = 0.6694\nk2 = 8.6634\ntheta2 = 2.1580\nOptimal log-likelihood: -3.3578\n\n\n\n\nGiải thích code\n\nDữ liệu đầu vào:\n\nx: Mảng các giá trị từ 1 đến 47.\nfx: Mảng các giá trị mật độ (chuyển từ % thành phần thập phân).\nChuẩn hóa fx để tổng bằng 1.\n\nHàm log-likelihood:\n\nĐịnh nghĩa hàm log-likelihood cho mô hình hỗn hợp hai Gamma.\nSử dụng gamma.pdf từ scipy.stats để tính PDF của Gamma.\nPhân chia miền: Gamma 1 áp dụng cho $ x $, Gamma 2 cho $ x &gt; 10 $.\nTrả về giá trị âm để tối ưu hóa bằng phương pháp minimize.\n\nTối ưu hóa:\n\nSử dụng minimize với phương pháp Nelder-Mead (không yêu cầu đạo hàm).\nGiá trị ban đầu: $ p = 0.75 $, $ k_1 = 2 $, $ _1 = 2 $, $ k_2 = 4 $, $ _2 = 1 $.\nGiới hạn $ p $ trong $ [0.01, 0.99] $ để tránh lỗi số học.\n\nKết quả:\n\nIn các tham số tối ưu và log-likelihood để đánh giá độ phù hợp.\n\n\n\n\nLưu ý\n\nThời gian chạy: EM hoặc MLE có thể cần nhiều lần lặp để hội tụ, tùy thuộc vào dữ liệu và giá trị ban đầu.\nĐiều chỉnh: Nếu kết quả không khớp, thử thay đổi giá trị ban đầu hoặc phương pháp tối ưu (ví dụ: L-BFGS-B).\nKiểm tra: So sánh PDF mô hình với fx_normalized để đánh giá độ chính xác.\n\n\n\nKết quả dự kiến\nKhi chạy code, bạn sẽ nhận được các giá trị tối ưu (kết quả thực tế phụ thuộc vào hội tụ của thuật toán). Ví dụ: - $ p -0.8 $ (phù hợp với phần đỉnh chiếm phần lớn). - $ k_1, _1 $ sẽ điều chỉnh để khớp đỉnh tại $ x = 4 $. - $ k_2, _2 $ sẽ điều chỉnh để khớp đuôi dài.\n\n# Tạo dữ liệu để vẽ đồ thị\nx_smooth = np.linspace(1, 47, 500)\nfx_model = np.array([mixture_pdf(xi, p_opt, k1_opt, theta1_opt, k2_opt, theta2_opt) for xi in x_smooth])\n\n# Chuẩn hóa fx_model để so sánh với fx_normalized\nfx_model_normalized = fx_model / np.trapezoid(fx_model, x_smooth)\n\n# Vẽ đồ thị\nplt.figure(figsize=(10, 6))\nplt.plot(x, fx_normalized, 'bo-', label='Data (Normalized PDF)')\nplt.plot(x_smooth, fx_model_normalized, 'r-', label='Fitted Mixture Gamma PDF')\nplt.title('Comparison of Data and Fitted Mixture Gamma Distribution')\nplt.xlabel('x')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLưu ý\n\nDo đồ thị chưa khớp tốt, có thể thử thay đổi initial_params hoặc điều chỉnh giới hạn miền (ví dụ: thay $ x $ bằng $ x $ hoặc $ x $).",
    "crumbs": [
      "Statistic",
      "Mixture distribution"
    ]
  },
  {
    "objectID": "statistic/Closeness_Evaluation_Measure__CEM_.html",
    "href": "statistic/Closeness_Evaluation_Measure__CEM_.html",
    "title": "Closeness Evaluation Measure (CEM)",
    "section": "",
    "text": "Công thức Closeness Evaluation Measure (CEM) là một công cụ được thiết kế để đánh giá các hệ thống phân loại thứ tự (ordinal classification), đặc biệt khi các nhãn có tính thứ tự và các lỗi phân loại có mức độ nghiêm trọng khác nhau. Công thức này dựa trên lý thuyết đo lường (Measurement Theory) và lý thuyết thông tin (Information Theory). Để hiểu rõ hơn, chúng ta sẽ phân tích từng thành phần trong công thức của CEM:"
  },
  {
    "objectID": "statistic/Closeness_Evaluation_Measure__CEM_.html#các-bước-thực-hiện-tính-toán-proximity-matrix-và-cem",
    "href": "statistic/Closeness_Evaluation_Measure__CEM_.html#các-bước-thực-hiện-tính-toán-proximity-matrix-và-cem",
    "title": "Closeness Evaluation Measure (CEM)",
    "section": "Các bước thực hiện tính toán Proximity Matrix và CEM",
    "text": "Các bước thực hiện tính toán Proximity Matrix và CEM\n\nBước 1: Dữ liệu ví dụ\nGiả sử bạn có tập dữ liệu sau:\n\n\n\nMẫu\nNhãn thực tế (Ground Truth)\nNhãn dự đoán (System Output)\n\n\n\n\n1\n0\n0\n\n\n2\n1\n2\n\n\n3\n2\n2\n\n\n4\n2\n2\n\n\n5\n1\n1\n\n\n6\n0\n0\n\n\n7\n2\n1\n\n\n8\n1\n1\n\n\n9\n0\n0\n\n\n10\n2\n2\n\n\n\nCác nhóm phân loại là: 0 = tiêu cực, 1 = trung lập, 2 = tích cực.\n\n\nBước 2: Tính toán ma trận Proximity\n\nXác định số lượng phần tử trong mỗi nhóm:\n\nNhóm 0: 3 phần tử.\nNhóm 1: 3 phần tử.\nNhóm 2: 4 phần tử.\nTổng số phần tử: 10.\n\nCông thức tính Proximity cho nhóm với chính nó:\n\nProximity của nhóm với chính nó được tính bằng công thức: \\[\n\\text{prox}(i, i) = -\\log\\left(\\frac{\\text{số lượng phần tử trong nhóm i}}{2 \\times \\text{tổng số phần tử}}\\right)\n\\]\nÁp dụng công thức cho từng nhóm:\n\nNhóm 0: \\[\n\\text{prox}(0, 0) = -\\log\\left(\\frac{3}{2 \\times 10}\\right) = -\\log\\left(0.15\\right) = 0.823\n\\]\nNhóm 1: \\[\n\\text{prox}(1, 1) = -\\log\\left(\\frac{3}{2 \\times 10}\\right) = -\\log\\left(0.15\\right) = 0.823\n\\]\nNhóm 2: \\[\n\\text{prox}(2, 2) = -\\log\\left(\\frac{4}{2 \\times 10}\\right) = -\\log\\left(0.2\\right) = 0.699\n\\]\n\n\nCông thức tính Proximity giữa các nhóm khác nhau:\n\nProximity giữa hai nhóm khác nhau được tính bằng công thức: \\[\n\\text{prox}(i, j) = -\\log\\left(\\frac{\\frac{\\text{số lượng phần tử trong nhóm i}}{2} + \\text{tổng số phần tử nằm giữa nhóm i và nhóm j}}{\\text{tổng số phần tử}}\\right)\n\\]\nTính toán cho các cặp nhóm:\n\nGiữa nhóm 0 và 1: \\[\n\\text{prox}(0, 1) = -\\log\\left(\\frac{\\frac{3}{2} + 4}{10}\\right) = -\\log\\left(\\frac{1.5 + 4}{10}\\right) = -\\log(0.55) = 0.260\n\\]\nGiữa nhóm 0 và 2: \\[\n\\text{prox}(0, 2) = -\\log\\left(\\frac{\\frac{3}{2} + 7}{10}\\right) = -\\log(0.85) = 0.071\n\\]\nGiữa nhóm 1 và 2: \\[\n\\text{prox}(1, 2) = -\\log\\left(\\frac{\\frac{3}{2} + 4}{10}\\right) = -\\log(0.55) = 0.260\n\\]\n\n\nMa trận Proximity:\n\nSau khi tính toán, ma trận Proximity Matrix sẽ như sau:\n\n\n\n\nGroup\nClass 0\nClass 1\nClass 2\n\n\n\n\n0\n0.823\n0.260\n0.071\n\n\n1\n0.260\n0.823\n0.260\n\n\n2\n0.071\n0.260\n0.699\n\n\n\n\n\n\nBước 3: Tính điểm CEM\n\nTính tổng proximity (tính trên các dự báo đúng):\n\nTính Confusion Matrix:\n\n\n\n\nGroup\nClass 0\nClass 1\nClass 2\n\n\n\n\n0\n3\n0\n0\n\n\n1\n0\n2\n1\n\n\n2\n0\n1\n3\n\n\n\n\nLấy từng phần tử trong Proximity Matrix nhân với từng phần tử trong Confusion Matrix : \\[\n\\text{Tổng proximity} = 0.823 * 3 + 0.823 * 2 + 0.260 * 1 + + 0.260 * 1 + 0.699 * 3 = 6.732\n\\]\n\nTính tổng proximity tối đa (khi tất cả các dự đoán là chính xác):\n\nChỉ sử dụng các giá trị trên đường chéo chính (khi dự đoán đúng với nhãn thực tế): \\[\n\\text{Tổng proximity tối đa} = 0.823 * 3 + .823 * 3 + 0.699 * 4 = 7.733\n\\]\n\nTính điểm CEM: \\[\n\\text{CEM} = \\frac{\\text{Tổng proximity}}{\\text{Tổng proximity tối đa}} = \\frac{6.732}{7.733} = 0.871\n\\]\n\n\n\nKết quả cuối cùng\n\nMa trận Proximity:\n\n\n\nGroup\nClass 0\nClass 1\nClass 2\n\n\n\n\n0\n0.823\n0.260\n0.071\n\n\n1\n0.260\n0.823\n0.260\n\n\n2\n0.071\n0.260\n0.699\n\n\n\nĐiểm CEM: 0.871 (hoặc 87.1%).\n\n\nimport numpy as np\nimport math\n\n# Example ground truth (actual) and system output (predicted) labels\nground_truth = np.array([0, 1, 2, 2, 1, 0, 2, 1, 0, 2])  # Actual labels\nsystem_output = np.array([0, 2, 2, 2, 1, 0, 1, 1, 0, 2])  # Predicted labels\n\ndef calculate_proximity(ground_truth):\n    \"\"\"\n    Calculate the proximity between the classes based on their distribution.\n    \"\"\"\n    classes, counts = np.unique(ground_truth, return_counts=True)\n    total_items = len(ground_truth)\n    proximity_matrix = np.zeros((len(classes), len(classes)))\n    \n    for i in range(len(classes)):\n        for j in range(len(classes)):\n            if i == j:\n                # Proximity of the class with itself\n                proximity_matrix[i, j] = -math.log(counts[i] / (2 * total_items))\n            else:\n                # Proximity of two different classes\n                ni = counts[i]\n                nj = counts[j]\n                total_between = sum(counts[min(i, j):max(i, j)])  # Total items between classes\n                proximity_matrix[i, j] = -math.log((ni / 2 + total_between) / total_items)\n\n    return proximity_matrix\n\ndef cem_score(system_output, ground_truth, proximity_matrix):\n    \"\"\"\n    Calculate the CEM score based on system output and ground truth.\n    \"\"\"\n    total_proximity = 0\n    max_proximity = 0\n    for i in range(len(system_output)):\n        pred_class = system_output[i]\n        true_class = ground_truth[i]\n        total_proximity += proximity_matrix[pred_class, true_class]\n        max_proximity += proximity_matrix[true_class, true_class]  # When prediction is perfect\n\n    # CEM score is the ratio of total proximity to max proximity\n    cem_value = total_proximity / max_proximity if max_proximity != 0 else 0\n    return cem_value\n\n# Calculate proximity matrix from the ground truth data\nproximity_matrix = calculate_proximity(ground_truth)\n\n# Compute the CEM score based on the proximity matrix\ncem_value = cem_score(system_output, ground_truth, proximity_matrix)\n\nprint(f\"CEM Score: {cem_value:.4f}\")\n\nCEM Score: 0.8869"
  },
  {
    "objectID": "statistic/Closeness_Evaluation_Measure_CEM.html",
    "href": "statistic/Closeness_Evaluation_Measure_CEM.html",
    "title": "Closeness Evaluation Measure (CEM)",
    "section": "",
    "text": "Công thức Closeness Evaluation Measure (CEM) là một công cụ được thiết kế để đánh giá các hệ thống phân loại thứ tự (ordinal classification), đặc biệt khi các nhãn có tính thứ tự và các lỗi phân loại có mức độ nghiêm trọng khác nhau. Công thức này dựa trên lý thuyết đo lường (Measurement Theory) và lý thuyết thông tin (Information Theory). Để hiểu rõ hơn, chúng ta sẽ phân tích từng thành phần trong công thức của CEM:",
    "crumbs": [
      "Statistic",
      "Closeness Evaluation Measure (CEM)"
    ]
  },
  {
    "objectID": "statistic/Closeness_Evaluation_Measure_CEM.html#các-bước-thực-hiện-tính-toán-proximity-matrix-và-cem",
    "href": "statistic/Closeness_Evaluation_Measure_CEM.html#các-bước-thực-hiện-tính-toán-proximity-matrix-và-cem",
    "title": "Closeness Evaluation Measure (CEM)",
    "section": "Các bước thực hiện tính toán Proximity Matrix và CEM",
    "text": "Các bước thực hiện tính toán Proximity Matrix và CEM\n\nBước 1: Dữ liệu ví dụ\nGiả sử bạn có tập dữ liệu sau:\n\n\n\nMẫu\nNhãn thực tế (Ground Truth)\nNhãn dự đoán (System Output)\n\n\n\n\n1\n0\n0\n\n\n2\n1\n2\n\n\n3\n2\n2\n\n\n4\n2\n2\n\n\n5\n1\n1\n\n\n6\n0\n0\n\n\n7\n2\n1\n\n\n8\n1\n1\n\n\n9\n0\n0\n\n\n10\n2\n2\n\n\n\nCác nhóm phân loại là: 0 = tiêu cực, 1 = trung lập, 2 = tích cực.\n\n\nBước 2: Tính toán ma trận Proximity\n\nXác định số lượng phần tử trong mỗi nhóm:\n\nNhóm 0: 3 phần tử.\nNhóm 1: 3 phần tử.\nNhóm 2: 4 phần tử.\nTổng số phần tử: 10.\n\nCông thức tính Proximity cho nhóm với chính nó:\n\nProximity của nhóm với chính nó được tính bằng công thức: \\[\n\\text{prox}(i, i) = -\\log\\left(\\frac{\\text{số lượng phần tử trong nhóm i}}{2 \\times \\text{tổng số phần tử}}\\right)\n\\]\nÁp dụng công thức cho từng nhóm:\n\nNhóm 0: \\[\n\\text{prox}(0, 0) = -\\log\\left(\\frac{3}{2 \\times 10}\\right) = -\\log\\left(0.15\\right) = 0.823\n\\]\nNhóm 1: \\[\n\\text{prox}(1, 1) = -\\log\\left(\\frac{3}{2 \\times 10}\\right) = -\\log\\left(0.15\\right) = 0.823\n\\]\nNhóm 2: \\[\n\\text{prox}(2, 2) = -\\log\\left(\\frac{4}{2 \\times 10}\\right) = -\\log\\left(0.2\\right) = 0.699\n\\]\n\n\nCông thức tính Proximity giữa các nhóm khác nhau:\n\nProximity giữa hai nhóm khác nhau được tính bằng công thức: \\[\n\\text{prox}(i, j) = -\\log\\left(\\frac{\\frac{\\text{số lượng phần tử trong nhóm i}}{2} + \\text{tổng số phần tử nằm giữa nhóm i và nhóm j}}{\\text{tổng số phần tử}}\\right)\n\\]\nTính toán cho các cặp nhóm:\n\nGiữa nhóm 0 và 1: \\[\n\\text{prox}(0, 1) = -\\log\\left(\\frac{\\frac{3}{2} + 4}{10}\\right) = -\\log\\left(\\frac{1.5 + 4}{10}\\right) = -\\log(0.55) = 0.260\n\\]\nGiữa nhóm 0 và 2: \\[\n\\text{prox}(0, 2) = -\\log\\left(\\frac{\\frac{3}{2} + 7}{10}\\right) = -\\log(0.85) = 0.071\n\\]\nGiữa nhóm 1 và 2: \\[\n\\text{prox}(1, 2) = -\\log\\left(\\frac{\\frac{3}{2} + 4}{10}\\right) = -\\log(0.55) = 0.260\n\\]\n\n\nMa trận Proximity:\n\nSau khi tính toán, ma trận Proximity Matrix sẽ như sau:\n\n\n\n\nGroup\nClass 0\nClass 1\nClass 2\n\n\n\n\n0\n0.823\n0.260\n0.071\n\n\n1\n0.260\n0.823\n0.260\n\n\n2\n0.071\n0.260\n0.699\n\n\n\n\n\n\nBước 3: Tính điểm CEM\n\nTính tổng proximity (tính trên các dự báo đúng):\n\nTính Confusion Matrix:\n\n\n\n\nGroup\nClass 0\nClass 1\nClass 2\n\n\n\n\n0\n3\n0\n0\n\n\n1\n0\n2\n1\n\n\n2\n0\n1\n3\n\n\n\n\nLấy từng phần tử trong Proximity Matrix nhân với từng phần tử trong Confusion Matrix : \\[\n\\text{Tổng proximity} = 0.823 * 3 + 0.823 * 2 + 0.260 * 1 + + 0.260 * 1 + 0.699 * 3 = 6.732\n\\]\n\nTính tổng proximity tối đa (khi tất cả các dự đoán là chính xác):\n\nChỉ sử dụng các giá trị trên đường chéo chính (khi dự đoán đúng với nhãn thực tế): \\[\n\\text{Tổng proximity tối đa} = 0.823 * 3 + .823 * 3 + 0.699 * 4 = 7.733\n\\]\n\nTính điểm CEM: \\[\n\\text{CEM} = \\frac{\\text{Tổng proximity}}{\\text{Tổng proximity tối đa}} = \\frac{6.732}{7.733} = 0.871\n\\]\n\n\n\nKết quả cuối cùng\n\nMa trận Proximity:\n\n\n\nGroup\nClass 0\nClass 1\nClass 2\n\n\n\n\n0\n0.823\n0.260\n0.071\n\n\n1\n0.260\n0.823\n0.260\n\n\n2\n0.071\n0.260\n0.699\n\n\n\nĐiểm CEM: 0.871 (hoặc 87.1%).\n\n\nimport numpy as np\nimport math\n\n# Example ground truth (actual) and system output (predicted) labels\nground_truth = np.array([0, 1, 2, 2, 1, 0, 2, 1, 0, 2])  # Actual labels\nsystem_output = np.array([0, 2, 2, 2, 1, 0, 1, 1, 0, 2])  # Predicted labels\n\ndef calculate_proximity(ground_truth):\n    \"\"\"\n    Calculate the proximity between the classes based on their distribution.\n    \"\"\"\n    classes, counts = np.unique(ground_truth, return_counts=True)\n    total_items = len(ground_truth)\n    proximity_matrix = np.zeros((len(classes), len(classes)))\n    \n    for i in range(len(classes)):\n        for j in range(len(classes)):\n            if i == j:\n                # Proximity of the class with itself\n                proximity_matrix[i, j] = -math.log(counts[i] / (2 * total_items))\n            else:\n                # Proximity of two different classes\n                ni = counts[i]\n                nj = counts[j]\n                total_between = sum(counts[min(i, j):max(i, j)])  # Total items between classes\n                proximity_matrix[i, j] = -math.log((ni / 2 + total_between) / total_items)\n\n    return proximity_matrix\n\ndef cem_score(system_output, ground_truth, proximity_matrix):\n    \"\"\"\n    Calculate the CEM score based on system output and ground truth.\n    \"\"\"\n    total_proximity = 0\n    max_proximity = 0\n    for i in range(len(system_output)):\n        pred_class = system_output[i]\n        true_class = ground_truth[i]\n        total_proximity += proximity_matrix[pred_class, true_class]\n        max_proximity += proximity_matrix[true_class, true_class]  # When prediction is perfect\n\n    # CEM score is the ratio of total proximity to max proximity\n    cem_value = total_proximity / max_proximity if max_proximity != 0 else 0\n    return cem_value\n\n# Calculate proximity matrix from the ground truth data\nproximity_matrix = calculate_proximity(ground_truth)\n\n# Compute the CEM score based on the proximity matrix\ncem_value = cem_score(system_output, ground_truth, proximity_matrix)\n\nprint(f\"CEM Score: {cem_value:.4f}\")\n\nCEM Score: 0.8869",
    "crumbs": [
      "Statistic",
      "Closeness Evaluation Measure (CEM)"
    ]
  },
  {
    "objectID": "statistic/confidenceIntervalWithSmallSampleSize.html",
    "href": "statistic/confidenceIntervalWithSmallSampleSize.html",
    "title": "Ước lượng khoảng tin cậy với cỡ mẫu nhỏ",
    "section": "",
    "text": "Để ước tính khoảng tin cậy cho trung bình của tổng thể với kích thước mẫu nhỏ, bạn cần xem xét cẩn thận do sự không chắc chắn gia tăng khi có dữ liệu hạn chế. Khi kích thước mẫu nhỏ, thường sử dụng phân phối t thay vì phân phối chuẩn để tính đến biến động. Dưới đây là cách bạn có thể ước tính khoảng tin cậy cho trung bình của dân số với kích thước mẫu nhỏ:\n\nThu thập và Tóm tắt Dữ liệu: Thu thập dữ liệu mẫu của bạn và tính trung bình mẫu (\\(\\bar{x}\\)) và độ lệch chuẩn mẫu (\\(s\\)).\nChọn Mức Độ Tin Cậy: Quyết định mức độ tin cậy mong muốn cho khoảng tin cậy, thường là 95% hoặc 99%.\nTính Giá Trị t-Critical: Tìm giá trị t-critical liên quan đến mức độ tin cậy đã chọn và bậc tự do, với bậc tự do cho mẫu kích thước \\(n\\) là \\(n - 1\\).\nTính Toán Sai Số Chuẩn: Tính sai số chuẩn (\\(SE\\)) của trung bình mẫu bằng công thức: \\[ SE = \\frac{s}{\\sqrt{n}} \\]\nTính Toán Sai Số Biên: Tính sai số biên (\\(MOE\\)) bằng công thức: \\[ MOE = t_{\\text{critical}} \\times SE \\]\nTính Khoảng Tin Cậy: Khoảng tin cậy được tính bằng cách trừ và cộng sai số biên từ trung bình mẫu: \\[ \\text{Khoảng Tin Cậy} = \\bar{x} \\pm MOE \\]\n\nDưới đây là ví dụ về cách tính khoảng tin cậy bằng Python:\nLưu ý với kích thước mẫu nhỏ, khoảng tin cậy kết quả có thể rộng, cho thấy mức độ không chắc chắn tương đối cao. Khi kích thước mẫu tăng lên, khoảng tin cậy sẽ trở nên hẹp hơn, cung cấp một ước tính chính xác hơn về trung bình của tổng thể.\n\nimport scipy.stats as stats\nimport numpy as np\n\n# Dữ liệu mẫu\nsample_data = [12, 14, 15, 17, 18, 19, 20]\nn = len(sample_data)\n\n# Tính trung bình mẫu và độ lệch chuẩn mẫu\nsample_mean = np.mean(sample_data)\nsample_std = np.std(sample_data, ddof=1)  # Sửa đổi Bessel cho độ lệch chuẩn mẫu\n\n# Đặt mức độ tin cậy\nconfidence_level = 0.95\n\n# Tính độ tự do\ndegrees_of_freedom = n - 1\n\n# Tính giá trị t-critical\nt_critical = stats.t.ppf(1 - (1 - confidence_level) / 2, df=degrees_of_freedom)\n\n# Tính sai số chuẩn\nSE = sample_std / np.sqrt(n)\n\n# Tính sai số biên\nMOE = t_critical * SE\n\n# Tính khoảng tin cậy\nconfidence_interval = (sample_mean - MOE, sample_mean + MOE)\n\nprint(\"Trung bình mẫu:\", sample_mean)\nprint(\"Khoảng tin cậy:\", confidence_interval)\n\nTrung bình mẫu: 16.428571428571427\nKhoảng tin cậy: (13.766410649936082, 19.09073220720677)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Parameters for the t-distribution\ndegrees_of_freedom = [3, 10, 30]\nx_range = np.linspace(-5, 5, 500)\n\n# Generate data for the normal distribution\nnormal_pdf = stats.norm.pdf(x_range, 0, 1)\n\n# Generate data for t-distributions with different degrees of freedom\nt_distributions = [stats.t.pdf(x_range, df) for df in degrees_of_freedom]\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(x_range, normal_pdf, label='Normal Distribution')\n\nfor i, df in enumerate(degrees_of_freedom):\n    plt.plot(x_range, t_distributions[i], label=f't-Distribution (df={df})')\n\nplt.title('Comparison of Normal and t-Distributions')\nplt.xlabel('x')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nTrong biểu đồ này, bạn sẽ thấy đường cong mật độ xác suất cho phân phối chuẩn tiêu chuẩn và phân phối t-Student với các độ tự do khác nhau (3, 10 và 30). Khi độ tự do tăng lên, phân phối t-Student tiến dần tới phân phối chuẩn. Với các độ tự do nhỏ hơn, phân phối t-Student có đuôi dày và đỉnh phẳnghơn so với phân phối chuẩn. Khi độ tự do tăng lên, phân phối t-Student trở nên giống phân phối chuẩn hơn.",
    "crumbs": [
      "Statistic",
      "Ước lượng khoảng tin cậy với cỡ mẫu nhỏ"
    ]
  },
  {
    "objectID": "statistic/StandardizedCoefficients.html",
    "href": "statistic/StandardizedCoefficients.html",
    "title": "Standardized Coefficients vs Unstandardized Coefficients",
    "section": "",
    "text": "Hệ số hồi quy có thể được chuẩn hóa hoặc không chuẩn hóa, và chúng có mục đích khác nhau trong phân tích thống kê. Dưới đây là sự khác biệt giữa hai loại hệ số này:\n\nHệ số không chuẩn hóa:\n\nHệ số không chuẩn hóa (Unstandardized Coefficients), còn được gọi là hệ số góc hoặc hệ số beta, biểu thị sự thay đổi trong biến phụ thuộc (Y) khi biến độc lập (X) thay đổi một đơn vị, trong khi giữ tất cả các biến khác cố định.\nChúng được biểu thị trong đơn vị đo lường gốc của các biến. Ví dụ, nếu bạn đang dự đoán mức lương (bằng đô la) dựa trên số năm kinh nghiệm, hệ số không chuẩn hóa có thể là, ví dụ, 5.000 đô la, có nghĩa là với mỗi năm kinh nghiệm bổ sung, mức lương dự kiến sẽ tăng 5.000 đô la.\nHệ số không chuẩn hóa hữu ích khi bạn muốn hiểu về tác động thực tế của biến độc lập lên biến phụ thuộc.\n\nHệ số chuẩn hóa:\n\nHệ số chuẩn hóa (Standardized Coefficients), còn được gọi là trọng số beta hoặc hệ số beta, là các hệ số đã được biến đổi sao cho có giá trị trung bình bằng 0 và độ lệch chuẩn bằng 1. Quá trình chuẩn hóa này cho phép so sánh trực tiếp sự quan trọng tương đối của các biến độc lập khác nhau trong một mô hình hồi quy.\nChúng đo lường về sự thay đổi của biến phụ thuộc theo bao nhiêu độ lệch chuẩn khi biến độc lập thay đổi một độ lệch chuẩn.\nHệ số chuẩn hóa đặc biệt hữu ích khi bạn muốn so sánh sự quan trọng tương đối của các biến dự đoán khác nhau trong một mô hình, đặc biệt là khi các biến được đo trên các thang đo khác nhau. Chúng giúp trả lời câu hỏi như “Biến dự đoán nào có tác động mạnh hơn đối với kết quả, bất kể đơn vị đo lường?”\n\n\nTóm lại, hệ số không chuẩn hóa hữu ích để hiểu về tác động thực tế của các biến độc lập lên biến phụ thuộc trong đơn vị gốc của chúng, trong khi hệ số chuẩn hóa hữu ích để so sánh sự quan trọng tương đối của các biến dự đoán khác nhau khi các biến đo trên các thang đo khác nhau. Sự lựa chọn giữa hai loại hệ số này phụ thuộc vào câu hỏi nghiên cứu cụ thể và mục tiêu của phân tích của bạn.",
    "crumbs": [
      "Statistic",
      "Standardized Coefficients vs Unstandardized Coefficients"
    ]
  },
  {
    "objectID": "statistic/StandardizedCoefficients.html#so-sánh",
    "href": "statistic/StandardizedCoefficients.html#so-sánh",
    "title": "Standardized Coefficients vs Unstandardized Coefficients",
    "section": "",
    "text": "Hệ số hồi quy có thể được chuẩn hóa hoặc không chuẩn hóa, và chúng có mục đích khác nhau trong phân tích thống kê. Dưới đây là sự khác biệt giữa hai loại hệ số này:\n\nHệ số không chuẩn hóa:\n\nHệ số không chuẩn hóa (Unstandardized Coefficients), còn được gọi là hệ số góc hoặc hệ số beta, biểu thị sự thay đổi trong biến phụ thuộc (Y) khi biến độc lập (X) thay đổi một đơn vị, trong khi giữ tất cả các biến khác cố định.\nChúng được biểu thị trong đơn vị đo lường gốc của các biến. Ví dụ, nếu bạn đang dự đoán mức lương (bằng đô la) dựa trên số năm kinh nghiệm, hệ số không chuẩn hóa có thể là, ví dụ, 5.000 đô la, có nghĩa là với mỗi năm kinh nghiệm bổ sung, mức lương dự kiến sẽ tăng 5.000 đô la.\nHệ số không chuẩn hóa hữu ích khi bạn muốn hiểu về tác động thực tế của biến độc lập lên biến phụ thuộc.\n\nHệ số chuẩn hóa:\n\nHệ số chuẩn hóa (Standardized Coefficients), còn được gọi là trọng số beta hoặc hệ số beta, là các hệ số đã được biến đổi sao cho có giá trị trung bình bằng 0 và độ lệch chuẩn bằng 1. Quá trình chuẩn hóa này cho phép so sánh trực tiếp sự quan trọng tương đối của các biến độc lập khác nhau trong một mô hình hồi quy.\nChúng đo lường về sự thay đổi của biến phụ thuộc theo bao nhiêu độ lệch chuẩn khi biến độc lập thay đổi một độ lệch chuẩn.\nHệ số chuẩn hóa đặc biệt hữu ích khi bạn muốn so sánh sự quan trọng tương đối của các biến dự đoán khác nhau trong một mô hình, đặc biệt là khi các biến được đo trên các thang đo khác nhau. Chúng giúp trả lời câu hỏi như “Biến dự đoán nào có tác động mạnh hơn đối với kết quả, bất kể đơn vị đo lường?”\n\n\nTóm lại, hệ số không chuẩn hóa hữu ích để hiểu về tác động thực tế của các biến độc lập lên biến phụ thuộc trong đơn vị gốc của chúng, trong khi hệ số chuẩn hóa hữu ích để so sánh sự quan trọng tương đối của các biến dự đoán khác nhau khi các biến đo trên các thang đo khác nhau. Sự lựa chọn giữa hai loại hệ số này phụ thuộc vào câu hỏi nghiên cứu cụ thể và mục tiêu của phân tích của bạn.",
    "crumbs": [
      "Statistic",
      "Standardized Coefficients vs Unstandardized Coefficients"
    ]
  },
  {
    "objectID": "statistic/StandardizedCoefficients.html#công-thức-tính-standardized-coefficients",
    "href": "statistic/StandardizedCoefficients.html#công-thức-tính-standardized-coefficients",
    "title": "Standardized Coefficients vs Unstandardized Coefficients",
    "section": "Công thức tính Standardized Coefficients",
    "text": "Công thức tính Standardized Coefficients\n\nTrong trường hợp hồi quy tuyến tính, công thức để tính hệ số beta chuẩn hóa ($ _{, i} $) cho biến độc lập $ X_i $ là:\n\\[ \\beta_{\\text{std}, i} = \\frac{\\beta_i}{\\sigma_i} \\]\n\n$ _{, i} $: Hệ số beta chuẩn hóa cho biến độc lập $ X_i $.\n$ _i $: Hệ số beta không chuẩn hóa cho biến độc lập $ X_i $.\n$ _i $: Độ lệch chuẩn (standard deviation) của biến độc lập $ X_i $.\n\nTrong trường hợp hồi quy hồi quy logistic, công thức để tính hệ số beta chuẩn hóa ($ _{, i} $) cho biến độc lập $ X_i $ là:\n\\[\\beta_{\\text{std}, i} = \\frac{\\beta_i}{\\sigma_i} \\times \\frac{\\sqrt{3}}{\\pi}\\]\nTrong đó:\n\n\\(\\beta_i\\) là hệ số (beta) của biến dự đoán \\(X_i\\) trong mô hình hồi quy logistic.\n\\(\\sigma_i\\) là độ lệch chuẩn của biến dự đoán \\(X_i\\) trong tập dữ liệu huấn luyện.\n\\(\\beta_{\\text{std}, i}\\) là hệ số beta chuẩn hóa cho biến dự đoán \\(X_i\\).",
    "crumbs": [
      "Statistic",
      "Standardized Coefficients vs Unstandardized Coefficients"
    ]
  },
  {
    "objectID": "machine-learning/Wine_Quality_pytorch.html",
    "href": "machine-learning/Wine_Quality_pytorch.html",
    "title": "Train a model on the Wine Quality dataset using ordinal loss and the Kappa metric in PyTorch",
    "section": "",
    "text": "Date: 2024-12-18\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load the dataset\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\ndata = pd.read_csv(url, sep=';')\n\n# Define features and target\nX = data.drop('quality', axis=1)\ny = data['quality'] - 3\n\n# Normalize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Convert to PyTorch tensors\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Create DataLoader for training and testing\ntrain_data = TensorDataset(X_train, y_train)\ntest_data = TensorDataset(X_test, y_test)\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=32)\n\n# Set the device (GPU if available, otherwise CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nUsing device: cuda",
    "crumbs": [
      "Machine learning",
      "Train a model on the Wine Quality dataset using ordinal loss and the Kappa metric in PyTorch"
    ]
  },
  {
    "objectID": "machine-learning/Wine_Quality_pytorch.html#define-the-model",
    "href": "machine-learning/Wine_Quality_pytorch.html#define-the-model",
    "title": "Train a model on the Wine Quality dataset using ordinal loss and the Kappa metric in PyTorch",
    "section": "Define the Model",
    "text": "Define the Model\nThe model definition remains the same as in the previous solution.\n\nimport torch.nn as nn\n\nclass OrdinalNN(nn.Module):\n    def __init__(self):\n        super(OrdinalNN, self).__init__()\n        self.fc1 = nn.Linear(X.shape[1], 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 6)  # Number of classes (3-8)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = OrdinalNN().to(device)\n\n\nclass OrdinalFocalLoss(nn.Module):\n    def __init__(self, gamma=2, alpha=0.25, num_classes=6):\n        super(OrdinalFocalLoss, self).__init__()\n        self.gamma = gamma  # Focusing parameter\n        self.alpha = alpha  # Weighting factor\n        self.num_classes = num_classes\n\n    def forward(self, outputs, targets):\n        \"\"\"\n        Compute the Ordinal Focal Loss\n\n        :param outputs: Predicted logits from the model (batch_size, num_classes)\n        :param targets: Ground truth labels (batch_size)\n        :return: Loss value\n        \"\"\"\n        # Convert targets to one-hot encoding\n        targets_one_hot = torch.zeros(targets.size(0), self.num_classes).to(targets.device)\n        targets_one_hot.scatter_(1, targets.unsqueeze(1), 1)\n\n        # Apply softmax to outputs to get class probabilities\n        probs = torch.softmax(outputs, dim=1)\n\n        # Calculate the probability of the true class\n        p_t = torch.sum(probs * targets_one_hot, dim=1)  # This is p_t for each instance\n\n        # Compute the focal loss\n        loss = -self.alpha * (1 - p_t) ** self.gamma * torch.log(p_t + 1e-8)  # Add epsilon to avoid log(0)\n\n        # Return the average loss\n        return torch.mean(loss)\n\n\nfrom sklearn.metrics import cohen_kappa_score\n# Initialize the criterion and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Function to compute the Kappa score\ndef compute_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true.cpu(), y_pred.cpu())\n\n# Train the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    y_true = []\n    y_pred = []\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate loss\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        # Collect true and predicted labels for Kappa score\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n    \n    # Calculate Kappa score\n    kappa_score = compute_kappa(torch.tensor(y_true), torch.tensor(y_pred))\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Kappa: {kappa_score}\")\n\nEpoch 1/100, Loss: 1.5258033365011214, Kappa: 0.027160395525093195\nEpoch 2/100, Loss: 1.14848440438509, Kappa: 0.1941581189042062\nEpoch 3/100, Loss: 1.0254976019263267, Kappa: 0.29289946055146165\nEpoch 4/100, Loss: 0.988839827477932, Kappa: 0.3383639594251141\nEpoch 5/100, Loss: 0.965928153693676, Kappa: 0.3631989771546358\nEpoch 6/100, Loss: 0.9539458021521569, Kappa: 0.3677904087658541\nEpoch 7/100, Loss: 0.9409917533397675, Kappa: 0.3671246515932114\nEpoch 8/100, Loss: 0.9327839985489845, Kappa: 0.3751479492595736\nEpoch 9/100, Loss: 0.9210372045636177, Kappa: 0.3685858232131014\nEpoch 10/100, Loss: 0.922743383049965, Kappa: 0.3722848662886773\nEpoch 11/100, Loss: 0.9097453355789185, Kappa: 0.3813580749062848\nEpoch 12/100, Loss: 0.9032061487436295, Kappa: 0.37044592857844605\nEpoch 13/100, Loss: 0.8965445622801781, Kappa: 0.3797972038405859\nEpoch 14/100, Loss: 0.8881877571344375, Kappa: 0.4032125634822731\nEpoch 15/100, Loss: 0.8835199296474456, Kappa: 0.40018818898663533\nEpoch 16/100, Loss: 0.8790309250354766, Kappa: 0.3903325462495525\nEpoch 17/100, Loss: 0.8702574223279953, Kappa: 0.40721159778533744\nEpoch 18/100, Loss: 0.8697167977690696, Kappa: 0.4072256904559757\nEpoch 19/100, Loss: 0.8619306489825249, Kappa: 0.4140637008713334\nEpoch 20/100, Loss: 0.8568678289651871, Kappa: 0.4211031088992041\nEpoch 21/100, Loss: 0.8524924352765083, Kappa: 0.41451338248257064\nEpoch 22/100, Loss: 0.8480689927935601, Kappa: 0.41419673103023\nEpoch 23/100, Loss: 0.8440612345933914, Kappa: 0.44178941625750134\nEpoch 24/100, Loss: 0.8406461104750633, Kappa: 0.4034622168334888\nEpoch 25/100, Loss: 0.8361509755253792, Kappa: 0.43327084440256236\nEpoch 26/100, Loss: 0.8344028279185295, Kappa: 0.41844363720856625\nEpoch 27/100, Loss: 0.8277643755078316, Kappa: 0.4222757717701464\nEpoch 28/100, Loss: 0.8244240581989288, Kappa: 0.43849674868294786\nEpoch 29/100, Loss: 0.8242936119437217, Kappa: 0.42040352491703603\nEpoch 30/100, Loss: 0.8143198490142822, Kappa: 0.43104302757442947\nEpoch 31/100, Loss: 0.8092397406697274, Kappa: 0.44060655010864125\nEpoch 32/100, Loss: 0.8103402808308602, Kappa: 0.445418000079017\nEpoch 33/100, Loss: 0.8025514364242554, Kappa: 0.4359494216495535\nEpoch 34/100, Loss: 0.801366850733757, Kappa: 0.45975261807226997\nEpoch 35/100, Loss: 0.7979567974805832, Kappa: 0.4482904623702938\nEpoch 36/100, Loss: 0.7909636944532394, Kappa: 0.4499257267039999\nEpoch 37/100, Loss: 0.7835334852337837, Kappa: 0.46419209788669\nEpoch 38/100, Loss: 0.7814476490020752, Kappa: 0.4648827375611594\nEpoch 39/100, Loss: 0.7808900579810143, Kappa: 0.46110282448596707\nEpoch 40/100, Loss: 0.7702956169843673, Kappa: 0.4892929788113062\nEpoch 41/100, Loss: 0.7734991297125816, Kappa: 0.47925739353272945\nEpoch 42/100, Loss: 0.7667867332696915, Kappa: 0.48040533912521344\nEpoch 43/100, Loss: 0.7656202584505081, Kappa: 0.475668626326781\nEpoch 44/100, Loss: 0.7598542019724845, Kappa: 0.4858225595558915\nEpoch 45/100, Loss: 0.7581931084394455, Kappa: 0.49598754033499703\nEpoch 46/100, Loss: 0.7500043898820877, Kappa: 0.4938932823137663\nEpoch 47/100, Loss: 0.7498278692364693, Kappa: 0.4952857622244943\nEpoch 48/100, Loss: 0.743773840367794, Kappa: 0.5079226029464861\nEpoch 49/100, Loss: 0.7405090779066086, Kappa: 0.5198281901732583\nEpoch 50/100, Loss: 0.7364834100008011, Kappa: 0.5023553770687714\nEpoch 51/100, Loss: 0.7336597308516503, Kappa: 0.5167070843799515\nEpoch 52/100, Loss: 0.7293936885893345, Kappa: 0.508031801733295\nEpoch 53/100, Loss: 0.7258813440799713, Kappa: 0.5246337519963721\nEpoch 54/100, Loss: 0.7226672306656837, Kappa: 0.5265172456118816\nEpoch 55/100, Loss: 0.7217974692583085, Kappa: 0.5230864181656874\nEpoch 56/100, Loss: 0.712223195284605, Kappa: 0.5266455229603828\nEpoch 57/100, Loss: 0.7112390361726284, Kappa: 0.5342339017248592\nEpoch 58/100, Loss: 0.7029153138399125, Kappa: 0.5468456178231427\nEpoch 59/100, Loss: 0.7027535423636436, Kappa: 0.5393219619781618\nEpoch 60/100, Loss: 0.7004692874848842, Kappa: 0.553876299131837\nEpoch 61/100, Loss: 0.6959677867591381, Kappa: 0.5544725929676446\nEpoch 62/100, Loss: 0.6945664048194885, Kappa: 0.5516791604790041\nEpoch 63/100, Loss: 0.6882325552403927, Kappa: 0.5410228096173484\nEpoch 64/100, Loss: 0.6827452167868614, Kappa: 0.552530873054403\nEpoch 65/100, Loss: 0.6810622230172158, Kappa: 0.5720976915356306\nEpoch 66/100, Loss: 0.6765776731073856, Kappa: 0.5615048120704035\nEpoch 67/100, Loss: 0.6707129381597042, Kappa: 0.5688434451478712\nEpoch 68/100, Loss: 0.6698078818619251, Kappa: 0.5779857384414895\nEpoch 69/100, Loss: 0.6700948402285576, Kappa: 0.5699745672150069\nEpoch 70/100, Loss: 0.6645593464374542, Kappa: 0.5621607359002477\nEpoch 71/100, Loss: 0.6604863002896308, Kappa: 0.5721880442236442\nEpoch 72/100, Loss: 0.6597750805318355, Kappa: 0.5939745996643764\nEpoch 73/100, Loss: 0.6539112649857998, Kappa: 0.5778682427539623\nEpoch 74/100, Loss: 0.6505351833999157, Kappa: 0.5958212350004243\nEpoch 75/100, Loss: 0.6485376708209515, Kappa: 0.5947310051377226\nEpoch 76/100, Loss: 0.6440189868211746, Kappa: 0.590627979197287\nEpoch 77/100, Loss: 0.6387162208557129, Kappa: 0.5899935591403664\nEpoch 78/100, Loss: 0.6353652991354466, Kappa: 0.5983798594678247\nEpoch 79/100, Loss: 0.6351467996835709, Kappa: 0.6006119625320245\nEpoch 80/100, Loss: 0.631549759209156, Kappa: 0.6093738987430988\nEpoch 81/100, Loss: 0.6277068927884102, Kappa: 0.61762519112683\nEpoch 82/100, Loss: 0.6236165843904018, Kappa: 0.6089009813451609\nEpoch 83/100, Loss: 0.6264286696910858, Kappa: 0.6153901337182963\nEpoch 84/100, Loss: 0.6178285926580429, Kappa: 0.6276718056686829\nEpoch 85/100, Loss: 0.6154053710401058, Kappa: 0.6062257547132006\nEpoch 86/100, Loss: 0.6118617177009582, Kappa: 0.5975909896858916\nEpoch 87/100, Loss: 0.6087865322828293, Kappa: 0.6062408326862287\nEpoch 88/100, Loss: 0.6076791845262051, Kappa: 0.6200215390448939\nEpoch 89/100, Loss: 0.6064654208719731, Kappa: 0.6229728790088249\nEpoch 90/100, Loss: 0.600296714156866, Kappa: 0.6301554272539882\nEpoch 91/100, Loss: 0.6021047808229923, Kappa: 0.6221714061388228\nEpoch 92/100, Loss: 0.5982880525290966, Kappa: 0.6244946251032697\nEpoch 93/100, Loss: 0.590181715041399, Kappa: 0.6320299679124919\nEpoch 94/100, Loss: 0.5918105013668538, Kappa: 0.6247179516270536\nEpoch 95/100, Loss: 0.5913812078535556, Kappa: 0.6348833139597976\nEpoch 96/100, Loss: 0.5805985651910305, Kappa: 0.6416545681465067\nEpoch 97/100, Loss: 0.5784861400723458, Kappa: 0.6488824583376482\nEpoch 98/100, Loss: 0.5749207615852356, Kappa: 0.6498347050063702\nEpoch 99/100, Loss: 0.5697298489511013, Kappa: 0.652995478504598\nEpoch 100/100, Loss: 0.5659848034381867, Kappa: 0.6566265295483256\n\n\n\n# Evaluate the model\nmodel.eval()\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Collect true and predicted labels for Kappa score\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n\n# Calculate Kappa score on the test set\ntest_kappa = compute_kappa(torch.tensor(y_true), torch.tensor(y_pred))\nprint(f\"Test Kappa Score: {test_kappa}\")\n\nTest Kappa Score: 0.38975208678525664",
    "crumbs": [
      "Machine learning",
      "Train a model on the Wine Quality dataset using ordinal loss and the Kappa metric in PyTorch"
    ]
  },
  {
    "objectID": "machine-learning/Wine_Quality_pytorch.html#so-sánh-cross-entropy-loss-và-focal-loss",
    "href": "machine-learning/Wine_Quality_pytorch.html#so-sánh-cross-entropy-loss-và-focal-loss",
    "title": "Train a model on the Wine Quality dataset using ordinal loss and the Kappa metric in PyTorch",
    "section": "So sánh Cross Entropy Loss và Focal Loss",
    "text": "So sánh Cross Entropy Loss và Focal Loss\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Assuming OrdinalFocalLoss and model are defined as before\n# CrossEntropy Loss\ncross_entropy_criterion = nn.CrossEntropyLoss().to(device)\n\n# Focal Loss\nfocal_loss_criterion = OrdinalFocalLoss().to(device)\n\n# Initialize your model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = OrdinalNN().to(device)\n\n# Helper function for Kappa Score and Accuracy\ndef compute_metrics(y_true, y_pred):\n    accuracy = accuracy_score(y_true.cpu(), y_pred.cpu())\n    kappa = cohen_kappa_score(y_true.cpu(), y_pred.cpu())\n    return accuracy, kappa\n\n# Training function for comparison\ndef train_model(criterion, model, num_epochs=100, train_loader=None, test_loader=None):\n\n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Train the model\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        y_true = []\n        y_pred = []\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate loss\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            # Collect true and predicted labels for Kappa score\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n        \n        # Calculate Kappa score\n        kappa_score = compute_kappa(torch.tensor(y_true), torch.tensor(y_pred))\n        # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Kappa: {kappa_score}\")\n\n    # Evaluate on test set\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n\n    accuracy, kappa = compute_metrics(torch.tensor(y_true), torch.tensor(y_pred))\n    print(f\"Test Accuracy: {accuracy:.4f}, Test Kappa: {kappa:.4f}\")\n\n# Run training and evaluation for both loss functions\nprint(\"Training with CrossEntropyLoss:\")\ntrain_model(cross_entropy_criterion, model=model, num_epochs=100, train_loader=train_loader, test_loader=test_loader)\n\nprint(\"\\nTraining with Focal Loss:\")\ntrain_model(focal_loss_criterion, model=model, num_epochs=100, train_loader=train_loader, test_loader=test_loader)\n\nTraining with CrossEntropyLoss:\nTest Accuracy: 0.6125, Test Kappa: 0.3739\n\nTraining with Focal Loss:\nTest Accuracy: 0.5969, Test Kappa: 0.3695\n\n\n\nclass OrdinalModelWithDropout(nn.Module):\n    def __init__(self, input_size, num_classes, dropout_rate=0.5):\n        super(OrdinalModelWithDropout, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        \n        # Adding dropout layers between fully connected layers\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)  # Apply dropout after the first layer\n        x = torch.relu(self.fc2(x))\n        x = self.dropout(x)  # Apply dropout after the second layer\n        x = self.fc3(x)\n        return x\n\n\n# Initialize your model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = OrdinalModelWithDropout(input_size=X.shape[1], num_classes=6).to(device)\n\n# Run training and evaluation for both loss functions\nprint(\"Training with CrossEntropyLoss:\")\ntrain_model(cross_entropy_criterion, model=model, num_epochs=100, train_loader=train_loader, test_loader=test_loader)\n\nprint(\"\\nTraining with Focal Loss:\")\ntrain_model(focal_loss_criterion, model=model, num_epochs=100, train_loader=train_loader, test_loader=test_loader)\n\nTraining with CrossEntropyLoss:\nTest Accuracy: 0.6188, Test Kappa: 0.3709\n\nTraining with Focal Loss:\nTest Accuracy: 0.6438, Test Kappa: 0.4167",
    "crumbs": [
      "Machine learning",
      "Train a model on the Wine Quality dataset using ordinal loss and the Kappa metric in PyTorch"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "",
    "text": "import shap\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Load the German Credit Data\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ny_train[y_train == 2] = 0\ny_test[y_test == 2] = 0",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#load-libraries-and-data",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#load-libraries-and-data",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "",
    "text": "import shap\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Load the German Credit Data\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ny_train[y_train == 2] = 0\ny_test[y_test == 2] = 0",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#train-a-logistic-regression-model",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#train-a-logistic-regression-model",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "Train a Logistic Regression Model",
    "text": "Train a Logistic Regression Model\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score\n# Create a pipeline with feature selection using SelectKBest and logistic regression\npipeline = Pipeline([\n    ('scaler', StandardScaler()),                       # Standardize features\n    ('feature_selector', SelectKBest(score_func=f_classif, k=10)),  # Select top k features using f_classif\n    ('classifier', LogisticRegression(max_iter=1000))   # Logistic regression classifier\n])\n# Train the pipeline on the training data\npipeline.fit(X_train, y_train)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('feature_selector', SelectKBest()),\n                ('classifier', LogisticRegression(max_iter=1000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('feature_selector', SelectKBest()),\n                ('classifier', LogisticRegression(max_iter=1000))])StandardScalerStandardScaler()SelectKBestSelectKBest()LogisticRegressionLogisticRegression(max_iter=1000)\n\n\n\n# Predict on the test data\ny_pred = pipeline.predict(X_test)\n\n# Calculate accuracy\nauc = roc_auc_score(y_test, y_pred)\nprint(f\"auc: {auc:.2f}\")\n\nauc: 0.60",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#create-shap-explainer",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#create-shap-explainer",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "Create SHAP Explainer",
    "text": "Create SHAP Explainer\n\n# Create a callable model class that wraps the pipeline\nclass PipelineModel:\n    def __init__(self, pipeline):\n        self.pipeline = pipeline\n    \n    def __call__(self, x):\n        return self.pipeline.predict_proba(x)[:, 1]\n\n# Create a SHAP explainer for the callable model\ncallable_model = PipelineModel(pipeline)\nexplainer = shap.Explainer(callable_model, X_train)\n\n# Choose an instance to explain\ninstance_to_explain = X_test.iloc[0]  # Choose the instance you want to explain\n\n# Calculate SHAP values for the instance\nshap_values = explainer.shap_values(instance_to_explain.values.reshape(1, -1))\n\n# Print SHAP values for each feature\nprint(\"SHAP Values:\")\nprint(shap_values)\n\nSHAP Values:\n[[ 0.02579396  0.00329706  0.          0.         -0.0606596   0.\n   0.         -0.01900188  0.         -0.081538    0.00779974  0.\n   0.         -0.0338136   0.          0.          0.          0.09000797\n   0.          0.          0.          0.          0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.          0.          0.01083979  0.          0.          0.02214676\n   0.          0.          0.          0.          0.          0.        ]]\n\n\n\n.629+.09-.08-.03-.06+.03+.02-.02+.02\n\n0.599\n\n\n\n# Choose the specific feature for which you want to calculate change\nobs_index = 0  # Replace with the index of the feature you're interested in\nshap_object  = explainer(X_test)\nshap.plots.waterfall(shap_object[obs_index])\n\n\n\n\n\n\n\n\n\n# Choose the specific feature for which you want to calculate change\nobs_index = 0  # Replace with the index of the feature you're interested in\nshap_object  = explainer(X_train)\nshap.plots.waterfall(shap_object[obs_index])\n\n\n\n\n\n\n\n\n\nX_train[0:]\n\n\n\n\n\n\n\n\nduration\ncredit_amount\ninstallment_rate\nresidence_since\nage\nexisting_credits\npeople_liable\nexisting_checking_A12\nexisting_checking_A13\nexisting_checking_A14\n...\nproperty_A124\nother_installment_plans_A142\nother_installment_plans_A143\nhousing_A152\nhousing_A153\njob_A172\njob_A173\njob_A174\ntelephone_A192\nforeign_worker_A202\n\n\n\n\n29\n60\n6836\n3\n4\n63\n2\n1\n0\n0\n0\n...\n1\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n535\n21\n2319\n2\n1\n33\n1\n1\n0\n1\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n695\n6\n1236\n2\n4\n50\n1\n1\n0\n0\n1\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n557\n21\n5003\n1\n4\n29\n2\n1\n0\n0\n1\n...\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n836\n12\n886\n4\n2\n21\n1\n1\n0\n0\n1\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n106\n18\n6458\n2\n4\n39\n2\n2\n0\n0\n1\n...\n1\n0\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n270\n18\n2662\n4\n3\n32\n1\n1\n0\n0\n1\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n\n\n860\n24\n5804\n4\n2\n27\n2\n1\n0\n0\n1\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n435\n12\n1484\n2\n1\n25\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n102\n6\n932\n3\n2\n24\n1\n1\n0\n0\n1\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n800 rows × 48 columns",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#so-sánh-giữa-các-giá-trị-shap-và-hệ-số-đối-với-hồi-quy-logistic",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#so-sánh-giữa-các-giá-trị-shap-và-hệ-số-đối-với-hồi-quy-logistic",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "So sánh giữa các giá trị SHAP và Hệ số đối với hồi quy logistic",
    "text": "So sánh giữa các giá trị SHAP và Hệ số đối với hồi quy logistic\n\n\n\n\n\n\n\n\nKhía cạnh\nGiá trị SHAP\nHệ số (Hồi quy logistic)\n\n\n\n\nMục đích\nGiải thích các dự đoán riêng lẻ, tác động của feature\nGiải thích mối quan hệ tổng thể của feature-log-odds\n\n\nKhả năng giải thích\nDễ hiểu trực tiếp hơn đối với các dự đoán riêng lẻ\nGiải thích thường yêu cầu ngữ cảnh\n\n\nHiệu ứng tương tác\nGhi lại các tương tác phức tạp giữa các features\nGiả sử tương tác tuyến tính (hạn chế)\n\n\nKhả năng ứng dụng\nÁp dụng cho nhiều mô hình khác nhau, kể cả những mô hình phức tạp\nCụ thể cho các mô hình tuyến tính như hồi quy logistic\n\n\nTrực quan hóa\nCó sẵn nhiều phương pháp trực quan hóa khác nhau để hiểu rõ hơn\nFeature importance\n\n\nSo sánh\nCó thể so sánh giữa các mô hình khác nhau\nCụ thể cho kiến trúc mô hình\n\n\nTính toán\nCó thể tốn kém về mặt tính toán đối với các mô hình phức tạp\nTính toán hiệu quả cho hồi quy logistic\n\n\nTính linh hoạt\nHỗ trợ tính phi tuyến tính, hữu ích cho các mô hình phức tạp\nGiả định mối quan hệ tuyến tính, kém linh hoạt\n\n\nChi tiết giải thích\nTác động feature chi tiết đến từng dự đoán\nTác động chung của feature lên log-odds",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#cách-giải-thích-hệ-số-mô-hình-logistic",
    "href": "machine-learning/SHAP_SHapley_Additive_exPlanations.html#cách-giải-thích-hệ-số-mô-hình-logistic",
    "title": "SHAP values and Coefficients of Logistic Regression",
    "section": "Cách giải thích hệ số mô hình logistic",
    "text": "Cách giải thích hệ số mô hình logistic\nCông thức của Logistic Regression có thể được biểu diễn dưới dạng log-odds như sau:\n\\[ \\text{log-odds} = \\ln \\left( \\frac{P(y=1 \\,|\\, \\mathbf{x})}{1 - P(y=1 \\,|\\, \\mathbf{x})} \\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p \\]\nTrong đó: - \\(P(y=1 \\,|\\, \\mathbf{x})\\) là xác suất có điều kiện rằng kết quả \\(y\\) bằng 1 dưới điều kiện các đặc trưng \\(\\mathbf{x}\\). - \\(\\beta_0\\) là hệ số chặn. - \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) là các hệ số cho các đặc trưng \\(x_1, x_2, \\ldots, x_p\\). - \\(\\ln\\) đại diện cho hàm logarit tự nhiên.\nBiểu thức \\(\\frac{P(y=1 \\,|\\, \\mathbf{x})}{1 - P(y=1 \\,|\\, \\mathbf{x})}\\) là tỷ lệ giữa xác suất nhóm 1 và xác suất nhóm 0, còn được gọi là tỷ lệ cơ hội (odds). Hàm logarit tự nhiên \\(\\ln\\) biến đổi tỷ lệ cơ hội thành giá trị log-odds.\nCông thức log-odds thể hiện mối liên hệ tuyến tính giữa các đặc trưng và log-odds của xác suất dương. Trong quá trình huấn luyện mô hình, các hệ số \\(\\beta_i\\) được điều chỉnh để tối ưu hóa khả năng dự đoán chính xác dựa trên dữ liệu huấn luyện.\nCác hệ số của mô hình hồi quy logistic biểu thị mối quan hệ giữa các biến độc lập (features) và tỷ lệ log-odds của biến phụ thuộc (kết quả nhị phân). Các hệ số này có thể được giải thích để hiểu những thay đổi trong features ảnh hưởng như thế nào đến xác suất của lớp dương (positive class).\nĐây là cách bạn có thể giải thích các hệ số của mô hình hồi quy logistic:\n\nHệ số dương:\n\nHệ số dương (ví dụ: \\(\\beta_1 &gt; 0\\)) chỉ ra rằng việc tăng giá trị biến độc lập tương ứng dẫn đến tăng tỷ lệ log-odds của lớp dương.\nHay là khi giá trị biến độc lập tăng thêm một đơn vị thì xác suất xuất hiện lớp dương cũng tăng.\n\nHệ số âm:\n\nHệ số âm (ví dụ: \\(\\beta_2 &lt; 0\\)) chỉ ra rằng việc tăng giá trị feature tương ứng dẫn đến giảm tỷ lệ log-odds của lớp dương.\nHay là khi giá trị biến độc lập tăng lên một đơn vị thì xác suất xuất hiện lớp dương sẽ giảm.\n\nĐộ lớn của các hệ số:\n\nĐộ lớn của các hệ số biểu thị mức độ mạnh của mối quan hệ giữa feature và outcome.\nGiá trị tuyệt đối lớn hơn cho thấy tác động mạnh hơn đến log-odds và do đó đến xác suất dự đoán.\n\nGiải thích về tỷ lệ Odds:\n\nVí dụ: nếu \\(\\beta_3 = 0.5\\), tỷ lệ chênh lệch là \\(e^{0.5} \\approx 1.648\\). Điều này có nghĩa là khi tính năng này tăng thêm một đơn vị, tỷ lệ xảy ra kết quả tích cực sẽ tăng theo hệ số xấp xỉ 1,648.\n\n\nDưới đây là ví dụ về cách bạn có thể giải thích hệ số dương: “Khi tăng một đơn vị trong biến ‘Tuổi’, tỷ lệ log của một khách hàng không trả được nợ sẽ tăng thêm 0,25. Điều này có nghĩa là khi độ tuổi của khách hàng tăng lên một năm, khả năng vỡ nợ cũng tăng lên.”\nVà một ví dụ về hệ số âm: “Đối với mỗi đơn vị tăng trong biến ‘Thu nhập’, tỷ lệ log của một khách hàng không trả được nợ sẽ giảm 0,15. Điều này cho thấy rằng mức thu nhập cao hơn có liên quan đến khả năng giảm vỡ nợ.”\nCông thức SHAP Value:\nCông thức chính của SHAP (SHapley Additive exPlanations) value cho một đặc trưng \\(i\\) đối với một dự đoán cụ thể là:\n\\[ SHAP_i = \\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} \\left[ f(x_S \\cup \\{i\\}) - f(x_S) \\right] \\]\nTrong đó: - \\(N\\) là tập hợp tất cả các đặc trưng. - \\(S\\) là một tập con của \\(N\\) không chứa đặc trưng \\(i\\). - \\(f(x_S)\\) là dự đoán của mô hình cho mẫu dữ liệu \\(x\\) khi chỉ giữ lại các đặc trưng trong tập \\(S\\). - \\(f(x_S \\cup \\{i\\})\\) là dự đoán của mô hình cho mẫu dữ liệu \\(x\\) khi thêm đặc trưng \\(i\\) vào tập \\(S\\).\nGiải thích SHAP Value:\nSHAP value giải thích sự đóng góp của từng đặc trưng vào giá trị dự đoán của mô hình cho một mẫu dữ liệu cụ thể. Đối với mỗi đặc trưng, SHAP value là sự khác biệt giữa giá trị dự đoán khi thêm đặc trưng đó và khi không thêm đặc trưng, được trung bình trên tất cả các tập con của các đặc trưng khác.\n\nNếu \\(SHAP_i &gt; 0\\), đặc trưng \\(i\\) có xu hướng tăng giá trị dự đoán so với kỳ vọng cơ bản (trung bình trên tất cả các tập con).\nNếu \\(SHAP_i &lt; 0\\), đặc trưng \\(i\\) có xu hướng giảm giá trị dự đoán so với kỳ vọng cơ bản.\nNếu \\(SHAP_i = 0\\), đặc trưng \\(i\\) không có sự đóng góp vào sự khác biệt giữa dự đoán thực tế và kỳ vọng cơ bản.\n\nSHAP value cho mỗi đặc trưng cung cấp cái nhìn về việc tại sao dự đoán cuối cùng lại có giá trị cụ thể. Bằng cách tổng hợp các SHAP value lại, bạn có thể hiểu được cách các đặc trưng ảnh hưởng đến dự đoán và làm thế nào chúng kết hợp lại để tạo ra giá trị dự đoán cho mẫu dữ liệu đó.",
    "crumbs": [
      "Machine learning",
      "SHAP values and Coefficients of Logistic Regression"
    ]
  },
  {
    "objectID": "machine-learning/Autoencoders.html",
    "href": "machine-learning/Autoencoders.html",
    "title": "Autoencoders - anomaly detection",
    "section": "",
    "text": "Bộ mã hóa tự động (Autoencoders) là một loại kiến trúc mạng thần kinh thường được sử dụng cho nhiều tác vụ khác nhau, bao gồm cả phát hiện sự bất thường. Chúng đặc biệt hiệu quả trong việc phát hiện sự bất thường không có giám sát, trong đó mô hình học cách tái tạo lại dữ liệu thông thường và xác định các điểm bất thường là các điểm dữ liệu sai lệch đáng kể so với phiên bản được xây dựng lại.\nSau đây là cách bộ mã hóa tự động hoạt động để phát hiện sự bất thường:\n\nKiến trúc: Bộ mã hóa tự động bao gồm bộ mã hóa và bộ giải mã (encoder - decoder). Bộ mã hóa nén dữ liệu đầu vào thành biểu diễn có chiều thấp hơn và bộ giải mã cố gắng tái tạo lại dữ liệu đầu vào ban đầu từ biểu diễn này.\nEncoder: Nén đầu vào thành một biểu diễn không gian ẩn. Lớp mã hóa mã hóa hình ảnh đầu vào dưới dạng biểu diễn nén ở kích thước giảm; bây giờ, ảnh nén trông giống ảnh gốc nhưng không giống ảnh gốc.\nDecoder: Bộ giải mã giải mã hình ảnh được mã hóa trở lại hình ảnh gốc có cùng kích thước. Bộ giải mã đưa dữ liệu từ không gian ẩn thấp hơn đến giai đoạn tái tạo, trong đó kích thước của đầu ra được giải mã bằng đầu ra X ban đầu. Nhưng nếu chúng ta xem nó là nén Ảnh thì sẽ có nén không mất dữ liệu, nhưng trong trường hợp Autoencoders, thông tin bị mất do nó nén và giải nén đầu vào. Khi giải nén, nó cố gắng đạt gần đầu vào, nhưng đầu ra không hoàn toàn giống ban đầu.\nKhông gian ẩn (Latent Space): Giảm số chiều của dữ liệu thấp hơn do bộ mã hóa tạo ra (thường được gọi là không gian ẩn hay nén dữ liệu). Ví dụ số chiều dữ liệu ban đầu là 10x10x1 sau khi giảm chiều hoặc nén lại còn 5x1 (kích thước latent space còn 5x1). Bây giờ, mỗi điểm dữ liệu 10x10x1 chỉ được xác định bởi 5 số\nLỗi tái tạo (Reconstruction Error): Do quá trình nén (giảm chiều) rồi tái tạo về số chiều ban đầu làm cho thông tin đầu vào và đầu ra bị khác nhau. Sai khác này được gọi là Lỗi tái tạo (Reconstruction Error). Các điểm bất thường được phát hiện dựa trên lỗi tái tạo của chúng. Với những điểm bất thường, lỗi tái tạo thường cao hơn nhiều so với dữ liệu thông thường.\nĐào tạo: Bộ mã hóa tự động thường được đào tạo trên tập dữ liệu chỉ chứa dữ liệu thông thường. Mục tiêu là giảm thiểu lỗi tái tạo, nghĩa là bộ mã hóa tự động sẽ có thể tái tạo lại dữ liệu bình thường một cách chính xác trong khi gặp khó khăn trong việc tái tạo lại các điểm bất thường.\nNgưỡng: Việc phát hiện bất thường bằng bộ mã hóa tự động thường liên quan đến việc đặt ngưỡng cho lỗi tái tạo. Điểm dữ liệu có lỗi tái tạo vượt quá ngưỡng được coi là bất thường.\n\nBộ mã hóa tự động có một số lợi thế để phát hiện sự bất thường:\n\nChúng có thể nắm bắt các mẫu phức tạp trong dữ liệu, khiến chúng phù hợp với dữ liệu nhiều chiều.\nChúng không được giám sát nên không yêu cầu dữ liệu bất thường được dán nhãn để huấn luyện.\nChúng có khả năng thích ứng với các loại dữ liệu khác nhau, bao gồm dữ liệu số và hình ảnh.",
    "crumbs": [
      "Machine learning",
      "Autoencoders - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/Autoencoders.html#autoencoders",
    "href": "machine-learning/Autoencoders.html#autoencoders",
    "title": "Autoencoders - anomaly detection",
    "section": "",
    "text": "Bộ mã hóa tự động (Autoencoders) là một loại kiến trúc mạng thần kinh thường được sử dụng cho nhiều tác vụ khác nhau, bao gồm cả phát hiện sự bất thường. Chúng đặc biệt hiệu quả trong việc phát hiện sự bất thường không có giám sát, trong đó mô hình học cách tái tạo lại dữ liệu thông thường và xác định các điểm bất thường là các điểm dữ liệu sai lệch đáng kể so với phiên bản được xây dựng lại.\nSau đây là cách bộ mã hóa tự động hoạt động để phát hiện sự bất thường:\n\nKiến trúc: Bộ mã hóa tự động bao gồm bộ mã hóa và bộ giải mã (encoder - decoder). Bộ mã hóa nén dữ liệu đầu vào thành biểu diễn có chiều thấp hơn và bộ giải mã cố gắng tái tạo lại dữ liệu đầu vào ban đầu từ biểu diễn này.\nEncoder: Nén đầu vào thành một biểu diễn không gian ẩn. Lớp mã hóa mã hóa hình ảnh đầu vào dưới dạng biểu diễn nén ở kích thước giảm; bây giờ, ảnh nén trông giống ảnh gốc nhưng không giống ảnh gốc.\nDecoder: Bộ giải mã giải mã hình ảnh được mã hóa trở lại hình ảnh gốc có cùng kích thước. Bộ giải mã đưa dữ liệu từ không gian ẩn thấp hơn đến giai đoạn tái tạo, trong đó kích thước của đầu ra được giải mã bằng đầu ra X ban đầu. Nhưng nếu chúng ta xem nó là nén Ảnh thì sẽ có nén không mất dữ liệu, nhưng trong trường hợp Autoencoders, thông tin bị mất do nó nén và giải nén đầu vào. Khi giải nén, nó cố gắng đạt gần đầu vào, nhưng đầu ra không hoàn toàn giống ban đầu.\nKhông gian ẩn (Latent Space): Giảm số chiều của dữ liệu thấp hơn do bộ mã hóa tạo ra (thường được gọi là không gian ẩn hay nén dữ liệu). Ví dụ số chiều dữ liệu ban đầu là 10x10x1 sau khi giảm chiều hoặc nén lại còn 5x1 (kích thước latent space còn 5x1). Bây giờ, mỗi điểm dữ liệu 10x10x1 chỉ được xác định bởi 5 số\nLỗi tái tạo (Reconstruction Error): Do quá trình nén (giảm chiều) rồi tái tạo về số chiều ban đầu làm cho thông tin đầu vào và đầu ra bị khác nhau. Sai khác này được gọi là Lỗi tái tạo (Reconstruction Error). Các điểm bất thường được phát hiện dựa trên lỗi tái tạo của chúng. Với những điểm bất thường, lỗi tái tạo thường cao hơn nhiều so với dữ liệu thông thường.\nĐào tạo: Bộ mã hóa tự động thường được đào tạo trên tập dữ liệu chỉ chứa dữ liệu thông thường. Mục tiêu là giảm thiểu lỗi tái tạo, nghĩa là bộ mã hóa tự động sẽ có thể tái tạo lại dữ liệu bình thường một cách chính xác trong khi gặp khó khăn trong việc tái tạo lại các điểm bất thường.\nNgưỡng: Việc phát hiện bất thường bằng bộ mã hóa tự động thường liên quan đến việc đặt ngưỡng cho lỗi tái tạo. Điểm dữ liệu có lỗi tái tạo vượt quá ngưỡng được coi là bất thường.\n\nBộ mã hóa tự động có một số lợi thế để phát hiện sự bất thường:\n\nChúng có thể nắm bắt các mẫu phức tạp trong dữ liệu, khiến chúng phù hợp với dữ liệu nhiều chiều.\nChúng không được giám sát nên không yêu cầu dữ liệu bất thường được dán nhãn để huấn luyện.\nChúng có khả năng thích ứng với các loại dữ liệu khác nhau, bao gồm dữ liệu số và hình ảnh.",
    "crumbs": [
      "Machine learning",
      "Autoencoders - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/Autoencoders.html#so-sánh-autoencoders-với-pca-phân-tích-thành-phần-chính-và-isolation-forest-trong-phát-hiện-bất-thường",
    "href": "machine-learning/Autoencoders.html#so-sánh-autoencoders-với-pca-phân-tích-thành-phần-chính-và-isolation-forest-trong-phát-hiện-bất-thường",
    "title": "Autoencoders - anomaly detection",
    "section": "So sánh Autoencoders với PCA (Phân tích thành phần chính) và Isolation Forest trong phát hiện bất thường:",
    "text": "So sánh Autoencoders với PCA (Phân tích thành phần chính) và Isolation Forest trong phát hiện bất thường:\n\nAutoencoders:\n\nThuận lợi:\n\nCó thể nắm bắt các mẫu phức tạp, phi tuyến trong dữ liệu.\nThích hợp cho dữ liệu nhiều chiều.\nCó thể tinh chỉnh cho các nhiệm vụ cụ thể.\n\nNhược điểm:\n\nYêu cầu nhiều dữ liệu và thời gian huấn luyện hơn so với PCA.\nCó thể cần phải điều chỉnh siêu tham số.\nCó thể bị overfit nếu không được điều chỉnh đúng cách.\n\n\nPCA:\n\nThuận lợi:\n\nĐơn giản và hiệu quả tính toán.\nHữu ích cho việc giảm kích thước và trực quan hóa.\nCung cấp các tính năng trực giao, có thể nắm bắt một số mẫu.\n\nNhược điểm:\n\nPhương pháp tuyến tính; có thể không nắm bắt được các mẫu phức tạp, phi tuyến.\nGiả sử dữ liệu được phân phối Gaussian.\nKhông có tiêu chí cụ thể về dị thường; dựa vào các thuộc tính thống kê.\n\n\nIsolation Forest:\n\nThuận lợi:\n\nĐược thiết kế riêng cho mục đích phát hiện sự bất thường.\nHoạt động tốt với cả dữ liệu có nhiều chiều và ít chiều.\nCó thể xử lý các kiểu dữ liệu hỗn hợp (số và phân loại).\n\nNhược điểm:\n\nDễ bị overfitting trên các tập dữ liệu nhỏ.\nCó thể yêu cầu tinh chỉnh các siêu tham số.\nKhông giải thích được tại sao trường hợp cụ thể lại bất thường\n\n\n\nCác yếu tố so sánh cần xem xét khi lựa chọn phương pháp cho nhiệm vụ phát hiện dấu vết bất thường của bạn:\n\nTính phức tạp của Dữ liệu: Autoencoders phù hợp cho dữ liệu phức tạp, phi tuyến tính, trong khi PCA và Isolation Forest thích hợp cho dữ liệu đơn giản, tuyến tính hơn.\nSố chiều: Nếu làm việc với dữ liệu có số chiều cao, autoencoders và PCA có thể hữu ích hơn. PCA đặc biệt tốt cho việc giảm chiều dữ liệu.\nKhả năng diễn giải: PCA cung cấp biến đổi đặc trưng và điểm số thành phần có thể diễn giải. Autoencoders và Isolation Forest không cung cấp điều này theo cách tự nhiên.\nTính toán: PCA hiệu quả tính toán nhất, theo sau là Isolation Forest. Autoencoders yêu cầu nhiều tài nguyên tính toán hơn cho quá trình đào tạo.\nKích thước Dữ liệu: Isolation Forest có thể gây overfitting trên tập dữ liệu nhỏ hơn. Autoencoders và PCA có thể mạnh mẽ hơn đối với tập dữ liệu nhỏ.\nLoại đặc trưng: Nếu dữ liệu của bạn bao gồm cả số và phân loại, Isolation Forest phù hợp hơn vì nó có thể xử lý cả hai loại dữ liệu. Autoencoders và PCA thường được sử dụng cho dữ liệu số.\nSự có sẵn của Dữ liệu Đào tạo: Autoencoders yêu cầu nhiều dữ liệu đào tạo hơn so với Isolation Forest và PCA, có thể xử lý các tập dữ liệu nhỏ hơn.\nTinh chỉnh siêu tham số: Tất cả các phương pháp có thể yêu cầu một mức độ tinh chỉnh siêu tham số, nhưng autoencoders có thể yêu cầu nhiều hơn.\nMục đích Sử dụng: Xem xét yêu cầu cụ thể và tính chất của vấn đề phát hiện dấu vết bất thường của bạn. Mỗi phương pháp có thể hoạt động tố\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\n# Load the German Credit Data (you can download it from the source)\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\n\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Convert categorical columns to one-hot encoding\ncategorical_columns = ['existing_checking', 'credit_history', 'purpose', 'savings', 'employment',\n                       'personal_status', 'other_debtors', 'property', 'other_installment_plans',\n                       'housing', 'job', 'telephone', 'foreign_worker']\n\ndata = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n\n# Split the data into features and labels\nX = data.drop('class', axis=1).values\ny = data['class'].values\n\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Define and train the autoencoder model\ninput_dim = X.shape[1]\nencoding_dim = 10  # Number of neurons in the encoding layer\n\ninput_layer = Input(shape=(input_dim,))\nencoded = Dense(encoding_dim, activation='relu')(input_layer)\ndecoded = Dense(input_dim, activation='linear')(encoded)\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')\n\nautoencoder.fit(X, X, epochs=50, batch_size=64, shuffle=True)\n\n# Evaluate the autoencoder model on the entire dataset\nreconstruction_errors = np.mean(np.square(X - autoencoder.predict(X)), axis=1)\n\n# Set a threshold for anomalies\nthreshold = np.percentile(reconstruction_errors, 95)  # Example threshold (adjust as needed)\n\n# Detect anomalies for the entire dataset\nanomalies_detected = reconstruction_errors &gt; threshold\n\n# Print the number of anomalies detected\nprint(f'Anomalies detected: {np.sum(anomalies_detected)} out of {len(X)}')\n\nEpoch 1/50\n16/16 [==============================] - 0s 2ms/step - loss: 1.1801\nEpoch 2/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.1139\nEpoch 3/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.0666\nEpoch 4/50\n16/16 [==============================] - 0s 2ms/step - loss: 1.0310\nEpoch 5/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.0032\nEpoch 6/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9797\nEpoch 7/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9594\nEpoch 8/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9411\nEpoch 9/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9241\nEpoch 10/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9077\nEpoch 11/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8921\nEpoch 12/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8774\nEpoch 13/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8631\nEpoch 14/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.8497\nEpoch 15/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.8370\nEpoch 16/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8249\nEpoch 17/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8138\nEpoch 18/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8033\nEpoch 19/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7934\nEpoch 20/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7843\nEpoch 21/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.7759\nEpoch 22/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7681\nEpoch 23/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7609\nEpoch 24/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7544\nEpoch 25/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7483\nEpoch 26/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7425\nEpoch 27/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7373\nEpoch 28/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7322\nEpoch 29/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7277\nEpoch 30/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7234\nEpoch 31/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7193\nEpoch 32/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7154\nEpoch 33/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7118\nEpoch 34/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7082\nEpoch 35/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7049\nEpoch 36/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7017\nEpoch 37/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6988\nEpoch 38/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6958\nEpoch 39/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6931\nEpoch 40/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6904\nEpoch 41/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6878\nEpoch 42/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6853\nEpoch 43/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6828\nEpoch 44/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6804\nEpoch 45/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6781\nEpoch 46/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6758\nEpoch 47/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6737\nEpoch 48/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.6716\nEpoch 49/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.6697\nEpoch 50/50\n16/16 [==============================] - 0s 10ms/step - loss: 0.6675\n32/32 [==============================] - 0s 936us/step\nAnomalies detected: 50 out of 1000\n\n\n\n# Select the rows of data that are anomalies\nanomalous_data = data[anomalies_detected]\nanomalous_data\n\n\n\n\n\n\n\n\nduration\ncredit_amount\ninstallment_rate\nresidence_since\nage\nexisting_credits\npeople_liable\nclass\nexisting_checking_A12\nexisting_checking_A13\n...\nproperty_A124\nother_installment_plans_A142\nother_installment_plans_A143\nhousing_A152\nhousing_A153\njob_A172\njob_A173\njob_A174\ntelephone_A192\nforeign_worker_A202\n\n\n\n\n38\n10\n1225\n2\n2\n37\n1\n1\n1\n0\n1\n...\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n42\n18\n6204\n2\n4\n44\n1\n2\n1\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n65\n27\n5190\n4\n4\n48\n4\n2\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n72\n8\n1164\n3\n4\n51\n2\n2\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n83\n24\n1755\n4\n4\n58\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n105\n24\n11938\n2\n3\n39\n2\n2\n2\n1\n0\n...\n0\n0\n1\n1\n0\n0\n0\n1\n1\n0\n\n\n156\n9\n1288\n3\n4\n48\n2\n2\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n\n\n157\n12\n339\n4\n1\n45\n1\n1\n1\n0\n0\n...\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n162\n15\n1262\n4\n3\n36\n2\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n203\n12\n902\n4\n4\n21\n1\n1\n2\n0\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n207\n12\n1424\n4\n3\n26\n1\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n209\n12\n1413\n3\n2\n55\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n\n\n215\n6\n932\n1\n3\n39\n2\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n\n\n226\n48\n10961\n1\n2\n27\n2\n1\n2\n1\n0\n...\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n244\n12\n3447\n4\n3\n35\n1\n2\n1\n0\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n\n\n274\n30\n11998\n1\n1\n34\n1\n1\n2\n0\n0\n...\n1\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n287\n48\n7582\n2\n4\n31\n1\n1\n1\n1\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n310\n48\n5381\n3\n4\n40\n1\n1\n1\n1\n0\n...\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n337\n15\n1275\n4\n2\n24\n1\n1\n2\n0\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n350\n9\n1236\n1\n4\n23\n1\n1\n1\n0\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n\n\n374\n60\n14782\n3\n4\n60\n2\n1\n2\n1\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n429\n18\n1190\n2\n4\n55\n3\n2\n2\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n431\n24\n11328\n2\n3\n29\n2\n1\n2\n1\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n436\n6\n660\n2\n4\n23\n1\n1\n1\n0\n0\n...\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n438\n42\n3394\n4\n4\n65\n2\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n442\n20\n2629\n2\n3\n29\n2\n1\n1\n1\n0\n...\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n449\n15\n1512\n3\n3\n61\n2\n1\n2\n1\n0\n...\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n458\n6\n343\n4\n1\n27\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n463\n12\n754\n4\n4\n38\n2\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n473\n6\n1238\n4\n4\n36\n1\n2\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n0\n1\n1\n0\n\n\n579\n24\n937\n4\n3\n27\n2\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n\n\n583\n36\n2384\n4\n1\n33\n1\n1\n2\n1\n0\n...\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n588\n18\n1217\n4\n3\n47\n1\n1\n2\n0\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n594\n24\n1358\n4\n3\n40\n1\n1\n2\n0\n0\n...\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n610\n12\n741\n4\n3\n22\n1\n1\n2\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n613\n24\n3632\n1\n4\n22\n1\n1\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n665\n24\n6314\n4\n2\n27\n2\n1\n1\n0\n0\n...\n1\n0\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n666\n30\n3496\n4\n2\n34\n1\n2\n1\n1\n0\n...\n0\n1\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n721\n6\n433\n4\n2\n24\n1\n2\n2\n1\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n735\n36\n3990\n3\n2\n29\n1\n1\n1\n1\n0\n...\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n754\n12\n1555\n4\n4\n55\n2\n2\n2\n0\n0\n...\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n\n\n782\n12\n1410\n2\n2\n31\n1\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n813\n48\n3051\n3\n4\n54\n1\n1\n2\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n816\n6\n1338\n1\n4\n62\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n818\n36\n15857\n2\n3\n43\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n856\n10\n894\n4\n3\n40\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n\n\n873\n15\n874\n4\n1\n24\n1\n1\n1\n0\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n915\n48\n18424\n1\n2\n32\n1\n1\n2\n1\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n\n\n964\n6\n454\n3\n1\n22\n1\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n\n\n970\n15\n1514\n4\n2\n22\n1\n1\n1\n1\n0\n...\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n50 rows × 49 columns\n\n\n\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define and train the autoencoder model\ninput_dim = X_train.shape[1]\nencoding_dim = 10  # Number of neurons in the encoding layer\n\ninput_layer = Input(shape=(input_dim,))\nencoded = Dense(encoding_dim, activation='relu')(input_layer)\ndecoded = Dense(input_dim, activation='linear')(encoded)\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')\n\nautoencoder.fit(X_train, X_train, epochs=50, batch_size=64, shuffle=True, validation_data=(X_test, X_test))\n\n# Evaluate the autoencoder model on the test set\nreconstruction_errors = np.mean(np.square(X_test - autoencoder.predict(X_test)), axis=1)\n\n# Set a threshold for anomalies\nthreshold = np.percentile(reconstruction_errors, 95)  # Example threshold (adjust as needed)\n\n# Detect anomalies\nanomalies_detected = reconstruction_errors &gt; threshold\n\n# Print the number of anomalies detected\nprint(f'Anomalies detected: {np.sum(anomalies_detected)} out of {len(X_test)}')\n\nEpoch 1/50\n13/13 [==============================] - 0s 11ms/step - loss: 1.2654 - val_loss: 1.1622\nEpoch 2/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.1879 - val_loss: 1.1093\nEpoch 3/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.1322 - val_loss: 1.0692\nEpoch 4/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.0886 - val_loss: 1.0381\nEpoch 5/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.0551 - val_loss: 1.0139\nEpoch 6/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.0286 - val_loss: 0.9934\nEpoch 7/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.0062 - val_loss: 0.9760\nEpoch 8/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.9867 - val_loss: 0.9606\nEpoch 9/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.9696 - val_loss: 0.9468\nEpoch 10/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.9543 - val_loss: 0.9342\nEpoch 11/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.9399 - val_loss: 0.9223\nEpoch 12/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.9263 - val_loss: 0.9112\nEpoch 13/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.9133 - val_loss: 0.9005\nEpoch 14/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.9009 - val_loss: 0.8902\nEpoch 15/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8892 - val_loss: 0.8801\nEpoch 16/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.8779 - val_loss: 0.8707\nEpoch 17/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.8670 - val_loss: 0.8614\nEpoch 18/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.8566 - val_loss: 0.8528\nEpoch 19/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.8468 - val_loss: 0.8442\nEpoch 20/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8375 - val_loss: 0.8360\nEpoch 21/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8283 - val_loss: 0.8283\nEpoch 22/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8197 - val_loss: 0.8211\nEpoch 23/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8116 - val_loss: 0.8140\nEpoch 24/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.8040 - val_loss: 0.8074\nEpoch 25/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7966 - val_loss: 0.8010\nEpoch 26/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7898 - val_loss: 0.7953\nEpoch 27/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7833 - val_loss: 0.7896\nEpoch 28/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7771 - val_loss: 0.7845\nEpoch 29/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7714 - val_loss: 0.7796\nEpoch 30/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7658 - val_loss: 0.7751\nEpoch 31/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7608 - val_loss: 0.7706\nEpoch 32/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7560 - val_loss: 0.7665\nEpoch 33/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7515 - val_loss: 0.7628\nEpoch 34/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7473 - val_loss: 0.7590\nEpoch 35/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7431 - val_loss: 0.7558\nEpoch 36/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7393 - val_loss: 0.7526\nEpoch 37/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7356 - val_loss: 0.7495\nEpoch 38/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7322 - val_loss: 0.7468\nEpoch 39/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7289 - val_loss: 0.7437\nEpoch 40/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7259 - val_loss: 0.7413\nEpoch 41/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7228 - val_loss: 0.7385\nEpoch 42/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7201 - val_loss: 0.7362\nEpoch 43/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7174 - val_loss: 0.7340\nEpoch 44/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7149 - val_loss: 0.7318\nEpoch 45/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.7123 - val_loss: 0.7297\nEpoch 46/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7100 - val_loss: 0.7275\nEpoch 47/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7078 - val_loss: 0.7256\nEpoch 48/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7056 - val_loss: 0.7235\nEpoch 49/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7035 - val_loss: 0.7215\nEpoch 50/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7197\n7/7 [==============================] - 0s 1ms/step\nAnomalies detected: 10 out of 200\n\n\n\n# Split the data into features and labels\nX = data.drop('class', axis=1).values\ny = data['class'].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Define and train the autoencoder model\ninput_dim = X.shape[1]\nencoding_dim = 10  # Number of neurons in the encoding layer\n\ninput_layer = Input(shape=(input_dim,))\nencoded = Dense(encoding_dim, activation='relu')(input_layer)\ndecoded = Dense(input_dim, activation='linear')(encoded)\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')\n\nautoencoder.fit(X, X, epochs=50, batch_size=64, shuffle=True)\n\n# Evaluate the autoencoder model on the entire dataset\nreconstruction_errors = np.mean(np.square(X - autoencoder.predict(X)), axis=1)\n\n# Set a threshold for anomalies\nthreshold = np.percentile(reconstruction_errors, 95)  # Example threshold (adjust as needed)\n\n# Detect anomalies for the entire dataset\nanomalies_detected = reconstruction_errors &gt; threshold\n\n# Print the number of anomalies detected\nprint(f'Anomalies detected: {np.sum(anomalies_detected)} out of {len(X)}')\n\nEpoch 1/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.2795\nEpoch 2/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.1882\nEpoch 3/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.1245\nEpoch 4/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.0789\nEpoch 5/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.0441\nEpoch 6/50\n16/16 [==============================] - 0s 1ms/step - loss: 1.0170\nEpoch 7/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9944\nEpoch 8/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9754\nEpoch 9/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9588\nEpoch 10/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9433\nEpoch 11/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9291\nEpoch 12/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9158\nEpoch 13/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.9028\nEpoch 14/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8905\nEpoch 15/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.8784\nEpoch 16/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8665\nEpoch 17/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8548\nEpoch 18/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8433\nEpoch 19/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8321\nEpoch 20/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.8211\nEpoch 21/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.8105\nEpoch 22/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.8005\nEpoch 23/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7909\nEpoch 24/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7817\nEpoch 25/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7733\nEpoch 26/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7654\nEpoch 27/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7583\nEpoch 28/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7516\nEpoch 29/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7455\nEpoch 30/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.7400\nEpoch 31/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7346\nEpoch 32/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7297\nEpoch 33/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7253\nEpoch 34/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.7211\nEpoch 35/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7170\nEpoch 36/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7133\nEpoch 37/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7099\nEpoch 38/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7067\nEpoch 39/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7036\nEpoch 40/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.7006\nEpoch 41/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6979\nEpoch 42/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6952\nEpoch 43/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6927\nEpoch 44/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6903\nEpoch 45/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6881\nEpoch 46/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6860\nEpoch 47/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6839\nEpoch 48/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.6819\nEpoch 49/50\n16/16 [==============================] - 0s 2ms/step - loss: 0.6800\nEpoch 50/50\n16/16 [==============================] - 0s 1ms/step - loss: 0.6782\n32/32 [==============================] - 0s 935us/step\nAnomalies detected: 50 out of 1000",
    "crumbs": [
      "Machine learning",
      "Autoencoders - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/deep_dive_stacking_model_multi_class.html",
    "href": "machine-learning/deep_dive_stacking_model_multi_class.html",
    "title": "Deep Dive into Stacking Model for Multi-Class Classification",
    "section": "",
    "text": "This notebook demonstrates how to implement a stacking model for multi-class classification using configurations for multiple base models and a meta-model (final estimator). The stacking approach combines predictions from multiple base models to improve performance.",
    "crumbs": [
      "Machine learning",
      "Deep Dive into Stacking Model for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/deep_dive_stacking_model_multi_class.html#overview",
    "href": "machine-learning/deep_dive_stacking_model_multi_class.html#overview",
    "title": "Deep Dive into Stacking Model for Multi-Class Classification",
    "section": "",
    "text": "This notebook demonstrates how to implement a stacking model for multi-class classification using configurations for multiple base models and a meta-model (final estimator). The stacking approach combines predictions from multiple base models to improve performance.",
    "crumbs": [
      "Machine learning",
      "Deep Dive into Stacking Model for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/deep_dive_stacking_model_multi_class.html#step-1-import-libraries",
    "href": "machine-learning/deep_dive_stacking_model_multi_class.html#step-1-import-libraries",
    "title": "Deep Dive into Stacking Model for Multi-Class Classification",
    "section": "Step 1: Import Libraries",
    "text": "Step 1: Import Libraries\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder",
    "crumbs": [
      "Machine learning",
      "Deep Dive into Stacking Model for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/deep_dive_stacking_model_multi_class.html#step-2-load-wine-quality-data",
    "href": "machine-learning/deep_dive_stacking_model_multi_class.html#step-2-load-wine-quality-data",
    "title": "Deep Dive into Stacking Model for Multi-Class Classification",
    "section": "Step 2: Load Wine Quality Data",
    "text": "Step 2: Load Wine Quality Data\n\n# Load the wine quality dataset\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\ndata = pd.read_csv(url, sep=';')\n\n# Convert target to multi-class (low, medium, high quality)\ndef quality_label(q):\n    if q &lt;= 5:\n        return 'low'\n    elif q == 6:\n        return 'medium'\n    else:\n        return 'high'\n\ndata['quality_label'] = data['quality'].apply(quality_label)\ndata.drop(columns=['quality'], inplace=True)\n\n# Encode target labels\nlabel_encoder = LabelEncoder()\ndata['quality_label'] = label_encoder.fit_transform(data['quality_label'])\n\n# Features and target\nX = data.drop(columns=['quality_label'])\ny = data['quality_label']",
    "crumbs": [
      "Machine learning",
      "Deep Dive into Stacking Model for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/deep_dive_stacking_model_multi_class.html#step-3-define-model-configurations",
    "href": "machine-learning/deep_dive_stacking_model_multi_class.html#step-3-define-model-configurations",
    "title": "Deep Dive into Stacking Model for Multi-Class Classification",
    "section": "Step 3: Define Model Configurations",
    "text": "Step 3: Define Model Configurations\n\nmodel_configs = {\n    'model_1': {\n        'feature_names': ['fixed acidity', 'volatile acidity'],\n        'hyperparameters': {'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 100},\n        'estimators': LogisticRegression\n    },\n    'model_2': {\n        'feature_names': ['citric acid', 'residual sugar'],\n        'hyperparameters': {'n_estimators': 200, 'learning_rate': 0.05, 'verbosity': -1},\n        'estimators': lgb.LGBMClassifier\n    },\n    'model_3': {\n        'feature_names': ['chlorides', 'free sulfur dioxide'],\n        'hyperparameters': None,\n        'estimators': None\n    }\n}\n\nfinal_estimator = lgb.LGBMClassifier(n_estimators=50, learning_rate=0.1, verbosity = -1)",
    "crumbs": [
      "Machine learning",
      "Deep Dive into Stacking Model for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/deep_dive_stacking_model_multi_class.html#step-4-define-the-stack-model-class",
    "href": "machine-learning/deep_dive_stacking_model_multi_class.html#step-4-define-the-stack-model-class",
    "title": "Deep Dive into Stacking Model for Multi-Class Classification",
    "section": "Step 4: Define the Stack Model Class",
    "text": "Step 4: Define the Stack Model Class\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.base import BaseEstimator\nimport numpy as np\nimport pandas as pd\n\nclass StackModel:\n    def __init__(self, model_configs, final_estimator, cv=None):\n        \"\"\"\n        Initialize the stacking model.\n\n        Args:\n            model_configs (dict): Configuration for base models. Each key is a model name, and \n                the value is a dictionary with the following keys:\n                - 'feature_names': List of feature names used by the model.\n                - 'estimators': The model class (e.g., sklearn classifier or regressor).\n                - 'hyperparameters': Dictionary of hyperparameters for the estimator.\n            final_estimator (BaseEstimator): Meta-model for stacking.\n            cv (int, cross-validation generator, or None): Cross-validation strategy for \n                generating meta-features. Default is None (5-fold CV).\n        \"\"\"\n        self.model_configs = model_configs\n        self.final_estimator = final_estimator\n        self.cv = cv or 5\n        self.models = {}\n\n    def fit(self, X, y):\n        \"\"\"\n        Train the stacking model.\n\n        Args:\n            X (pd.DataFrame): Feature matrix.\n            y (pd.Series or np.ndarray): Target vector.\n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"X must be a pandas DataFrame.\")\n        if not isinstance(y, (pd.Series, np.ndarray)):\n            raise ValueError(\"y must be a pandas Series or a numpy array.\")\n\n        self.models = {}\n        meta_features_list = []\n\n        # Train base models and generate cross-validated meta-features\n        for model_name, config in self.model_configs.items():\n            features = config['feature_names']\n            if config['estimators'] is not None:\n                estimator = config['estimators'](**config['hyperparameters'])\n                \n                # Generate cross-validated meta-features\n                meta_features = cross_val_predict(\n                    estimator, X[features], y, cv=self.cv, method='predict_proba'\n                )\n                meta_features_list.append(meta_features)\n\n                # Train the model on the full dataset\n                estimator.fit(X[features], y)\n\n                self.models[model_name] = {\n                    'features': features,\n                    'model': estimator\n                }\n            else:\n                # Use raw features directly for models without estimators\n                meta_features = X[features].values\n                meta_features_list.append(meta_features)\n\n                self.models[model_name] = {\n                    'features': features,\n                    'model': None\n                }\n\n        # Combine all meta-features\n        self.meta_features = np.hstack(meta_features_list)\n\n        # Train the final estimator using meta-features\n        self.final_estimator.fit(self.meta_features, y)\n\n    def predict(self, X):\n        \"\"\"\n        Predict class labels using the stacking model.\n\n        Args:\n            X (pd.DataFrame): Feature matrix.\n\n        Returns:\n            np.ndarray: Predicted class labels.\n        \"\"\"\n        meta_features = self.transform(X)\n        return self.final_estimator.predict(meta_features)\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict probabilities using the stacking model.\n\n        Args:\n            X (pd.DataFrame): Feature matrix.\n\n        Returns:\n            np.ndarray: Predicted probabilities.\n        \"\"\"\n        meta_features = self.transform(X)\n        return self.final_estimator.predict_proba(meta_features)\n\n    def transform(self, X):\n        \"\"\"\n        Generate meta-features for a given dataset, transforming the input features.\n\n        Args:\n            X (pd.DataFrame): Feature matrix.\n\n        Returns:\n            np.ndarray: Transformed meta-features as a numpy array.\n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"X must be a pandas DataFrame.\")\n\n        meta_features = []\n\n        for model_name, model_info in self.models.items():\n            features = model_info['features']\n            if model_info['model'] is not None:\n                model = model_info['model']\n                meta_features.append(model.predict_proba(X[features]))\n            else:\n                meta_features.append(X[features].values)\n\n        return np.hstack(meta_features)",
    "crumbs": [
      "Machine learning",
      "Deep Dive into Stacking Model for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/deep_dive_stacking_model_multi_class.html#step-5-train-and-evaluate-the-stack-model",
    "href": "machine-learning/deep_dive_stacking_model_multi_class.html#step-5-train-and-evaluate-the-stack-model",
    "title": "Deep Dive into Stacking Model for Multi-Class Classification",
    "section": "Step 5: Train and Evaluate the Stack Model",
    "text": "Step 5: Train and Evaluate the Stack Model\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize and train the model\nstack_model = StackModel(model_configs, final_estimator)\nstack_model.fit(X_train, y_train)\n\n# Predict and evaluate\npredictions = stack_model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n\nprint(f\"Accuracy of the stacking model: {accuracy:.4f}\")\n\nAccuracy of the stacking model: 0.5583\n\n\n\n# Export meta-features for inspection\nmeta_features_train = stack_model.transform(X_train)\nmeta_features_test = stack_model.transform(X_test)\npd.DataFrame(meta_features_train)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n0\n0.324457\n0.215730\n0.459813\n0.411349\n0.134452\n0.454199\n0.064\n53.0\n\n\n1\n0.185403\n0.419977\n0.394620\n0.036433\n0.646075\n0.317492\n0.071\n6.0\n\n\n2\n0.102198\n0.483839\n0.413963\n0.000267\n0.861533\n0.138200\n0.084\n12.0\n\n\n3\n0.055714\n0.592318\n0.351968\n0.004361\n0.420008\n0.575631\n0.045\n19.0\n\n\n4\n0.088008\n0.520087\n0.391905\n0.086474\n0.548249\n0.365277\n0.077\n27.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1114\n0.088350\n0.542695\n0.368955\n0.010300\n0.650717\n0.338983\n0.058\n5.0\n\n\n1115\n0.072132\n0.569988\n0.357881\n0.008834\n0.430833\n0.560332\n0.073\n25.0\n\n\n1116\n0.072850\n0.553664\n0.373486\n0.019392\n0.769255\n0.211352\n0.077\n15.0\n\n\n1117\n0.330940\n0.202417\n0.466644\n0.542588\n0.052167\n0.405246\n0.054\n7.0\n\n\n1118\n0.231645\n0.267904\n0.500451\n0.006196\n0.557755\n0.436049\n0.063\n3.0\n\n\n\n\n1119 rows × 8 columns",
    "crumbs": [
      "Machine learning",
      "Deep Dive into Stacking Model for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/deep_dive_stacking_model_multi_class.html#step-6-compare-the-results-of-stackingclassifier-from-sklearn-and-the-custom-stackmodel",
    "href": "machine-learning/deep_dive_stacking_model_multi_class.html#step-6-compare-the-results-of-stackingclassifier-from-sklearn-and-the-custom-stackmodel",
    "title": "Deep Dive into Stacking Model for Multi-Class Classification",
    "section": "Step 6: Compare the results of StackingClassifier (from sklearn) and the custom StackModel",
    "text": "Step 6: Compare the results of StackingClassifier (from sklearn) and the custom StackModel\n\nStep 6.1: Compare passthrough\n\nmodel_configs_passthrough = {\n    'model_1': model_configs['model_1'],\n    'model_2': model_configs['model_2']\n}\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.dummy import DummyClassifier\n\n# Define feature selectors for each model\nmodel_1_features = model_configs_passthrough['model_1']['feature_names']\nmodel_2_features = model_configs_passthrough['model_2']['feature_names']\n\n# Create pipelines for each base model\nmodel_1_pipeline = Pipeline([\n    ('selector', ColumnTransformer([('select', 'passthrough', model_1_features)])),\n    ('model', LogisticRegression(**model_configs_passthrough['model_1']['hyperparameters']))\n])\n\nmodel_2_pipeline = Pipeline([\n    ('selector', ColumnTransformer([('select', 'passthrough', model_2_features)])),\n    ('model', lgb.LGBMClassifier(**model_configs_passthrough['model_2']['hyperparameters']))\n])\n\n\n# Define base models\nbase_models = [\n    ('model_1', model_1_pipeline),\n    ('model_2', model_2_pipeline)\n]\n\n# Define the stacking classifier\nstacking_clf = StackingClassifier(\n    estimators=base_models,\n    final_estimator=final_estimator,\n    passthrough=False\n)\n\n\nfrom sklearn.metrics import classification_report\n\n# Custom StackModel\nstack_model = StackModel(model_configs_passthrough, final_estimator)\nstack_model.fit(X_train, y_train)\n\n# Predict and evaluate using the custom StackModel\ncustom_predictions = stack_model.predict(X_test)\ncustom_accuracy = accuracy_score(y_test, custom_predictions)\ncustom_report = classification_report(y_test, custom_predictions, target_names=label_encoder.classes_)\n\nprint(\"Custom StackModel Results:\")\nprint(f\"Accuracy: {custom_accuracy:.4f}\")\nprint(\"Classification Report:\")\nprint(custom_report)\n\n# Sklearn StackingClassifier\nstacking_clf.fit(X_train, y_train)\n\n# Predict and evaluate using the sklearn StackingClassifier\nsklearn_predictions = stacking_clf.predict(X_test)\nsklearn_accuracy = accuracy_score(y_test, sklearn_predictions)\nsklearn_report = classification_report(y_test, sklearn_predictions, target_names=label_encoder.classes_)\n\nprint(\"\\nSklearn StackingClassifier Results:\")\nprint(f\"Accuracy: {sklearn_accuracy:.4f}\")\nprint(\"Classification Report:\")\nprint(sklearn_report)\n\n# Compare Results\ncomparison = {\n    \"Metric\": [\"Accuracy\"],\n    \"Custom StackModel\": [custom_accuracy],\n    \"Sklearn StackingClassifier\": [sklearn_accuracy]\n}\ncomparison_df = pd.DataFrame(comparison)\nprint(\"\\nComparison of Results:\")\nprint(comparison_df)\n\nCustom StackModel Results:\nAccuracy: 0.5229\nClassification Report:\n              precision    recall  f1-score   support\n\n        high       0.50      0.34      0.41        67\n         low       0.56      0.64      0.60       213\n      medium       0.48      0.46      0.47       200\n\n    accuracy                           0.52       480\n   macro avg       0.51      0.48      0.49       480\nweighted avg       0.52      0.52      0.52       480\n\n\nSklearn StackingClassifier Results:\nAccuracy: 0.5229\nClassification Report:\n              precision    recall  f1-score   support\n\n        high       0.50      0.34      0.41        67\n         low       0.56      0.64      0.60       213\n      medium       0.48      0.46      0.47       200\n\n    accuracy                           0.52       480\n   macro avg       0.51      0.48      0.49       480\nweighted avg       0.52      0.52      0.52       480\n\n\nComparison of Results:\n     Metric  Custom StackModel  Sklearn StackingClassifier\n0  Accuracy           0.522917                    0.522917\n\n\n==&gt; ok results is the same\n\n\nStep 6.2: Use config estimator = None\nUse PassthroughClassifier to ensure Compatibility: By implementing the standard scikit-learn methods (fit, predict, predict_proba, transform), this class can be used in pipelines or ensemble models (like StackingClassifier) that expect these methods, even though it does not perform any actual prediction or transformation.\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nimport numpy as np\n\nclass PassthroughClassifier(BaseEstimator, ClassifierMixin):\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        self.named_estimators_ = {\n            'passthrough': self\n        }\n        return self\n    \n    def predict(self, X):        \n        return  X\n    \n    def predict_proba(self, X):        \n        return X\n    \n    def transform(self, X):\n        # Return the original X unchanged\n        return X\n\n\n# Define feature selectors for each model\nmodel_1_features = model_configs['model_1']['feature_names']\nmodel_2_features = model_configs['model_2']['feature_names']\nmodel_3_features = model_configs['model_3']['feature_names']\n\n# Create pipelines for each base model\nmodel_1_pipeline = Pipeline([\n    ('selector', ColumnTransformer([('select', 'passthrough', model_1_features)])),\n    ('model', LogisticRegression(**model_configs['model_1']['hyperparameters']))\n])\n\nmodel_2_pipeline = Pipeline([\n    ('selector', ColumnTransformer([('select', 'passthrough', model_2_features)])),\n    ('model', lgb.LGBMClassifier(**model_configs['model_2']['hyperparameters']))\n])\n\nmodel_3_pipeline = Pipeline([\n    ('selector', ColumnTransformer([('select', 'passthrough', model_3_features)])),\n    ('model', PassthroughClassifier())\n])\n\n# Define base models\nbase_models = [\n    ('model_1', model_1_pipeline),\n    ('model_2', model_2_pipeline),\n    ('model_3', model_3_pipeline)\n]\n\n# Define the stacking classifier\nstacking_clf = StackingClassifier(\n    estimators=base_models,\n    final_estimator=final_estimator,\n    passthrough=False\n)\n\n\nfrom sklearn.metrics import classification_report\n\n# Custom StackModel\nstack_model = StackModel(model_configs, final_estimator)\nstack_model.fit(X_train, y_train)\n\n# Predict and evaluate using the custom StackModel\ncustom_predictions = stack_model.predict(X_test)\ncustom_accuracy = accuracy_score(y_test, custom_predictions)\ncustom_report = classification_report(y_test, custom_predictions, target_names=label_encoder.classes_)\n\nprint(\"Custom StackModel Results:\")\nprint(f\"Accuracy: {custom_accuracy:.4f}\")\nprint(\"Classification Report:\")\nprint(custom_report)\n\n# Sklearn StackingClassifier\nstacking_clf.fit(X_train, y_train)\n\n# Predict and evaluate using the sklearn StackingClassifier\nsklearn_predictions = stacking_clf.predict(X_test)\nsklearn_accuracy = accuracy_score(y_test, sklearn_predictions)\nsklearn_report = classification_report(y_test, sklearn_predictions, target_names=label_encoder.classes_)\n\nprint(\"\\nSklearn StackingClassifier Results:\")\nprint(f\"Accuracy: {sklearn_accuracy:.4f}\")\nprint(\"Classification Report:\")\nprint(sklearn_report)\n\n# Compare Results\ncomparison = {\n    \"Metric\": [\"Accuracy\"],\n    \"Custom StackModel\": [custom_accuracy],\n    \"Sklearn StackingClassifier\": [sklearn_accuracy]\n}\ncomparison_df = pd.DataFrame(comparison)\nprint(\"\\nComparison of Results:\")\nprint(comparison_df)\n\nCustom StackModel Results:\nAccuracy: 0.5583\nClassification Report:\n              precision    recall  f1-score   support\n\n        high       0.47      0.33      0.39        67\n         low       0.60      0.71      0.65       213\n      medium       0.52      0.47      0.49       200\n\n    accuracy                           0.56       480\n   macro avg       0.53      0.50      0.51       480\nweighted avg       0.55      0.56      0.55       480\n\n\nSklearn StackingClassifier Results:\nAccuracy: 0.5583\nClassification Report:\n              precision    recall  f1-score   support\n\n        high       0.47      0.33      0.39        67\n         low       0.60      0.71      0.65       213\n      medium       0.52      0.47      0.49       200\n\n    accuracy                           0.56       480\n   macro avg       0.53      0.50      0.51       480\nweighted avg       0.55      0.56      0.55       480\n\n\nComparison of Results:\n     Metric  Custom StackModel  Sklearn StackingClassifier\n0  Accuracy           0.558333                    0.558333\n\n\n\n\nStep 7: Data input for each model in the stacking process\n\nimport pandas as pd\nimport numpy as np\n\ndef generate_meta_features_clf(stacking_clf, X):\n    \"\"\"\n    Generate meta-features from the base models in a stacking classifier.\n\n    Args:\n        stacking_clf: A fitted StackingClassifier instance.\n        X (pd.DataFrame or np.ndarray): The input feature matrix.\n\n    Returns:\n        pd.DataFrame: A DataFrame of meta-features generated by base models.\n    \"\"\"\n    meta_features = []\n    columns = []\n\n    for name, model in stacking_clf.named_estimators_.items():\n        if hasattr(model, 'predict_proba'):  # Probability-based model\n            proba = model.predict_proba(X)\n            meta_features.append(proba)\n            columns.extend([f\"{name}_class_{i}\" for i in range(proba.shape[1])])\n        elif hasattr(model, 'decision_function'):  # Decision function-based model\n            decision_scores = model.decision_function(X)\n            if len(decision_scores.shape) == 1:  # Binary classification case\n                decision_scores = decision_scores.reshape(-1, 1)\n            meta_features.append(decision_scores)\n            columns.append(f\"{name}_decision\")\n        elif hasattr(model, 'predict'):  # Regressor or non-probability classifier\n            preds = model.predict(X).reshape(-1, 1)\n            meta_features.append(preds)\n            columns.append(f\"{name}_pred\")\n\n    # Stack the meta-features and convert to a DataFrame\n    meta_features_array = np.hstack(meta_features)\n    return pd.DataFrame(meta_features_array, columns=columns)\n\n\nproba_train_clf = generate_meta_features_clf(stacking_clf, X_train)\nproba_test_clf = generate_meta_features_clf(stacking_clf, X_test)\n\n\nproba_train_clf.head()\n\n\n\n\n\n\n\n\nmodel_1_class_0\nmodel_1_class_1\nmodel_1_class_2\nmodel_2_class_0\nmodel_2_class_1\nmodel_2_class_2\nmodel_3_class_0\nmodel_3_class_1\n\n\n\n\n0\n0.324457\n0.215730\n0.459813\n0.411349\n0.134452\n0.454199\n0.064\n53.0\n\n\n1\n0.185403\n0.419977\n0.394620\n0.036433\n0.646075\n0.317492\n0.071\n6.0\n\n\n2\n0.102198\n0.483839\n0.413963\n0.000267\n0.861533\n0.138200\n0.084\n12.0\n\n\n3\n0.055714\n0.592318\n0.351968\n0.004361\n0.420008\n0.575631\n0.045\n19.0\n\n\n4\n0.088008\n0.520087\n0.391905\n0.086474\n0.548249\n0.365277\n0.077\n27.0\n\n\n\n\n\n\n\n\npd.DataFrame(meta_features_train).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n0\n0.324457\n0.215730\n0.459813\n0.411349\n0.134452\n0.454199\n0.064\n53.0\n\n\n1\n0.185403\n0.419977\n0.394620\n0.036433\n0.646075\n0.317492\n0.071\n6.0\n\n\n2\n0.102198\n0.483839\n0.413963\n0.000267\n0.861533\n0.138200\n0.084\n12.0\n\n\n3\n0.055714\n0.592318\n0.351968\n0.004361\n0.420008\n0.575631\n0.045\n19.0\n\n\n4\n0.088008\n0.520087\n0.391905\n0.086474\n0.548249\n0.365277\n0.077\n27.0\n\n\n\n\n\n\n\n\nproba_test_clf.head()\n\n\n\n\n\n\n\n\nmodel_1_class_0\nmodel_1_class_1\nmodel_1_class_2\nmodel_2_class_0\nmodel_2_class_1\nmodel_2_class_2\nmodel_3_class_0\nmodel_3_class_1\n\n\n\n\n0\n0.096785\n0.503089\n0.400127\n0.404486\n0.374178\n0.221336\n0.114\n14.0\n\n\n1\n0.123998\n0.449699\n0.426303\n0.075519\n0.901799\n0.022682\n0.082\n21.0\n\n\n2\n0.070821\n0.607823\n0.321356\n0.001647\n0.461695\n0.536658\n0.107\n17.0\n\n\n3\n0.150150\n0.415067\n0.434782\n0.047777\n0.138113\n0.814110\n0.078\n32.0\n\n\n4\n0.136066\n0.411397\n0.452537\n0.006221\n0.597996\n0.395783\n0.077\n18.0\n\n\n\n\n\n\n\n\npd.DataFrame(meta_features_test).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n0\n0.096785\n0.503089\n0.400127\n0.404486\n0.374178\n0.221336\n0.114\n14.0\n\n\n1\n0.123998\n0.449699\n0.426303\n0.075519\n0.901799\n0.022682\n0.082\n21.0\n\n\n2\n0.070821\n0.607823\n0.321356\n0.001647\n0.461695\n0.536658\n0.107\n17.0\n\n\n3\n0.150150\n0.415067\n0.434782\n0.047777\n0.138113\n0.814110\n0.078\n32.0\n\n\n4\n0.136066\n0.411397\n0.452537\n0.006221\n0.597996\n0.395783\n0.077\n18.0",
    "crumbs": [
      "Machine learning",
      "Deep Dive into Stacking Model for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/deep_dive_stacking_model_multi_class.html#kết-luận",
    "href": "machine-learning/deep_dive_stacking_model_multi_class.html#kết-luận",
    "title": "Deep Dive into Stacking Model for Multi-Class Classification",
    "section": "Kết luận",
    "text": "Kết luận\n\nCustom StackModel tương đương StackingClassifier:\n\nCustom StackModel đã được kiểm chứng hoạt động tương đương với StackingClassifier trong scikit-learn, đảm bảo đầu ra (predictions và probabilities) khớp nhau khi sử dụng cùng một cấu hình.\n\nCustom StackModel chỉ hỗ trợ stack_method = 'predict_proba':\n\nHiện tại, StackModel chỉ hỗ trợ các base model có thể sử dụng phương pháp predict_proba. Nếu cần mở rộng để hỗ trợ decision_function hoặc predict, cần cập nhật thêm logic tương tự StackingClassifier.\n\nCustom StackModel tương đương StackingClassifier(passthrough=False):\n\nKhi passthrough=False, chỉ các meta-features từ base models (e.g., xác suất hoặc decision scores) được sử dụng làm đầu vào cho final_estimator. Custom StackModel đang hoạt động theo cơ chế này.\n\nSử dụng đầu ra của các base models trong mô hình nhiều lớp:\n\nTrong bài toán phân loại nhiều lớp, các stack models sẽ sử dụng đầu ra của các base model (e.g., xác suất cho mỗi lớp từ predict_proba) và tạo n_classes biến đầu vào tương ứng cho final_estimator.",
    "crumbs": [
      "Machine learning",
      "Deep Dive into Stacking Model for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html",
    "href": "machine-learning/delong_test.html",
    "title": "Delong method for AUC interval and AUC test",
    "section": "",
    "text": "AUC là viết tắt của “Area Under the ROC Curve” trong tiếng Anh, dịch sang tiếng Việt là “Diện tích dưới đường cong ROC”. Đây là một phép đo quan trọng trong đánh giá hiệu suất của mô hình phân loại (classifier) trong trường hợp binary classification (phân loại nhị phân).\nĐể hiểu AUC, hãy cùng nhau xem xét đến đường cong ROC (Receiver Operating Characteristic curve). ROC curve biểu diễn sự tương quan giữa tỷ lệ True Positive Rate (TPR) và tỷ lệ False Positive Rate (FPR) của mô hình phân loại, khi ngưỡng phân loại thay đổi từ 0 đến 1.\n\nTrue Positive Rate (TPR), còn gọi là Sensitivity hoặc Recall, đo lường tỷ lệ các trường hợp positive mà mô hình dự đoán chính xác.\nFalse Positive Rate (FPR) đo lường tỷ lệ các trường hợp negative bị phân loại sai (tức là dự đoán nhầm thành positive).\n\nAUC là diện tích nằm dưới đường cong ROC và nó thể hiện khả năng của mô hình phân loại phân biệt giữa hai lớp positive và negative. AUC càng lớn thì mô hình càng có khả năng phân loại tốt hơn, với AUC = 1 thể hiện mô hình hoàn hảo, trong khi AUC = 0.5 chỉ ra mô hình không khác biệt so với một dự đoán ngẫu nhiên.\nAUC là một phép đo định lượng và phổ biến trong đánh giá hiệu suất của các mô hình phân loại như Logistic Regression, Support Vector Machines (SVM), Random Forest, Neural Networks, và nhiều mô hình khác.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\naSAH = pd.read_excel(\"aSAH.xlsx\")\n\ny_true_str = aSAH['outcome']\ny_pred = aSAH['s100b']\n\n# Create a dictionary to map string labels to binary labels\nlabel_mapping = {\"Poor\": 1, \"Good\": 0}\n\n# Map true labels to binary labels\ny_true = [label_mapping[label] for label in y_true_str]\n\n# Tính đường cong ROC và AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_pred)\nroc_auc = auc(fpr, tpr)\n\n# Vẽ biểu đồ AUC\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#auc",
    "href": "machine-learning/delong_test.html#auc",
    "title": "Delong method for AUC interval and AUC test",
    "section": "",
    "text": "AUC là viết tắt của “Area Under the ROC Curve” trong tiếng Anh, dịch sang tiếng Việt là “Diện tích dưới đường cong ROC”. Đây là một phép đo quan trọng trong đánh giá hiệu suất của mô hình phân loại (classifier) trong trường hợp binary classification (phân loại nhị phân).\nĐể hiểu AUC, hãy cùng nhau xem xét đến đường cong ROC (Receiver Operating Characteristic curve). ROC curve biểu diễn sự tương quan giữa tỷ lệ True Positive Rate (TPR) và tỷ lệ False Positive Rate (FPR) của mô hình phân loại, khi ngưỡng phân loại thay đổi từ 0 đến 1.\n\nTrue Positive Rate (TPR), còn gọi là Sensitivity hoặc Recall, đo lường tỷ lệ các trường hợp positive mà mô hình dự đoán chính xác.\nFalse Positive Rate (FPR) đo lường tỷ lệ các trường hợp negative bị phân loại sai (tức là dự đoán nhầm thành positive).\n\nAUC là diện tích nằm dưới đường cong ROC và nó thể hiện khả năng của mô hình phân loại phân biệt giữa hai lớp positive và negative. AUC càng lớn thì mô hình càng có khả năng phân loại tốt hơn, với AUC = 1 thể hiện mô hình hoàn hảo, trong khi AUC = 0.5 chỉ ra mô hình không khác biệt so với một dự đoán ngẫu nhiên.\nAUC là một phép đo định lượng và phổ biến trong đánh giá hiệu suất của các mô hình phân loại như Logistic Regression, Support Vector Machines (SVM), Random Forest, Neural Networks, và nhiều mô hình khác.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\naSAH = pd.read_excel(\"aSAH.xlsx\")\n\ny_true_str = aSAH['outcome']\ny_pred = aSAH['s100b']\n\n# Create a dictionary to map string labels to binary labels\nlabel_mapping = {\"Poor\": 1, \"Good\": 0}\n\n# Map true labels to binary labels\ny_true = [label_mapping[label] for label in y_true_str]\n\n# Tính đường cong ROC và AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_pred)\nroc_auc = auc(fpr, tpr)\n\n# Vẽ biểu đồ AUC\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#liên-hệ-auc-và-thống-kê-mann-whitney",
    "href": "machine-learning/delong_test.html#liên-hệ-auc-và-thống-kê-mann-whitney",
    "title": "Delong method for AUC interval and AUC test",
    "section": "Liên hệ AUC và thống kê Mann-Whitney",
    "text": "Liên hệ AUC và thống kê Mann-Whitney\n\nThống kê Mann-Whitney (U-test):\nGiả sử bạn có hai nhóm dữ liệu độc lập, \\(X\\) và \\(Y\\), với kích thước lần lượt là \\(n_x\\) và \\(n_y\\). Để tính U-statistic, bạn cần xếp hạng dữ liệu trong mỗi nhóm và tính tổng các xếp hạng trong nhóm \\(X\\), ký hiệu là \\(R_X\\). Sau đó, U-statistic được tính bằng công thức:\n\\[ U = n_x \\cdot n_y + \\frac{n_x \\cdot (n_x + 1)}{2} - R_X \\]\nNếu \\(U\\) là giá trị U-statistic (thống kê Mann-Whitney), thì AUC được tính như sau:\n\\[ AUC = \\frac{U - \\frac{n_x \\cdot (n_x + 1)}{2}}{n_x \\cdot n_y} \\]\nDeLong’s test:\nGiả sử bạn có hai mô hình phân loại và đã tính được các xác suất dự đoán của chúng trên tập dữ liệu kiểm tra. Để thực hiện kiểm định DeLong, bạn cần tính U-statistic và phương sai của U-statistic (Var(U)) như đã giải thích trước đó.\nTiếp theo, để tính Z-score, bạn cần có AUC của hai mô hình (\\(AUC_1\\) và \\(AUC_2\\)) và Var(U) như sau:\n\\[ Z = \\frac{AUC_1 - AUC_2}{\\sqrt{\\text{Var}(U)}} \\]\nSau đó, sử dụng Z-score để tính p-value, và nếu p-value nhỏ hơn mức ý nghĩa đã chọn (thường là 0.05), bạn có thể kết luận rằng có sự khác biệt đáng kể giữa AUC của hai mô hình.\nCông thức tính khoảng tin cậy (confidence interval) cho AUC dựa trên phương pháp DeLong như sau:\nGiả sử bạn đã tính được U-statistic (U) và Var(U) từ kiểm định DeLong, thì khoảng tin cậy 95% cho AUC sẽ được tính bằng công thức sau:\n\\[ \\text{CI}_{\\text{lower}} = AUC - z_{\\alpha/2} \\cdot \\sqrt{\\text{Var}(U)} \\]\n\\[ \\text{CI}_{\\text{upper}} = AUC + z_{\\alpha/2} \\cdot \\sqrt{\\text{Var}(U)} \\]\nTrong đó:\n\nAUC là diện tích dưới đường cong ROC đã tính từ mô hình phân loại và tập dữ liệu.\nVar(U) là phương sai của U-statistic, tính từ kiểm định DeLong.\n\\(z_{\\alpha/2}\\) là giá trị thống kê từ phân phối chuẩn tương ứng với mức đáng tin cậy 95%. Với mức đáng tin cậy 95%, \\(\\alpha = 0.05\\), nên \\(z_{\\alpha/2}\\) tương ứng với giá trị thống kê 1.96.",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#thuật-toán-sun-và-xu",
    "href": "machine-learning/delong_test.html#thuật-toán-sun-và-xu",
    "title": "Delong method for AUC interval and AUC test",
    "section": "Thuật toán Sun và Xu",
    "text": "Thuật toán Sun và Xu\nThuật toán được đề xuất bởi Sun và Xu trong bài báo “Fast Implementation of DeLong’s Algorithm for Comparing the Areas Under Correlated Receiver Operating Characteristic Curves” cung cấp một cách tính toán hiệu suất và nhanh chóng để tính phương sai của sự khác biệt giữa diện tích dưới đường cong ROC (AUC) bằng phương pháp DeLong cho các đường cong ROC tương quan. Thuật toán của họ giảm đáng kể độ phức tạp tính toán so với phương pháp DeLong gốc, làm cho nó phù hợp cho các tập dữ liệu lớn.\nDưới đây là tổng quan về các bước của thuật toán:\n\nAUC được tính theo công thức\n\\[AUC = \\hat{\\theta} = \\frac{\\sum_{i=1}^{m} R_i - \\frac{m \\cdot (m + 1)}{2}}{m \\cdot n} = \\frac{1}{mn} \\sum_{i=1}^{m} \\mathbb{T}_{Z}(X_{i}) - \\frac{m+1}{2n}\\]\nTrong đó:\n\nm, n lần lượt là số lượng nhãn 1, 0\n\\(R_i={T}_{Z}(X_{i})\\) là giá trị score hoặc PD được sắp xếp theo thứ tự giảm dần (trường hợp bad = 1, good = 0). \\({T}_{Z}(X_{i})\\) được tính theo hàm Heaviside\nHàm Heaviside \\(\\mathcal{H}(t)\\):\n\n\\[ \\mathcal{H}(t) = \\begin{cases}1 & t &gt; 0 \\\\ \\frac{1}{2} & t = 0 \\\\ 0 & t &lt; 0\\end{cases} \\]\n\nMối liên hệ giữa Mid-Ranks và Hàm Heaviside:\n\n\\[ \\mathbb{T}_{\\mathcal{Z}}(\\mathcal{Z}_{i}) = \\sum_{j=1}^{M} \\mathcal{H}(\\mathcal{Z}_{i} - \\mathcal{Z}_{j}) + \\frac{1}{2} \\]\nTính Z-score và p-value: Tính Z-score bằng cách sử dụng công thức:\n\\[ z \\triangleq \\frac{\\hat{\\theta}^{(1)}-\\hat{\\theta}^{(2)}}{\\sqrt{\\mathbb{V}\\left[\\hat{\\theta}^{(1)}-\\hat{\\theta}^{(2)}\\right]}}=\\frac{\\hat{\\theta}^{(1)}-\\hat{\\theta}^{(2)}}{\\sqrt{\\mathbb{V}\\left[\\hat{\\theta}^{(1)}\\right]+\\mathbb{V}\\left[\\hat{\\theta}^{(2)}\\right]-2 \\mathbb{C}\\left[\\hat{\\theta}^{(1)}, \\hat{\\theta}^{(2)}\\right]}} \\]\nTrong đó:\n\n\\(\\hat{\\theta}^{(1)}\\) và \\(\\hat{\\theta}^{(2)}\\): Đây là hai ước tính của tham số \\(\\theta\\) hay AUC trong hai mẫu khác nhau mà đang so sánh.\n\\(\\mathbb{V}\\left[\\hat{\\theta}^{(1)}\\right]\\) và \\(\\mathbb{V}\\left[\\hat{\\theta}^{(2)}\\right]\\): Đây là phương sai của hai ước tính \\(\\hat{\\theta}^{(1)}\\) và \\(\\hat{\\theta}^{(2)}\\) tương ứng.\n\\(\\mathbb{C}\\left[\\hat{\\theta}^{(1)}, \\hat{\\theta}^{(2)}\\right]\\): Đây là hiệp phương sai giữa hai ước tính \\(\\hat{\\theta}^{(1)}\\) và \\(\\hat{\\theta}^{(2)}\\).\n\nSau đó, tính giá trị p-value từ giá trị phân phối chuẩn Z-score.\nKết quả: Dựa vào giá trị p-value tính toán được và một ngưỡng ý nghĩa đã chọn (ví dụ, 0.05), bạn có thể xác định xem sự khác biệt trong AUC giữa hai mô hình có ý nghĩa thống kê hay không.\n\n\ndef midrank(x):\n    J, Z = zip(*sorted(enumerate(x), key=lambda x:x[1]))\n    J = list(J)\n    Z = list(Z)\n    Z.append(Z[-1]+1)\n    N = len(x)\n    T = np.zeros(N)\n\n    i = 1\n    while i &lt;= N:\n        a = i\n        j = a\n        while Z[j-1] == Z[a-1]:\n            j = j + 1\n        b = j - 1\n        for k in range(a, b+1):\n            T[k-1] = (a + b) / 2\n        i = b + 1\n    T2 = np.zeros(N)\n    T2[J] = T\n\n    return T2\n\ndef fastDeLong(samples):\n    # %FASTDELONGCOV\n    # %The fast version of DeLong's method for computing the covariance of\n    # %unadjusted AUC.\n    # %% Reference:\n    # % @article{sun2014fast,\n    # %   title={Fast Implementation of DeLong's Algorithm for Comparing the Areas Under Correlated Receiver Operating Characteristic Curves},\n    # %   author={Xu Sun and Weichao Xu},\n    # %   journal={IEEE Signal Processing Letters},\n    # %   volume={21},\n    # %   number={11},\n    # %   pages={1389--1393},\n    # %   year={2014},\n    # %   publisher={IEEE}\n    # % }\n    # %% [aucs, delongcov] = fastDeLong(samples)\n    # %%\n    # % Edited by Xu Sun.\n    # % Homepage: https://pamixsun.github.io\n    # % Version: 2014/12\n    # %%\n\n    if np.sum(samples.spsizes) != samples.ratings.shape[1] or len(samples.spsizes) != 2:\n        assert False, 'Argument mismatch error'\n\n    z = samples.ratings    \n    m, n = samples.spsizes\n    x = z[:, :m]    \n    y = z[:, m:]\n    k = z.shape[0]\n\n    tx = np.zeros([k, m])\n    ty = np.zeros([k, n])\n    tz = np.zeros([k, m + n])\n    for r in range(k):\n        tx[r, :] = midrank(x[r, :])\n        ty[r, :] = midrank(y[r, :])\n        tz[r, :] = midrank(z[r, :])\n\n    aucs = np.sum(tz[:, :m], axis=1) / m / n - float(m + 1.0) / 2.0 / n\n    v01 = (tz[:, :m] - tx[:, :]) / n\n    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n    sx = np.cov(v01)\n    sy = np.cov(v10)\n    delongcov = sx / m + sy / n\n\n    return aucs, delongcov, v01, v10\n\n\nclass Samples:\n    def __init__(self, y_true, y_score1, y_score2):         \n        n_samples = len(y_true)\n        m = int(sum(y_true))\n        n = n_samples - m\n\n        # Zip the lists together\n        zipped_data = list(zip(y_true, y_score1, y_score2))\n\n        # Sort the zipped data based on labels in decreasing order\n        sorted_data = sorted(zipped_data, key=lambda x: x[0], reverse=True)\n\n        # Unzip the sorted data to get separate sorted values and labels\n        sorted_y_true, sorted_y_score1, sorted_y_score2 = zip(*sorted_data)\n    \n        # Stack the sorted scores into a ratings matrix\n        ratings = np.vstack((sorted_y_score1, sorted_y_score2))\n        \n        # Create a tuple with the sample sizes\n        spsizes = (m, n)\n        \n        self.ratings = ratings\n        self.spsizes = spsizes\n        \nimport numpy as np\nimport pandas as pd\naSAH = pd.read_excel(\"aSAH.xlsx\")\n\ny_true_str = aSAH['outcome']\ny_pred1 = aSAH['s100b']\ny_pred2 = aSAH['wfns']\n\n# Create a dictionary to map string labels to binary labels\nlabel_mapping = {\"Poor\": 1, \"Good\": 0}\n\n# Map true labels to binary labels\ny_true = [label_mapping[label] for label in y_true_str]\n\nsamples = Samples(y_true, y_pred1, y_pred2)\n\naucs, delongcov, v01, v10 = fastDeLong(samples)\n# Print the results\nprint(\"AUCs:\", aucs)\nprint(\"DeLong Covariance:\", delongcov)\n\nAUCs: [0.73136856 0.82367886]\nDeLong Covariance: [[0.00266868 0.00119616]\n [0.00119616 0.00146991]]\n\n\n\nKiểm định sự khác biệt AUC của 2 mô hình theo Delong\n\nimport scipy\ndef calpvalue(aucs, sigma):\n    l = np.array([[1, -1]])\n    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n    pvalue = 2 * (1 - scipy.stats.norm.cdf(z, loc=0, scale=1))\n    return pvalue\n\nAUC_DS, C, _, _ = fastDeLong(samples)\np_value = calpvalue(AUC_DS, C)[0][0]\n\nprint(f\"AUC Model 1: {AUC_DS[0]:.3f}\")\nprint(f\"AUC Model 2: {AUC_DS[1]:.3f}\")\nprint(\"P-value for DeLong test:\", p_value)\n# So sánh AUC của hai mô hình\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Có sự khác biệt đáng kể giữa AUC của hai mô hình.\")\nelse:\n    print(\"Không có sự khác biệt đáng kể giữa AUC của hai mô hình.\")\n\nAUC Model 1: 0.731\nAUC Model 2: 0.824\nP-value for DeLong test: 0.02717578222918804\nCó sự khác biệt đáng kể giữa AUC của hai mô hình.",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#so-sánh-auc-của-hai-mô-hình-bằng-bootstrap",
    "href": "machine-learning/delong_test.html#so-sánh-auc-của-hai-mô-hình-bằng-bootstrap",
    "title": "Delong method for AUC interval and AUC test",
    "section": "So sánh AUC của hai mô hình bằng bootstrap",
    "text": "So sánh AUC của hai mô hình bằng bootstrap\n\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\n# Tạo dữ liệu mẫu với độ dài chuỗi y_true là 1000\n# Tính AUC cho hai mô hình\nauc_model1 = roc_auc_score(y_true, predictions_model1)\nauc_model2 = roc_auc_score(y_true, predictions_model2)\n\n# Thực hiện bootstrap để tính khoảng tin cậy 95% cho AUC của cả hai mô hình\nn_iterations = 1000  # Số lượng lần lấy mẫu bootstrap\nn_size = len(y_true)  # Số lượng phần tử trong chuỗi y_true\n\nauc_scores_model1 = []\nauc_scores_model2 = []\n\nfor _ in range(n_iterations):\n    # Lấy ngẫu nhiên mẫu từ dữ liệu với replacement\n    indices = np.random.randint(0, n_size, n_size)\n    y_true_bootstrap = np.array(y_true)[indices]\n    y_pred_bootstrap_model1 = predictions_model1[indices]\n    y_pred_bootstrap_model2 = predictions_model2[indices]\n    \n    # Tính AUC cho mẫu lấy ra từ dữ liệu bootstrap cho hai mô hình\n    auc_score_model1 = roc_auc_score(y_true_bootstrap, y_pred_bootstrap_model1)\n    auc_score_model2 = roc_auc_score(y_true_bootstrap, y_pred_bootstrap_model2)\n    \n    auc_scores_model1.append(auc_score_model1)\n    auc_scores_model2.append(auc_score_model2)\n\n# Tính khoảng tin cậy 95% cho AUC cho cả hai mô hình\nlower_bound_model1 = np.percentile(auc_scores_model1, 2.5)\nupper_bound_model1 = np.percentile(auc_scores_model1, 97.5)\n\nlower_bound_model2 = np.percentile(auc_scores_model2, 2.5)\nupper_bound_model2 = np.percentile(auc_scores_model2, 97.5)\n\nprint(f\"AUC Model 1: {auc_model1:.3f} (95% CI: [{lower_bound_model1:.3f}, {upper_bound_model1:.3f}])\")\nprint(f\"AUC Model 2: {auc_model2:.3f} (95% CI: [{lower_bound_model2:.3f}, {upper_bound_model2:.3f}])\")\n\n# So sánh khoảng tin cậy của AUC của hai mô hình\nif lower_bound_model1 &gt; upper_bound_model2 or lower_bound_model2 &gt; upper_bound_model1:\n    print(\"Có sự khác biệt đáng kể giữa AUC của hai mô hình.\")\nelse:\n    print(\"Không có sự khác biệt đáng kể giữa AUC của hai mô hình.\")\n\nAUC Model 1: 0.523 (95% CI: [0.403, 0.638])\nAUC Model 2: 0.506 (95% CI: [0.383, 0.613])\nKhông có sự khác biệt đáng kể giữa AUC của hai mô hình.",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#tính-khoảng-tin-cậy-auc-sử-dụng-phương-pháp-delong",
    "href": "machine-learning/delong_test.html#tính-khoảng-tin-cậy-auc-sử-dụng-phương-pháp-delong",
    "title": "Delong method for AUC interval and AUC test",
    "section": "Tính khoảng tin cậy AUC sử dụng phương pháp Delong",
    "text": "Tính khoảng tin cậy AUC sử dụng phương pháp Delong\n\nfrom scipy.stats import norm\ndef calculate_auc_ci(y_true, y_score, alpha = 0.05):\n    samples = Samples(y_true, y_score, y_score)    \n\n    auc, auc_cov,_,_ = fastDeLong(samples)\n    auc = auc[0]\n    var_U = auc_cov[0,0]    \n    \n    z_score = norm.ppf(1 - alpha / 2)\n    lower_bound = auc - z_score * np.sqrt(var_U)\n    upper_bound = auc + z_score * np.sqrt(var_U)\n  \n    return auc, var_U, lower_bound, upper_bound\n\nauc_model, _, lower_bound, upper_bound = calculate_auc_ci(y_true, y_pred)\nprint(f\"AUC: {auc_model:.3f}\")\nprint(f\"95% Confidence Interval for AUC using bootstrap method: [{lower_bound:.3f}, {upper_bound:.3f}]\")\n\nAUC: 0.731\n95% Confidence Interval for AUC using bootstrap method: [0.630, 0.833]",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/delong_test.html#tính-khoảng-tin-cậy-auc-sử-dụng-phương-pháp-bootstrap",
    "href": "machine-learning/delong_test.html#tính-khoảng-tin-cậy-auc-sử-dụng-phương-pháp-bootstrap",
    "title": "Delong method for AUC interval and AUC test",
    "section": "Tính khoảng tin cậy AUC sử dụng phương pháp Bootstrap",
    "text": "Tính khoảng tin cậy AUC sử dụng phương pháp Bootstrap\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n\n# Tính AUC cho mô hình ban đầu\nauc_model = roc_auc_score(y_true, y_pred)\n\n# Số lượng lần lấy mẫu (bootstrap iterations)\nn_iterations = 1000\n\n# Số lượng mẫu lấy ra từ dữ liệu ban đầu trong mỗi lần lấy mẫu\nn_size = len(y_true)\n\n# Tạo mảng để lưu kết quả AUC từ các lần lấy mẫu\nauc_scores = []\n\nfor _ in range(n_iterations):\n    # Lấy ngẫu nhiên mẫu từ dữ liệu với replacement\n    indices = np.random.randint(0, n_size, n_size)\n    y_true_bootstrap = np.array(y_true)[indices]\n    y_pred_bootstrap = y_pred[indices]\n    \n    # Tính AUC cho mẫu lấy ra từ dữ liệu bootstrap\n    auc_score = roc_auc_score(y_true_bootstrap, y_pred_bootstrap)\n    auc_scores.append(auc_score)\n\n# Tính khoảng tin cậy 95% cho AUC\nlower_bound = np.percentile(auc_scores, 2.5)\nupper_bound = np.percentile(auc_scores, 97.5)\n\nprint(f\"AUC: {auc_model:.3f}\")\nprint(f\"95% Confidence Interval for AUC using bootstrap method: [{lower_bound:.3f}, {upper_bound:.3f}]\")\n\nAUC: 0.731\n95% Confidence Interval for AUC using bootstrap method: [0.624, 0.824]",
    "crumbs": [
      "Machine learning",
      "Delong method for AUC interval and AUC test"
    ]
  },
  {
    "objectID": "machine-learning/Classifier_Chain.html",
    "href": "machine-learning/Classifier_Chain.html",
    "title": "Classifier Chain",
    "section": "",
    "text": "Mô hình Classifier Chain cho dự báo rủi ro tín dụng với các mức DPD\nBối cảnh:\n- Dữ liệu đặc trưng:\n- Nhân khẩu học: Tuổi, thu nhập, tình trạng việc làm\n- Lịch sử tín dụng: Dư nợ hiện tại, số lần trễ hạn trước đây, tỷ lệ sử dụng tín dụng\n- Đặc trưng khoản vay: Số tiền vay, lãi suất, kỳ hạn, tỷ lệ nợ trên giá trị tài sản\n\nNhãn (Labels) đa nhãn:\n\nDPD30+: Trễ ≥30 ngày\n\nDPD60+: Trễ ≥60 ngày\n\nDPD90+: Trễ ≥90 ngày\n\n\nCác nhãn này không loại trừ lẫn nhau. Ví dụ, nếu khách hàng trễ 90 ngày, họ chắc chắn đã trễ 30 và 60 ngày trước đó. Điều này tạo ra mối liên hệ chặt chẽ giữa các nhãn.\n\nÝ tưởng chính của Classifier Chain:\nThay vì dự đoán từng nhãn một cách độc lập, mô hình sẽ dự đoán tuần tự và sử dụng kết quả dự đoán trước đó như một phần đầu vào cho bước tiếp theo. Điều này giúp mô hình tận dụng quan hệ giữa các nhãn.\nMinh họa (mô tả ảnh):\nHãy tưởng tượng một sơ đồ dạng chuỗi:\n        ┌────────────────┐\n        │   Đặc trưng    │\n        │  (Features)    │\n        └───────┬────────┘\n                │\n                v\n        ┌────────────────┐\n        │ Classifier A    │\n        │ Dự đoán DPD30+  │\n        └───────┬────────┘\n                │\n         (Xác suất DPD30+)\n                │\n                v\n        ┌────────────────┐\n        │ Classifier B    │\n        │ Dự đoán DPD60+  │\n        │ (dựa trên       │\n        │ đặc trưng +     │\n        │ kết quả A)      │\n        └───────┬────────┘\n                │\n         (Xác suất DPD60+)\n                │\n                v\n        ┌────────────────┐\n        │ Classifier C    │\n        │ Dự đoán DPD90+  │\n        │ (dựa trên       │\n        │ đặc trưng +     │\n        │ kết quả A, B)   │\n        └───────┬────────┘\n                │\n         (Xác suất DPD90+)\n                │\n                v\n              (Kết quả)\n\nClassifier A: Dự đoán DPD30+ dựa trên đặc trưng ban đầu.\n\nClassifier B: Dự đoán DPD60+, nhưng đầu vào gồm cả đặc trưng ban đầu và dự đoán từ A (DPD30+).\n\nClassifier C: Dự đoán DPD90+, đầu vào gồm đặc trưng ban đầu và dự đoán từ A, B.\n\nTrong sơ đồ trên, mỗi mũi tên thể hiện việc truyền thông tin: Dự đoán ở bước trước hỗ trợ bước sau.\n\nQuy trình dự đoán:\n1. Bước 1:\n- Đầu vào: Đặc trưng khách hàng (nhân khẩu học, lịch sử tín dụng, khoản vay)\n- Đầu ra A: Xác suất khách trễ ≥30 ngày (ví dụ: 0.7)\n\nBước 2:\n\nĐầu vào: Đặc trưng ban đầu + kết quả dự đoán DPD30+ (0.7)\n\nĐầu ra B: Xác suất trễ ≥60 ngày (ví dụ: 0.4)\n\nBước 3:\n\nĐầu vào: Đặc trưng ban đầu + kết quả DPD30+ (0.7) + kết quả DPD60+ (0.4)\n\nĐầu ra C: Xác suất trễ ≥90 ngày (ví dụ: 0.1)\n\n\nĐiểm nổi bật:\n- Khai thác quan hệ giữa các nhãn:\nCác mức trễ hạn cao (DPD90+) phụ thuộc logic vào khả năng trễ hạn ở mức thấp hơn (DPD30+, DPD60+).\n\nDự đoán thực tế hơn:\nNếu khả năng trễ 30 ngày và 60 ngày thấp, thì khả năng trễ 90 ngày cũng sẽ thấp. Ngược lại, nếu hệ thống dự đoán 30 và 60 ngày trễ hạn cao, mô hình sẽ phản ánh điều đó vào dự đoán 90 ngày.\nDễ mở rộng:\nCó thể thử nhiều thứ tự nhãn khác nhau hoặc sử dụng các mô hình mạnh mẽ (như XGBoost, Deep Learning) bên trong mỗi bước phân loại.\n\n\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multioutput import ClassifierChain\nfrom sklearn.model_selection import train_test_split\n\n# Giả lập dữ liệu:\n#  - X: ma trận đặc trưng, giả sử có 1000 khách hàng, mỗi khách có 10 đặc trưng.\n#  - Y: ma trận nhãn, giả sử có 3 nhãn: DPD30+, DPD60+, DPD90+.\n# Dữ liệu giả lập mang tính minh họa, trong thực tế bạn dùng dữ liệu thật.\nnp.random.seed(42)\nX = np.random.rand(1000, 10)           # 1000 khách, 10 đặc trưng\nY = np.zeros((1000, 3))                # 3 nhãn: DPD30+, DPD60+, DPD90+\n\n# Tạo nhãn giả lập dựa trên X. Ví dụ:\n# - Nếu tổng một số đặc trưng &gt; 5 thì khách có khả năng DPD30+,\n# - DPD60+ và DPD90+ sẽ phụ thuộc vào DPD30+ (giả lập quan hệ nhãn).\nthreshold_30 = 5.0\nthreshold_60 = 5.5\nthreshold_90 = 6.0\nsum_feats = X.sum(axis=1)\n\nY[:, 0] = (sum_feats &gt; threshold_30).astype(int)  # DPD30+\nY[:, 1] = ((sum_feats &gt; threshold_60) & (Y[:,0] == 1)).astype(int)  # DPD60+ (phụ thuộc vào DPD30+)\nY[:, 2] = ((sum_feats &gt; threshold_90) & (Y[:,1] == 1)).astype(int)  # DPD90+ (phụ thuộc vào DPD60+)\n\n# Chia tập dữ liệu\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Khởi tạo mô hình LogisticRegression\nbase_lr = LogisticRegression()\n\n# Tạo Classifier Chain\n# Thứ tự nhãn: [DPD30+, DPD60+, DPD90+] tương ứng các cột Y[:,0], Y[:,1], Y[:,2]\nchain = ClassifierChain(base_lr, order=[0, 1, 2], cv=None)\n\n# Huấn luyện mô hình\nchain.fit(X_train, Y_train)\n\n# Dự đoán trên dữ liệu test\nY_pred = chain.predict(X_test)\n\n# Đánh giá\nfrom sklearn.metrics import accuracy_score, hamming_loss\n\n# accuracy_score với multi-label sẽ tính số mẫu mà tất cả các nhãn dự đoán đúng\nacc = accuracy_score(Y_test, Y_pred)\nh_loss = hamming_loss(Y_test, Y_pred)  # Tỷ lệ nhãn sai trên tổng số nhãn\n\nprint(\"Accuracy (multi-label exact match):\", acc)\nprint(\"Hamming Loss:\", h_loss)\n\n# In ra vài kết quả dự đoán\nprint(\"Dự đoán một vài khách hàng (DPD30+, DPD60+, DPD90+):\")\nfor i in range(5):\n    print(\"Thực tế:\", Y_test[i], \"Dự đoán:\", Y_pred[i])\n\nAccuracy (multi-label exact match): 0.925\nHamming Loss: 0.025\nDự đoán một vài khách hàng (DPD30+, DPD60+, DPD90+):\nThực tế: [0. 0. 0.] Dự đoán: [0. 0. 0.]\nThực tế: [0. 0. 0.] Dự đoán: [0. 0. 0.]\nThực tế: [0. 0. 0.] Dự đoán: [0. 0. 0.]\nThực tế: [0. 0. 0.] Dự đoán: [0. 0. 0.]\nThực tế: [0. 0. 0.] Dự đoán: [0. 0. 0.]\n\n\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import ClassifierChain\nfrom sklearn.metrics import cohen_kappa_score, roc_auc_score\nfrom lightgbm import LGBMClassifier\n\nnp.random.seed(42)\n\nX = np.random.rand(1000, 10)\nY = np.zeros((1000, 5))\n\nsum_feats = X.sum(axis=1)\nthresholds = [4.5, 5.0, 5.5, 6.0, 6.5]\n\nY[:, 0] = (sum_feats &gt; thresholds[0]).astype(int)\nY[:, 1] = ((sum_feats &gt; thresholds[1]) & (Y[:, 0] == 1)).astype(int)\nY[:, 2] = ((sum_feats &gt; thresholds[2]) & (Y[:, 1] == 1)).astype(int)\nY[:, 3] = ((sum_feats &gt; thresholds[3]) & (Y[:, 2] == 1)).astype(int)\nY[:, 4] = ((sum_feats &gt; thresholds[4]) & (Y[:, 3] == 1)).astype(int)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Thay LogisticRegression bằng LGBMClassifier\nbase_clf = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n\nchain = ClassifierChain(base_clf, order=[0,1,2,3,4], cv=None)\nchain.fit(X_train, Y_train)\n\nY_pred = chain.predict(X_test)\nY_pred_proba = chain.predict_proba(X_test)\n\nkappas = []\nginis = []\n\nfor i in range(5):\n    kappa_i = cohen_kappa_score(Y_test[:, i], Y_pred[:, i])\n    kappas.append(kappa_i)\n    \n    proba_i = Y_pred_proba[i]\n    # Kiểm tra xem có hai lớp (2 chiều) không\n    if proba_i.ndim == 1 or proba_i.shape[1] == 1:\n        # Nếu chỉ có một lớp\n        ginis.append(0.0)\n    else:\n        auc_i = roc_auc_score(Y_test[:, i], proba_i[:, 1])\n        gini_i = 2 * auc_i - 1\n        ginis.append(gini_i)\n\nmean_kappa = np.mean(kappas)\nmean_gini = np.mean(ginis)\n\nprint(\"Kết quả đánh giá:\")\nfor i in range(5):\n    print(f\"Nhãn {i}: Kappa = {kappas[i]:.4f}, Gini = {ginis[i]:.4f}\")\n\nprint(f\"Trung bình Kappa (macro): {mean_kappa:.4f}\")\nprint(f\"Trung bình Gini (macro): {mean_gini:.4f}\")\n\nKết quả đánh giá:\nNhãn 0: Kappa = 0.7465, Gini = 0.0000\nNhãn 1: Kappa = 0.7375, Gini = 0.0000\nNhãn 2: Kappa = 0.7867, Gini = 0.0000\nNhãn 3: Kappa = 0.6131, Gini = 0.0000\nNhãn 4: Kappa = 0.1870, Gini = 0.0000\nTrung bình Kappa (macro): 0.6142\nTrung bình Gini (macro): 0.0000\n\n\n\nprint(\"Y.shape:\", Y.shape)\nprint(\"Y_train.shape:\", Y_train.shape)\n\nY.shape: (1000, 5)\nY_train.shape: (800, 5)",
    "crumbs": [
      "Machine learning",
      "Classifier Chain"
    ]
  },
  {
    "objectID": "machine-learning/EnhanceTraditionalScoreCardByReinforcementLearning.html",
    "href": "machine-learning/EnhanceTraditionalScoreCardByReinforcementLearning.html",
    "title": "Use Reinforcement learning to enhance traditional scorecard",
    "section": "",
    "text": "Sử dụng học tăng cường để tăng cường các mô hình tính điểm tín dụng truyền thống liên quan đến việc tận dụng điểm mạnh của cả hai phương pháp. Dưới đây là hướng dẫn từng bước về cách thực hiện:\n\nXác định mục tiêu kinh doanh: Xác định rõ mục tiêu kinh doanh của bạn và vấn đề bạn muốn giải quyết bằng cách sử dụng phương pháp học tăng cường. Xác định những khía cạnh của quy trình chấm điểm tín dụng mà bạn muốn cải thiện.\nData Preparation:\n\nThu thập và xử lý tiền dữ liệu tín dụng lịch sử, bao gồm thông tin người nộp đơn, biến đầu vào và kết quả (default or non-default).\nChia dữ liệu thành các tập huấn luyện, xác nhận và kiểm tra (training, validation, and test sets).\n\nXác định chức năng phần thưởng: Thiết kế chức năng phần thưởng phản ánh các mục tiêu của hệ thống tính điểm tín dụng của bạn. Chức năng khen thưởng sẽ khuyến khích agent đưa ra quyết định phù hợp với mục tiêu kinh doanh của bạn, chẳng hạn như tối đa hóa lợi nhuận hoặc giảm thiểu khả năng vỡ nợ.\nFeature Engineering: Trích xuất và tiền xử lý các biến đầu vào từ dữ liệu tín dụng mà tác nhân học tăng cường sẽ sử dụng để đưa ra quyết định. Dữ liệu này có thể bao gồm nhân khẩu học của người nộp đơn, chỉ số tài chính, lịch sử tín dụng, v.v.\nKết hợp các mô hình:\n\nHuấn luyện mô hình chấm điểm tín dụng truyền thống (ví dụ: hồi quy logistic, cây quyết định) bằng cách sử dụng dữ liệu đào tạo của bạn.\nSử dụng các dự đoán của mô hình này như một phần của biểu diễn trạng thái trong thiết lập học tăng cường.\n\nXây dựng thiết lập Reinforcement Learning:\n\nXác định không gian trạng thái: Kết hợp các kết quả đầu ra của mô hình truyền thống với các đặc điểm liên quan khác như biểu diễn trạng thái.\nXác định không gian hành động: Xác định các hành động có thể thực hiện (phê duyệt, từ chối) hoặc xem xét các quyết định khác như đặt giới hạn tín dụng.\nThực hiện các chiến lược thăm dò: Cân bằng thăm dò và khai thác để thu thập dữ liệu cho việc học.\n\nReinforcement Learning Algorithm:\n\nSử dụng thuật toán như kẻ cướp theo ngữ cảnh hoặc kẻ cướp nhiều vũ trang cho phép đưa ra các quyết định tuần tự dựa trên trạng thái và hành động.\nCập nhật các tham số mô hình dựa trên phần thưởng nhận được từ chức năng phần thưởng.\n\nTraining and Evaluation:\n\nHuấn luyện tác nhân học tăng cường bằng cách sử dụng dữ liệu huấn luyện và xác thực hiệu suất của nó trên bộ xác thực.\nTinh chỉnh các siêu tham số và chiến lược để tối ưu hóa hiệu suất.\n\nKhả năng diễn giải và tính công bằng của mô hình:\n\nĐảm bảo rằng các quyết định của mô hình học tăng cường có thể diễn giải và giải thích được, đặc biệt là trong bối cảnh quy định.\nThực hiện các chiến lược nhận thức về sự công bằng để ngăn chặn sự thiên vị và đảm bảo sự công bằng trong quá trình ra quyết định.\n\nThử nghiệm và triển khai:\n\nThử nghiệm mô hình tính điểm tín dụng nâng cao trên dữ liệu thử nghiệm chưa từng thấy để đánh giá hiệu quả hoạt động trong thế giới thực của nó.\nTriển khai mô hình trong môi trường được kiểm soát và theo dõi hiệu suất của nó theo thời gian.\n\nHọc tập và tối ưu hóa liên tục:\n\nTriển khai thiết lập học trực tuyến trong đó mô hình tiếp tục học từ dữ liệu mới khi có sẵn.\nThường xuyên đánh giá và cập nhật mô hình để thích ứng với các xu hướng và hành vi thay đổi.\n\n\nHãy nhớ rằng việc áp dụng học tập tăng cường trong việc chấm điểm tín dụng đòi hỏi phải xem xét cẩn thận các khía cạnh pháp lý, đạo đức và quy định. Tính minh bạch, công bằng và tuân thủ luật pháp là rất quan trọng khi đưa ra quyết định tín dụng bằng hệ thống AI.\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Simulated credit data\n# Features: Age, Income, Credit History, Loan Amount, ...\nX = np.random.rand(1000, 5)\ny = np.random.choice([0, 1], size=1000)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a traditional credit scoring model (Logistic Regression)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate the traditional model\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Traditional Model Accuracy:\", accuracy)\n\n# Reinforcement Learning Setup (Simplified)\nclass CreditScoringEnvironment:\n    def __init__(self, features):\n        self.features = features\n        self.current_instance = 0\n    \n    def reset(self):\n        self.current_instance = 0\n        return self.features[self.current_instance]\n    \n    def step(self, action):\n        reward = 1 if action == y[self.current_instance] else -1\n        self.current_instance += 1\n        done = self.current_instance &gt;= len(self.features)\n        if done:\n            return None, reward, done, {}\n        else:\n            return self.features[self.current_instance], reward, done, {}\n\n# Initialize environment\nenv = CreditScoringEnvironment(X_test)\n\n# Reinforcement Learning Algorithm (Simplified)\nnum_episodes = 100\nlearning_rate = 0.1\ndiscount_factor = 0.95\nexploration_prob = 0.2\n\nQ = np.zeros((len(X_test), 2))  # Q-values (approve, deny)\n\nfor episode in range(num_episodes):\n    state = env.reset()\n    state_index = env.current_instance\n    \n    done = False\n    \n    while not done:\n        if np.random.rand() &lt; exploration_prob:\n            action = np.random.choice([0, 1])  # Exploration\n        else:\n            action = np.argmax(Q[state_index, :])  # Exploitation\n        \n        new_state, reward, done, _ = env.step(action)\n        \n        if new_state is not None:\n            new_state_index = env.current_instance\n            \n            Q[state_index, action] = (1 - learning_rate) * Q[state_index, action] + \\\n                                     learning_rate * (reward + discount_factor * np.max(Q[new_state_index, :]))\n            \n            state_index = new_state_index\n\n# Evaluate the enhanced model\ny_rl_pred = np.argmax(Q, axis=1)\nrl_accuracy = accuracy_score(y_test, y_rl_pred)\nprint(\"Enhanced Model Accuracy:\", rl_accuracy)\n\nTraditional Model Accuracy: 0.475\nEnhanced Model Accuracy: 0.525",
    "crumbs": [
      "Machine learning",
      "Use Reinforcement learning to enhance traditional scorecard"
    ]
  },
  {
    "objectID": "machine-learning/LightGBM_Predict_Contrib_for_Multi_Class.html",
    "href": "machine-learning/LightGBM_Predict_Contrib_for_Multi_Class.html",
    "title": "LightGBM Predict Contrib for Multi-Class Classification",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport shap\nimport matplotlib.pyplot as plt\nimport os\n\nclass ShapExplain:\n    def __init__(self, model):\n        if hasattr(model, 'classes_') and hasattr(model, 'feature_name_'):\n            self.model = model\n            self.raw_scores = None\n            self.proba_scores = None\n            self.labels = None\n            self.shap_values = None\n            self.shap_contributions = None\n\n            # Initialize class attributes from the model\n            self.num_classes = len(model.classes_)\n            self.num_features = len(model.feature_name_)\n            self.feature_names = model.feature_name_ + [\"Bias\"]\n        else:\n            raise ValueError(\"The provided model does not have the required attributes 'classes_' or 'feature_name_'.\")\n\n    def predict_raw_scores(self, X):\n        if hasattr(self.model, 'predict'):\n            self.raw_scores = self.model.predict(X, raw_score=True)\n            return self.raw_scores\n        else:\n            raise AttributeError(\"The model does not support raw score predictions.\")\n\n    def predict_proba(self, X):\n        if hasattr(self.model, 'predict_proba'):\n            self.proba_scores = self.model.predict_proba(X)\n            return self.proba_scores\n        else:\n            raise AttributeError(\"The model does not support probability predictions.\")\n\n    def predict_labels(self, X):\n        self.predict_proba(X)  # Ensure probabilities are always calculated\n        self.labels = [np.argmax(score) for score in self.proba_scores]\n        return self.labels\n\n    def calculate_shap_values(self, X):\n        if hasattr(self.model, 'predict'):\n            self.shap_values = self.model.predict(X, pred_contrib=True)\n            return self.shap_values\n        else:\n            raise AttributeError(\"The model does not support SHAP value predictions.\")\n\n    def split_shap_values(self):\n        if self.shap_values is None:\n            raise ValueError(\"SHAP values have not been calculated. Call calculate_shap_values first.\")\n        \n        shap_contributions = {}\n        for class_idx in range(self.num_classes):\n            start_idx = class_idx * (self.num_features + 1)\n            end_idx = start_idx + (self.num_features + 1)\n            shap_contributions[f\"Class_{class_idx}\"] = pd.DataFrame(\n                self.shap_values[:, start_idx:end_idx], columns=self.feature_names\n            )\n        self.shap_contributions = shap_contributions\n        return self.shap_contributions\n    \n    def calculate_mean_absolute_shap_contributions(self):\n        \"\"\"\n        Calculate the mean absolute SHAP contributions for each feature and class.\n\n        Returns:\n            pd.DataFrame: A DataFrame where rows represent features, columns represent classes,\n                        and values are the mean absolute SHAP contributions.\n        \"\"\"\n        if self.shap_contributions is None:\n            raise ValueError(\"SHAP contributions have not been split. Call split_shap_values first.\")\n        \n        mean_abs_contributions = {}\n        for class_name, shap_values_df in self.shap_contributions.items():\n            # Exclude the \"Bias\" column and calculate the mean absolute value\n            mean_abs_contributions[class_name] = shap_values_df.iloc[:, :-1].abs().mean(axis=0)\n        \n        # Combine into a DataFrame for easier interpretation\n        mean_abs_df = pd.DataFrame(mean_abs_contributions)\n        mean_abs_df.index.name = \"Feature\"\n        \n        return mean_abs_df\n\n    def plot_shap_summary(self, plot_type='bar', output_dir=None, title_prefix=None):\n        \n        # Ensure output directory exists if specified\n        if output_dir and not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        for class_name, shap_values_df in self.shap_contributions.items():\n            shap_values = shap_values_df.iloc[:, :-1].values\n            feature_names = shap_values_df.columns[:-1]  # Exclude 'Bias' from feature names\n\n            plt.figure()\n            shap.summary_plot(shap_values, feature_names=feature_names, plot_type=plot_type, show=False)\n\n            title = f'SHAP Summary Plot for {class_name}'\n            if title_prefix:\n                title = f'{title_prefix} {title}'\n            plt.title(title)\n\n            if output_dir:\n                filename = f'{title_prefix}_{class_name}_shap_summary.png' if title_prefix else f'{class_name}_shap_summary.png'\n                plt.savefig(os.path.join(output_dir, filename), bbox_inches='tight')\n                plt.close()\n            else:\n                plt.show()\n\n# Example usage:\n# Assuming `model` and `X_test` are defined, and the model is already fitted with attributes like classes_ and feature_names_.\n# shap_explain = ShapExplain(model)\n# raw_scores = shap_explain.predict_raw_scores(X_test)\n# proba_scores = shap_explain.predict_proba(X_test)\n# labels = shap_explain.predict_labels(X_test)\n# shap_values = shap_explain.calculate_shap_values(X_test)\n# shap_contributions = shap_explain.split_shap_values()\n\n# # Create a Pandas Excel writer using openpyxl as the engine\n# with pd.ExcelWriter('shap_explain_output.xlsx', engine='openpyxl') as writer:\n#     # Convert raw_scores and labels to DataFrames and write to Excel\n#     pd.DataFrame(raw_scores).to_excel(writer, sheet_name='Raw Scores')\n#     pd.DataFrame(proba_scores, columns=[f\"Class_{i}\" for i in range(shap_explain.num_classes)]).to_excel(writer, sheet_name='Probabilities')\n#     pd.DataFrame(labels, columns=['Predicted Labels']).to_excel(writer, sheet_name='Labels')\n\n#     # Write each class's SHAP contributions to a separate sheet\n#     for class_name, df in shap_contributions.items():\n#         df.to_excel(writer, sheet_name=f'SHAP Values {class_name}')\nClass ShapExplain được thiết kế để hỗ trợ việc giải thích các mô hình Machine Learning sử dụng SHAP (SHapley Additive exPlanations). Class này giúp người dùng: 1. Tính toán các giá trị SHAP cho mô hình. 2. Tách riêng các đóng góp SHAP theo lớp (class). 3. Tạo các biểu đồ SHAP để minh họa ảnh hưởng của từng đặc trưng đến dự đoán của mô hình.\nClass yêu cầu mô hình đầu vào phải có hai thuộc tính quan trọng: - classes_: Danh sách các lớp trong bài toán phân loại. - feature_name_: Danh sách các đặc trưng được sử dụng trong mô hình.\n\n\n\n__init__:\n\nKhởi tạo class với mô hình và kiểm tra mô hình có các thuộc tính cần thiết (classes_, feature_name_).\nGán các giá trị ban đầu (số lượng lớp, số lượng đặc trưng, v.v.).\n\npredict_raw_scores:\n\nDự đoán điểm số “thô” (raw scores) từ mô hình.\n\npredict_proba:\n\nDự đoán xác suất của từng lớp.\n\npredict_labels:\n\nDự đoán nhãn (label) bằng cách chọn lớp có xác suất cao nhất.\n\ncalculate_shap_values:\n\nTính giá trị SHAP bằng cách sử dụng phương pháp predict với pred_contrib=True.\n\nsplit_shap_values:\n\nPhân tách giá trị SHAP theo từng lớp (class) thành các DataFrame riêng biệt.\n\ncalculate_mean_absolute_shap_contributions:\n\nTính giá trị trung bình tuyệt đối của các đóng góp SHAP (mean absolute SHAP contributions) để hiểu đặc trưng nào quan trọng nhất.\n\nplot_shap_summary:\n\nTạo biểu đồ SHAP (summary plot) cho từng lớp, lưu vào thư mục hoặc hiển thị trực tiếp.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport shap\nimport matplotlib.pyplot as plt\nimport os\n\nclass ShapExplain:\n    def __init__(self, model):\n        if hasattr(model, 'classes_') and hasattr(model, 'feature_name_'):\n            self.model = model\n            self.raw_scores = None\n            self.proba_scores = None\n            self.labels = None\n            self.shap_values = None\n            self.shap_contributions = None\n\n            # Initialize class attributes from the model\n            self.num_classes = len(model.classes_)\n            self.num_features = len(model.feature_name_)\n            self.feature_names = model.feature_name_ + [\"Bias\"]\n        else:\n            raise ValueError(\"The provided model does not have the required attributes 'classes_' or 'feature_name_'.\")\n\n    def predict_raw_scores(self, X):\n        if hasattr(self.model, 'predict'):\n            self.raw_scores = self.model.predict(X, raw_score=True)\n            return self.raw_scores\n        else:\n            raise AttributeError(\"The model does not support raw score predictions.\")\n\n    def predict_proba(self, X):\n        if hasattr(self.model, 'predict_proba'):\n            self.proba_scores = self.model.predict_proba(X)\n            return self.proba_scores\n        else:\n            raise AttributeError(\"The model does not support probability predictions.\")\n\n    def predict_labels(self, X):\n        self.predict_proba(X)  # Ensure probabilities are always calculated\n        self.labels = [np.argmax(score) for score in self.proba_scores]\n        return self.labels\n\n    def calculate_shap_values(self, X):\n        if hasattr(self.model, 'predict'):\n            self.shap_values = self.model.predict(X, pred_contrib=True)\n            return self.shap_values\n        else:\n            raise AttributeError(\"The model does not support SHAP value predictions.\")\n\n    def split_shap_values(self):\n        if self.shap_values is None:\n            raise ValueError(\"SHAP values have not been calculated. Call calculate_shap_values first.\")\n        \n        shap_contributions = {}\n        for class_idx in range(self.num_classes):\n            start_idx = class_idx * (self.num_features + 1)\n            end_idx = start_idx + (self.num_features + 1)\n            shap_contributions[f\"Class_{class_idx}\"] = pd.DataFrame(\n                self.shap_values[:, start_idx:end_idx], columns=self.feature_names\n            )\n        self.shap_contributions = shap_contributions\n        return self.shap_contributions\n    \n    def calculate_mean_absolute_shap_contributions(self):\n        \"\"\"\n        Calculate the mean absolute SHAP contributions for each feature and class.\n\n        Returns:\n            pd.DataFrame: A DataFrame where rows represent features, columns represent classes,\n                        and values are the mean absolute SHAP contributions.\n        \"\"\"\n        if self.shap_contributions is None:\n            raise ValueError(\"SHAP contributions have not been split. Call split_shap_values first.\")\n        \n        mean_abs_contributions = {}\n        for class_name, shap_values_df in self.shap_contributions.items():\n            # Exclude the \"Bias\" column and calculate the mean absolute value\n            mean_abs_contributions[class_name] = shap_values_df.iloc[:, :-1].abs().mean(axis=0)\n        \n        # Combine into a DataFrame for easier interpretation\n        mean_abs_df = pd.DataFrame(mean_abs_contributions)\n        mean_abs_df.index.name = \"Feature\"\n        \n        return mean_abs_df\n\n    def plot_shap_summary(self, plot_type='bar', output_dir=None, title_prefix=None):\n        \n        # Ensure output directory exists if specified\n        if output_dir and not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        for class_name, shap_values_df in self.shap_contributions.items():\n            shap_values = shap_values_df.iloc[:, :-1].values\n            feature_names = shap_values_df.columns[:-1]  # Exclude 'Bias' from feature names\n\n            plt.figure()\n            shap.summary_plot(shap_values, feature_names=feature_names, plot_type=plot_type, show=False)\n\n            title = f'SHAP Summary Plot for {class_name}'\n            if title_prefix:\n                title = f'{title_prefix} {title}'\n            plt.title(title)\n\n            if output_dir:\n                filename = f'{title_prefix}_{class_name}_shap_summary.png' if title_prefix else f'{class_name}_shap_summary.png'\n                plt.savefig(os.path.join(output_dir, filename), bbox_inches='tight')\n                plt.close()\n            else:\n                plt.show()\n\n\n# LightGBM Predict Contrib for Multi-Class Classification\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport shap\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Step 1: Load Dataset\nwine = load_wine()\nX = pd.DataFrame(wine.data, columns=wine.feature_names)\ny = wine.target\n\n# Step 2: Split Dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Step 3: Train LightGBM Model\nmodel = lgb.LGBMClassifier(verbosity = -1)\nmodel.fit(X_train, y_train)\n\n\nLGBMClassifier(verbosity=-1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifierLGBMClassifier(verbosity=-1)\n\n\n\n# Example usage:\nshap_explain = ShapExplain(model)\nraw_scores = shap_explain.predict_raw_scores(X_test)\nproba_scores = shap_explain.predict_proba(X_test)\nlabels = shap_explain.predict_labels(X_test)\nshap_values = shap_explain.calculate_shap_values(X_test)\nshap_contributions = shap_explain.split_shap_values()\nmean_abs_contributions = shap_explain.calculate_mean_absolute_shap_contributions()\n\n\nshap_explain.plot_shap_summary()",
    "crumbs": [
      "Machine learning",
      "LightGBM Predict Contrib for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/LightGBM_Predict_Contrib_for_Multi_Class.html#class-shapexplain",
    "href": "machine-learning/LightGBM_Predict_Contrib_for_Multi_Class.html#class-shapexplain",
    "title": "LightGBM Predict Contrib for Multi-Class Classification",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport shap\nimport matplotlib.pyplot as plt\nimport os\n\nclass ShapExplain:\n    def __init__(self, model):\n        if hasattr(model, 'classes_') and hasattr(model, 'feature_name_'):\n            self.model = model\n            self.raw_scores = None\n            self.proba_scores = None\n            self.labels = None\n            self.shap_values = None\n            self.shap_contributions = None\n\n            # Initialize class attributes from the model\n            self.num_classes = len(model.classes_)\n            self.num_features = len(model.feature_name_)\n            self.feature_names = model.feature_name_ + [\"Bias\"]\n        else:\n            raise ValueError(\"The provided model does not have the required attributes 'classes_' or 'feature_name_'.\")\n\n    def predict_raw_scores(self, X):\n        if hasattr(self.model, 'predict'):\n            self.raw_scores = self.model.predict(X, raw_score=True)\n            return self.raw_scores\n        else:\n            raise AttributeError(\"The model does not support raw score predictions.\")\n\n    def predict_proba(self, X):\n        if hasattr(self.model, 'predict_proba'):\n            self.proba_scores = self.model.predict_proba(X)\n            return self.proba_scores\n        else:\n            raise AttributeError(\"The model does not support probability predictions.\")\n\n    def predict_labels(self, X):\n        self.predict_proba(X)  # Ensure probabilities are always calculated\n        self.labels = [np.argmax(score) for score in self.proba_scores]\n        return self.labels\n\n    def calculate_shap_values(self, X):\n        if hasattr(self.model, 'predict'):\n            self.shap_values = self.model.predict(X, pred_contrib=True)\n            return self.shap_values\n        else:\n            raise AttributeError(\"The model does not support SHAP value predictions.\")\n\n    def split_shap_values(self):\n        if self.shap_values is None:\n            raise ValueError(\"SHAP values have not been calculated. Call calculate_shap_values first.\")\n        \n        shap_contributions = {}\n        for class_idx in range(self.num_classes):\n            start_idx = class_idx * (self.num_features + 1)\n            end_idx = start_idx + (self.num_features + 1)\n            shap_contributions[f\"Class_{class_idx}\"] = pd.DataFrame(\n                self.shap_values[:, start_idx:end_idx], columns=self.feature_names\n            )\n        self.shap_contributions = shap_contributions\n        return self.shap_contributions\n    \n    def calculate_mean_absolute_shap_contributions(self):\n        \"\"\"\n        Calculate the mean absolute SHAP contributions for each feature and class.\n\n        Returns:\n            pd.DataFrame: A DataFrame where rows represent features, columns represent classes,\n                        and values are the mean absolute SHAP contributions.\n        \"\"\"\n        if self.shap_contributions is None:\n            raise ValueError(\"SHAP contributions have not been split. Call split_shap_values first.\")\n        \n        mean_abs_contributions = {}\n        for class_name, shap_values_df in self.shap_contributions.items():\n            # Exclude the \"Bias\" column and calculate the mean absolute value\n            mean_abs_contributions[class_name] = shap_values_df.iloc[:, :-1].abs().mean(axis=0)\n        \n        # Combine into a DataFrame for easier interpretation\n        mean_abs_df = pd.DataFrame(mean_abs_contributions)\n        mean_abs_df.index.name = \"Feature\"\n        \n        return mean_abs_df\n\n    def plot_shap_summary(self, plot_type='bar', output_dir=None, title_prefix=None):\n        \n        # Ensure output directory exists if specified\n        if output_dir and not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        for class_name, shap_values_df in self.shap_contributions.items():\n            shap_values = shap_values_df.iloc[:, :-1].values\n            feature_names = shap_values_df.columns[:-1]  # Exclude 'Bias' from feature names\n\n            plt.figure()\n            shap.summary_plot(shap_values, feature_names=feature_names, plot_type=plot_type, show=False)\n\n            title = f'SHAP Summary Plot for {class_name}'\n            if title_prefix:\n                title = f'{title_prefix} {title}'\n            plt.title(title)\n\n            if output_dir:\n                filename = f'{title_prefix}_{class_name}_shap_summary.png' if title_prefix else f'{class_name}_shap_summary.png'\n                plt.savefig(os.path.join(output_dir, filename), bbox_inches='tight')\n                plt.close()\n            else:\n                plt.show()\n\n# Example usage:\n# Assuming `model` and `X_test` are defined, and the model is already fitted with attributes like classes_ and feature_names_.\n# shap_explain = ShapExplain(model)\n# raw_scores = shap_explain.predict_raw_scores(X_test)\n# proba_scores = shap_explain.predict_proba(X_test)\n# labels = shap_explain.predict_labels(X_test)\n# shap_values = shap_explain.calculate_shap_values(X_test)\n# shap_contributions = shap_explain.split_shap_values()\n\n# # Create a Pandas Excel writer using openpyxl as the engine\n# with pd.ExcelWriter('shap_explain_output.xlsx', engine='openpyxl') as writer:\n#     # Convert raw_scores and labels to DataFrames and write to Excel\n#     pd.DataFrame(raw_scores).to_excel(writer, sheet_name='Raw Scores')\n#     pd.DataFrame(proba_scores, columns=[f\"Class_{i}\" for i in range(shap_explain.num_classes)]).to_excel(writer, sheet_name='Probabilities')\n#     pd.DataFrame(labels, columns=['Predicted Labels']).to_excel(writer, sheet_name='Labels')\n\n#     # Write each class's SHAP contributions to a separate sheet\n#     for class_name, df in shap_contributions.items():\n#         df.to_excel(writer, sheet_name=f'SHAP Values {class_name}')\nClass ShapExplain được thiết kế để hỗ trợ việc giải thích các mô hình Machine Learning sử dụng SHAP (SHapley Additive exPlanations). Class này giúp người dùng: 1. Tính toán các giá trị SHAP cho mô hình. 2. Tách riêng các đóng góp SHAP theo lớp (class). 3. Tạo các biểu đồ SHAP để minh họa ảnh hưởng của từng đặc trưng đến dự đoán của mô hình.\nClass yêu cầu mô hình đầu vào phải có hai thuộc tính quan trọng: - classes_: Danh sách các lớp trong bài toán phân loại. - feature_name_: Danh sách các đặc trưng được sử dụng trong mô hình.\n\n\n\n__init__:\n\nKhởi tạo class với mô hình và kiểm tra mô hình có các thuộc tính cần thiết (classes_, feature_name_).\nGán các giá trị ban đầu (số lượng lớp, số lượng đặc trưng, v.v.).\n\npredict_raw_scores:\n\nDự đoán điểm số “thô” (raw scores) từ mô hình.\n\npredict_proba:\n\nDự đoán xác suất của từng lớp.\n\npredict_labels:\n\nDự đoán nhãn (label) bằng cách chọn lớp có xác suất cao nhất.\n\ncalculate_shap_values:\n\nTính giá trị SHAP bằng cách sử dụng phương pháp predict với pred_contrib=True.\n\nsplit_shap_values:\n\nPhân tách giá trị SHAP theo từng lớp (class) thành các DataFrame riêng biệt.\n\ncalculate_mean_absolute_shap_contributions:\n\nTính giá trị trung bình tuyệt đối của các đóng góp SHAP (mean absolute SHAP contributions) để hiểu đặc trưng nào quan trọng nhất.\n\nplot_shap_summary:\n\nTạo biểu đồ SHAP (summary plot) cho từng lớp, lưu vào thư mục hoặc hiển thị trực tiếp.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport shap\nimport matplotlib.pyplot as plt\nimport os\n\nclass ShapExplain:\n    def __init__(self, model):\n        if hasattr(model, 'classes_') and hasattr(model, 'feature_name_'):\n            self.model = model\n            self.raw_scores = None\n            self.proba_scores = None\n            self.labels = None\n            self.shap_values = None\n            self.shap_contributions = None\n\n            # Initialize class attributes from the model\n            self.num_classes = len(model.classes_)\n            self.num_features = len(model.feature_name_)\n            self.feature_names = model.feature_name_ + [\"Bias\"]\n        else:\n            raise ValueError(\"The provided model does not have the required attributes 'classes_' or 'feature_name_'.\")\n\n    def predict_raw_scores(self, X):\n        if hasattr(self.model, 'predict'):\n            self.raw_scores = self.model.predict(X, raw_score=True)\n            return self.raw_scores\n        else:\n            raise AttributeError(\"The model does not support raw score predictions.\")\n\n    def predict_proba(self, X):\n        if hasattr(self.model, 'predict_proba'):\n            self.proba_scores = self.model.predict_proba(X)\n            return self.proba_scores\n        else:\n            raise AttributeError(\"The model does not support probability predictions.\")\n\n    def predict_labels(self, X):\n        self.predict_proba(X)  # Ensure probabilities are always calculated\n        self.labels = [np.argmax(score) for score in self.proba_scores]\n        return self.labels\n\n    def calculate_shap_values(self, X):\n        if hasattr(self.model, 'predict'):\n            self.shap_values = self.model.predict(X, pred_contrib=True)\n            return self.shap_values\n        else:\n            raise AttributeError(\"The model does not support SHAP value predictions.\")\n\n    def split_shap_values(self):\n        if self.shap_values is None:\n            raise ValueError(\"SHAP values have not been calculated. Call calculate_shap_values first.\")\n        \n        shap_contributions = {}\n        for class_idx in range(self.num_classes):\n            start_idx = class_idx * (self.num_features + 1)\n            end_idx = start_idx + (self.num_features + 1)\n            shap_contributions[f\"Class_{class_idx}\"] = pd.DataFrame(\n                self.shap_values[:, start_idx:end_idx], columns=self.feature_names\n            )\n        self.shap_contributions = shap_contributions\n        return self.shap_contributions\n    \n    def calculate_mean_absolute_shap_contributions(self):\n        \"\"\"\n        Calculate the mean absolute SHAP contributions for each feature and class.\n\n        Returns:\n            pd.DataFrame: A DataFrame where rows represent features, columns represent classes,\n                        and values are the mean absolute SHAP contributions.\n        \"\"\"\n        if self.shap_contributions is None:\n            raise ValueError(\"SHAP contributions have not been split. Call split_shap_values first.\")\n        \n        mean_abs_contributions = {}\n        for class_name, shap_values_df in self.shap_contributions.items():\n            # Exclude the \"Bias\" column and calculate the mean absolute value\n            mean_abs_contributions[class_name] = shap_values_df.iloc[:, :-1].abs().mean(axis=0)\n        \n        # Combine into a DataFrame for easier interpretation\n        mean_abs_df = pd.DataFrame(mean_abs_contributions)\n        mean_abs_df.index.name = \"Feature\"\n        \n        return mean_abs_df\n\n    def plot_shap_summary(self, plot_type='bar', output_dir=None, title_prefix=None):\n        \n        # Ensure output directory exists if specified\n        if output_dir and not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        for class_name, shap_values_df in self.shap_contributions.items():\n            shap_values = shap_values_df.iloc[:, :-1].values\n            feature_names = shap_values_df.columns[:-1]  # Exclude 'Bias' from feature names\n\n            plt.figure()\n            shap.summary_plot(shap_values, feature_names=feature_names, plot_type=plot_type, show=False)\n\n            title = f'SHAP Summary Plot for {class_name}'\n            if title_prefix:\n                title = f'{title_prefix} {title}'\n            plt.title(title)\n\n            if output_dir:\n                filename = f'{title_prefix}_{class_name}_shap_summary.png' if title_prefix else f'{class_name}_shap_summary.png'\n                plt.savefig(os.path.join(output_dir, filename), bbox_inches='tight')\n                plt.close()\n            else:\n                plt.show()\n\n\n# LightGBM Predict Contrib for Multi-Class Classification\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport shap\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Step 1: Load Dataset\nwine = load_wine()\nX = pd.DataFrame(wine.data, columns=wine.feature_names)\ny = wine.target\n\n# Step 2: Split Dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Step 3: Train LightGBM Model\nmodel = lgb.LGBMClassifier(verbosity = -1)\nmodel.fit(X_train, y_train)\n\n\nLGBMClassifier(verbosity=-1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifierLGBMClassifier(verbosity=-1)\n\n\n\n# Example usage:\nshap_explain = ShapExplain(model)\nraw_scores = shap_explain.predict_raw_scores(X_test)\nproba_scores = shap_explain.predict_proba(X_test)\nlabels = shap_explain.predict_labels(X_test)\nshap_values = shap_explain.calculate_shap_values(X_test)\nshap_contributions = shap_explain.split_shap_values()\nmean_abs_contributions = shap_explain.calculate_mean_absolute_shap_contributions()\n\n\nshap_explain.plot_shap_summary()",
    "crumbs": [
      "Machine learning",
      "LightGBM Predict Contrib for Multi-Class Classification"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html",
    "href": "machine-learning/transformer.html",
    "title": "Transformer",
    "section": "",
    "text": "Kiến trúc Transformer là một mô hình học sâu phổ biến được giới thiệu trong bài báo “Attention is All You Need” của Vaswani và đồng nghiệp (2017). Nó đã cách mạng hóa nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và trở thành cơ sở cho các mô hình tiên tiến như BERT và GPT.\nKiến trúc Transformer dựa trên khái niệm self-attention, cho phép mô hình tập trung vào các phần khác nhau của chuỗi đầu vào khi xử lý từng phần tử. Dưới đây là một cái nhìn tổng quan về kiến trúc Transformer:\n\n\n\nKiến trúc Transformer\n\n\n\nNhúng Đầu Vào (Input Embeddings):\n\nChuỗi đầu vào được nhúng thành các biểu diễn liên tục được gọi là nhúng đầu vào.\nMỗi từ hoặc thành phần trong chuỗi đầu vào được biểu diễn bằng một vector mật độ.\n\nMã Hóa Vị Trí (Positional Encoding):\n\nMã hóa vị trí được thêm vào nhúng đầu vào để cung cấp thông tin về thứ tự hoặc vị trí của các từ trong chuỗi.\nĐiều này cho phép mô hình bắt chước thông tin tuần tự, vì kiến trúc Transformer không có kết nối lặp lại.\n\nBộ Mã Hóa (Encoder):\n\nTransformer bao gồm một cấu trúc mã hóa-giải mã (encoder-decoder), nhưng để đơn giản, chúng ta tập trung vào bộ mã hóa.\nBộ mã hóa bao gồm nhiều lớp giống nhau, mỗi lớp bao gồm hai lớp con:\n\nMulti-head Self-Attention:\n\nSelf-attention cho phép mô hình đánh trọng số sự quan trọng của các từ khác nhau trong chuỗi đầu vào khi xử lý một từ cụ thể.\nNó tính toán tổng trọng số của các nhúng từ tất cả các từ, trong đó trọng số được xác định bởi sự tương đồng giữa các từ.\n\nFeed-Forward Neural Network:\n\nA simple feed-forward neural network được áp dụng cho từng vị trí trong chuỗi độc lập.\nNó giúp bắt chước mối quan hệ phi tuyến giữa các từ trong chuỗi.\n\n\n\nBộ Giải Mã (Decoder):\n\nBộ giải mã tương tự như bộ mã hóa nhưng có cơ chế attention bổ sung.\nNó tạo ra chuỗi đầu ra bằng cách chú ý đến các biểu diễn đầu ra của bộ mã hóa và các từ đã được tạo ra trước đó.\n\nCơ Chế Che (Masking):\n\nTrong quá trình huấn luyện, cơ chế che được sử dụng để ngăn mô hình chú ý đến các vị trí trong tương lai.\nĐiều này đảm bảo rằng mỗi vị trí chỉ có thể chú ý đến các vị trí trước đó trong chuỗi đầu vào.\n\nTạo Ra Đầu Ra:\n\nLớp cuối cùng của bộ giải mã tạo ra xác suất đầu ra cho mỗi vị trí trong chuỗi đầu ra.\nCác xác suất này thường được tính bằng cách sử dụng hàm kích hoạt softmax.\n\n\nKiến trúc Transformer có một số lợi thế, như khả năng song song hóa, khả năng bắt chước các phụ thuộc xa, và khả năng tổng quát tốt hơn nhờ self-attention. Nó đã đạt được kết quả tiên tiến trên nhiều nhiệm vụ NLP, bao gồm dịch máy, tóm tắt văn bản và hiểu ngôn ngữ.\n\n\n\n\n\nSelf-attention, còn được gọi là intra-attention hay scaled dot-product attention (tích vô hướng giữa 2 vector), là một cơ chế tính toán trọng số chú ý cho các phần tử trong cùng một chuỗi đầu vào. Nó cho phép mô hình đánh giá mức độ quan trọng của mỗi phần tử trong chuỗi dựa trên mức liên quan của nó đối với các phần tử khác.\n\n\n\nTrong self-attention, mỗi từ trong chuỗi đầu vào được biểu diễn bằng một vector, thông thường được gọi là nhúng hay vector nhúng (embedding vector). Giả sử nhúng từ cho chuỗi đầu vào như sau:\nVí dụ câu như sau: “Con mèo ngồi trên chiếc thảm.”\nĐể áp dụng self-attention, chúng ta trước tiên biểu diễn mỗi từ trong câu thành một vector nhúng. Hãy giả sử chúng ta có các vector nhúng từ sau đây:\nNhúng từ:\n- \"Con\": [0.2, 0.5, -0.3]\n- \"mèo\": [-0.1, 0.7, 0.2]\n- \"ngồi\": [0.4, -0.2, 0.6]\n- \"trên\": [-0.5, 0.3, -0.1]\n- \"chiếc\": [0.2, 0.5, -0.3]\n- \"thảm\": [0.3, 0.1, 0.8]\n- \".\": [0.0, 0.0, 0.0]\nBây giờ, chúng ta tính toán trọng số self-attention cho mỗi từ trong câu. Trong self-attention, mỗi từ được so sánh với tất cả các từ khác để xác định độ quan trọng hoặc liên quan của nó. Quá trình so sánh được thực hiện bằng cách tính tích vô hướng giữa nhúng từ, sau đó áp dụng phép softmax để thu được trọng số chú ý.\nVí dụ, đối với từ “Con,” tích vô hướng được tính toán với nhúng từ của tất cả các từ khác:\nTrọng số Chú ý cho “Con”:\n- \"Con\" so sánh với \"Con\": 0.3\n- \"Con\" so sánh với \"mèo\": 0.1\n- \"Con\" so sánh với \"ngồi\": 0.2\n- \"Con\" so sánh với \"trên\": 0.05\n- \"Con\" so sánh với \"chiếc\": 0.35\n- \"Con\" so sánh với \"thảm\": 0.0\n- \"Con\" so sánh với \".\": 0.0\nCác trọng số chú ý phản ánh sự quan trọng của mỗi từ đối với “Con” dựa trên sự tương đồng ngữ nghĩa hoặc liên quan ngữ cảnh của chúng. Phép softmax đảm bảo rằng các trọng số tổng cộng bằng 1.\nChúng ta lặp lại quá trình này cho mỗi từ trong câu để thu được trọng số self-attention cho toàn bộ câu. Những trọng số chú ý này sau đó có thể được sử dụng để tổng hợp thông tin từ các từ khác nhau và bắt chước các mối quan hệ quan trọng trong câu để tiếp tục xử lý trong mô hình.\n\n\n\n\n\n\nMulti-Head Attention là một phần mở rộng của self-attention cho phép mô hình tập trung vào các khía cạnh khác nhau của chuỗi đầu vào cùng một lúc. Đó là việc áp dụng self-attention nhiều lần song song, mỗi attention head tập trung vào một biểu diễn khác nhau của đầu vào. Bằng cách sử dụng nhiều attention head, mô hình có thể nắm bắt các thông tin khác nhau và học các mẫu đa dạng, nâng cao khả năng hiểu và xử lý các chuỗi phức tạp một cách hiệu quả.\n\n\n\n\nXem xét một câu tiếng Anh: “I love cats.” -&gt; Và chúng ta muốn dịch nó sang tiếng Pháp: “J’adore les chats.”\nGiả sử cả hai câu tiếng Anh và tiếng Pháp đã được nhúng thành các vector như sau:\nNhúng từ tiếng Anh:\n\n“I”: [0.1, 0.2, 0.3]\n“love”: [0.4, 0.5, 0.6]\n“cats”: [0.7, 0.8, 0.9]\n\nNhúng từ tiếng Pháp:\n\n“J’adore”: [0.9, 0.8, 0.7]\n“les”: [0.6, 0.5, 0.4]\n“chats”: [0.3, 0.2, 0.1]\n\nTính toán trọng số\n\nGiả sử chúng ta đang sử dụng cơ chế multi-head attention với 2 attention head. Mỗi head sẽ tập trung vào chuỗi đầu vào tiếng Anh (nguồn) và chuỗi đầu vào tiếng Pháp (đích) riêng biệt.\nVới mỗi attention head, chúng ta sẽ tính toán trọng số attention cho cả chuỗi đầu vào tiếng Anh và tiếng Pháp. Các trọng số attention xác định mức độ quan trọng của mỗi từ trong chuỗi nguồn liên quan đến các từ trong chuỗi đích.\nVí dụ,\n\nVới attention head đầu tiên, chúng ta tính toán trọng số attention cho chuỗi đầu vào tiếng Anh:\nAttention_scores_source_head1 = softmax(dot_product(english_source_embeddings, [french_target_embedding1, french_target_embedding2, french_target_embedding3]))\nTương tự, chúng ta tính toán trọng số attention cho chuỗi đích tiếng Pháp:\nAttention_scores_target_head1 = softmax(dot_product(french_target_embeddings, [english_source_embedding1, english_source_embedding2, english_source_embedding3]))\nLặp lại các bước trên cho attention head tiếp theo, kết quả sẽ được các attention score cho cả chuỗi nguồn và chuỗi đích.\nSau khi có các trọng số attention, chúng ta áp dụng chúng vào nhúng tương ứng để có tổng có trọng số cho mỗi chuỗi.\nVí dụ, đối với chuỗi nguồn tiếng Anh và attention head đầu tiên:\nWeighted_sum_source_head1 = attention_score1 * english_source_embedding1 + attention_score2 * english_source_embedding2 + attention_score3 * english_source_embedding3\nTương tự, đối với chuỗi đích tiếng Pháp và attention head đầu tiên:\nWeighted_sum_target_head1 = attention_score1 * french_target_embedding1 + attention_score2 * french_target_embedding2 + attention_score3 * french_target_embedding3\nChúng ta lặp lại các bước trên cho attention head thứ hai.\nCuối cùng, chúng ta nối các đầu ra từ mỗi attention head và thông qua một phép biến đổi tuyến tính để có đầu ra multi-head attention cuối cùng.\nNhư vậy, cơ chế multi-head attention cho phép mô hình nắm bắt các thông tin và mối quan hệ khác nhau giữa chuỗi nguồn và chuỗi đích, tăng khả năng dịch giữa các ngôn ngữ một cách chính xác.\n\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.query_projection = nn.Linear(embed_dim, embed_dim)\n        self.key_projection = nn.Linear(embed_dim, embed_dim)\n        self.value_projection = nn.Linear(embed_dim, embed_dim)\n        self.output_projection = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n        \n        # Apply linear projections for query, key, and value\n        query = self.query_projection(query)\n        key = self.key_projection(key)\n        value = self.value_projection(value)\n        \n        # Reshape query, key, and value to split heads\n        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Compute attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n        attention_weights = torch.softmax(scores, dim=-1)\n        \n        # Apply attention weights to value\n        attended_values = torch.matmul(attention_weights, value)\n        \n        # Reshape and concatenate attended values\n        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n        \n        # Apply linear projection for output\n        output = self.output_projection(attended_values)\n        \n        return output\n\n\n\n\n\n\n\nPhép mã hóa vị trí (Positional Encoding) là một kỹ thuật được sử dụng trong các mô hình dựa trên chuỗi, đặc biệt là trong kiến trúc Transformer, để đưa thông tin về vị trí tương đối của các yếu tố trong chuỗi đầu vào. Nó giải quyết vấn đề rằng tự chú ý (self-attention) và các cơ chế chú ý khác trong các mô hình này không ngầm định mã hóa thông tin về vị trí.\nTrong các nhiệm vụ xử lý ngôn ngữ tự nhiên, thứ tự của các từ trong một câu mang ý nghĩa và ngữ cảnh quan trọng. Tuy nhiên, mô hình Transformer xử lý chuỗi đầu vào theo cách song song, không rõ ràng ghi nhận thứ tự tuần tự. Phép mã hóa vị trí giúp mô hình phân biệt giữa các yếu tố ở các vị trí khác nhau trong chuỗi.\nMã hóa vị trí được thêm vào nhúng đầu vào của chuỗi trước khi đưa chúng vào mô hình Transformer. Thông thường, điều này được thực hiện bằng cách kết hợp các hàm sin-cos với các nhúng đầu vào. Mỗi chiều của mã hóa vị trí đại diện cho một vị trí cụ thể trong chuỗi.\nSự lựa chọn các hàm sin-cos cho phép mô hình dễ dàng mở rộng cho các độ dài chuỗi dài hơn so với những gì mô hình đã được huấn luyện. Tần số của các hàm sin-cos tuân theo một cấp số học, cho phép mô hình học để chú ý đến các vị trí khác nhau với các tỷ lệ khác nhau.\nBằng cách thêm mã hóa vị trí vào nhúng đầu vào, mô hình Transformer có thể hiệu quả nắm bắt thứ tự và vị trí tương đối của các yếu tố trong chuỗi. Điều này giúp mô hình hiểu các phụ thuộc và mối quan hệ giữa các vị trí khác nhau và cho phép thực hiện các nhiệm vụ yêu cầu xử lý thông tin tuần tự, chẳng hạn như dịch máy hoặc hiểu ngôn ngữ.\n\n\n\n\nGiả sử chúng ta có một chuỗi đầu vào gồm ba từ: [“Tôi”, “yêu”, “mèo”].\nMỗi từ sẽ được biểu diễn bằng một vector nhúng từ, nắm bắt ý nghĩa ngữ nghĩa của nó. Hãy giả định các vector nhúng từ như sau:\n\n“Tôi”: [0.2, 0.3, -0.1]\n“yêu”: [-0.5, 0.7, 0.2]\n“mèo”: [0.7, 0.8, 0.9]\n\nĐể tích hợp thông tin về vị trí, chúng ta sử dụng mã hóa vị trí. Trong ví dụ này, chúng ta sẽ sử dụng các hàm sin và cos để tạo các vector mã hóa vị trí.\nGiả sử chiều nhúng là 3, các vector mã hóa vị trí sẽ được tính như sau:\nCho vị trí 0 (từ đầu tiên): - PE(0) = [sin(0/10000^0), cos(0/10000^0), sin(0/10000^0)]\nCho vị trí 1 (từ thứ hai): - PE(1) = [sin(1/10000^1), cos(1/10000^1), sin(1/10000^1)]\nCho vị trí 2 (từ thứ ba): - PE(2) = [sin(2/10000^0), cos(2/10000^0), sin(2/10000^0)]\nVì sin(0) = 0, sin(1) = 0.8415, sin(2) = 0.9093, cos(0) = 1, cos(1) = 0.5403, cos(2) = -0.4161.\nNên, các vector mã hóa vị trí là:\nPE(0) = [0, 1, 0] PE(1) = [0.8415, 0.5403, 0.8415] PE(2) = [0.9093, -0.4161, 0.9093]\nĐể tích hợp mã hóa vị trí, chúng ta cộng vector mã hóa vị trí tương ứng vào mỗi vector nhúng từ:\n\nTừ “Tôi” với mã hóa vị trí: [0.2, 0.3, -0.1] + [0, 1, 0] = [0.2, 1.3, -0.1]\nTừ “yêu” với mã hóa vị trí: [-0.5, 0.7, 0.2] + [0.8415, 0.5403, 0.8415] =\n\n[0.3415, 1.2403, 1.0415] - Từ “mèo” với mã hóa vị trí: [0.7, 0.8, 0.9] + [0.9093, -0.4161, 0.9093] = [1.6093, 0.3839, 1.8093]\nBằng cách cộng các vector mã hóa vị trí vào các vector nhúng từ, chúng ta giới thiệu thông tin về vị trí tương đối của các từ trong chuỗi đầu vào. Điều này cho phép mô hình Transformer nắm bắt thứ tự tuần tự và các phụ thuộc giữa các từ, điều quan trọng cho các nhiệm vụ như dịch máy hoặc phân tích cảm xúc.\n\n\n\n\nLayer Normalization là một kỹ thuật được sử dụng trong các mô hình học sâu để chuẩn hóa các kích hoạt (kết quả chuyển đổi đầu vào thành đầu ra bằng hàm kích hoạt - tạm gọi là các kích hoạt) của các đơn vị trong một lớp. Nó giải quyết vấn đề của sự thay đổi phân phối đầu vào của lớp trong quá trình huấn luyện, gây khó khăn cho quá trình học.\nCông thức cho Layer Normalization có thể được biểu diễn như sau:\noutput = scale * (input - mean) / sqrt(variance + epsilon) + bias\n\nỞ đây, đầu vào đại diện cho các kích hoạt đầu vào của một lớp, trung bình và phương sai đại diện cho giá trị trung bình và phương sai được tính theo chiều đầu vào, epsilon là một hằng số nhỏ được thêm vào để đảm bảo tính ổn định số học, và scale và bias là các tham số có thể học giúp mô hình điều chỉnh và dịch chuyển các kích hoạt đã được chuẩn hóa.\nLayer Normalization được áp dụng độc lập cho mỗi lô dữ liệu (batch), điều này làm cho nó phù hợp cho các nhiệm vụ với kích thước lô nhỏ. Nó đã được chứng minh là hiệu quả trong ổn định quá trình huấn luyện, tăng tốc quá trình hội tụ và cải thiện hiệu suất tổng quát của các mô hình học sâu.\nTổng quát, Layer Normalization giúp giảm sự phụ thuộc vào tỷ lệ của các kích hoạt đầu vào, thúc đẩy việc truyền gradient ổn định và cải thiện động học huấn luyện tổng thể (overall training dynamics - gồm learning rate, gradient, loss rate) của mạng neural. Nó đã được sử dụng rộng rãi trong các lĩnh vực khác nhau, bao gồm xử lý ngôn ngữ tự nhiên, thị giác máy tính và nhận dạng giọng nói.\n\n\n\nGiả sử chúng ta có một câu: “Tôi thích chơi bóng đá.”\nTrong mô hình ngôn ngữ, mục tiêu là dự đoán từ tiếp theo trong ngữ cảnh đã cho. Tuy nhiên, trong quá trình huấn luyện, chúng ta thường không muốn cho mô hình truy cập vào các từ trong tương lai vì nó có thể dẫn đến rò rỉ dữ liệu và dự đoán không thực tế. Vì vậy, chế che được áp dụng.\nĐể áp dụng cơ chế che, thường được ký hiệu là [MASK], để thay thế một số từ trong chuỗi đầu vào trong quá trình huấn luyện. Trong trường hợp này, chúng ta có thể ngẫu nhiên che một số từ trong câu, dẫn đến:\nCâu Đầu Vào (có cơ chế che): “Tôi thích chơi [MASK] bóng đá.”\nBây giờ, trong quá trình huấn luyện, mục tiêu của mô hình là dự đoán từ bị che dựa trên ngữ cảnh xung quanh. Nó học cách hiểu cấu trúc câu, ý nghĩa và sự phụ thuộc giữa các từ để đưa ra dự đoán chính xác. Mô hình nhận chuỗi đầu vào đã được chỉnh sửa (thêm [MASK]) và cố gắng dự đoán bản gốc, không bị che, ở vị trí đã được che.\nTrong quá trình huấn luyện hoặc đánh giá, cơ chế che không được sử dụng và mô hình được cung cấp với toàn bộ chuỗi đầu vào. Nó có thể tạo ra dự đoán cho từ tiếp theo trong chuỗi dựa trên kiến thức đã học từ quá trình huấn luyện.\nCơ chế che giúp mô hình tổng quát tốt và xử lý các trường hợp mà nó gặp phải các từ chưa được nhìn thấy hoặc thiếu. Nó khuyến khích mô hình dựa vào ngữ cảnh có sẵn để đưa ra dự đoán chính xác và cải thiện khả năng hiểu cấu trúc câu.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#transformer-architecture",
    "href": "machine-learning/transformer.html#transformer-architecture",
    "title": "Transformer",
    "section": "",
    "text": "Kiến trúc Transformer là một mô hình học sâu phổ biến được giới thiệu trong bài báo “Attention is All You Need” của Vaswani và đồng nghiệp (2017). Nó đã cách mạng hóa nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và trở thành cơ sở cho các mô hình tiên tiến như BERT và GPT.\nKiến trúc Transformer dựa trên khái niệm self-attention, cho phép mô hình tập trung vào các phần khác nhau của chuỗi đầu vào khi xử lý từng phần tử. Dưới đây là một cái nhìn tổng quan về kiến trúc Transformer:\n\n\n\nKiến trúc Transformer\n\n\n\nNhúng Đầu Vào (Input Embeddings):\n\nChuỗi đầu vào được nhúng thành các biểu diễn liên tục được gọi là nhúng đầu vào.\nMỗi từ hoặc thành phần trong chuỗi đầu vào được biểu diễn bằng một vector mật độ.\n\nMã Hóa Vị Trí (Positional Encoding):\n\nMã hóa vị trí được thêm vào nhúng đầu vào để cung cấp thông tin về thứ tự hoặc vị trí của các từ trong chuỗi.\nĐiều này cho phép mô hình bắt chước thông tin tuần tự, vì kiến trúc Transformer không có kết nối lặp lại.\n\nBộ Mã Hóa (Encoder):\n\nTransformer bao gồm một cấu trúc mã hóa-giải mã (encoder-decoder), nhưng để đơn giản, chúng ta tập trung vào bộ mã hóa.\nBộ mã hóa bao gồm nhiều lớp giống nhau, mỗi lớp bao gồm hai lớp con:\n\nMulti-head Self-Attention:\n\nSelf-attention cho phép mô hình đánh trọng số sự quan trọng của các từ khác nhau trong chuỗi đầu vào khi xử lý một từ cụ thể.\nNó tính toán tổng trọng số của các nhúng từ tất cả các từ, trong đó trọng số được xác định bởi sự tương đồng giữa các từ.\n\nFeed-Forward Neural Network:\n\nA simple feed-forward neural network được áp dụng cho từng vị trí trong chuỗi độc lập.\nNó giúp bắt chước mối quan hệ phi tuyến giữa các từ trong chuỗi.\n\n\n\nBộ Giải Mã (Decoder):\n\nBộ giải mã tương tự như bộ mã hóa nhưng có cơ chế attention bổ sung.\nNó tạo ra chuỗi đầu ra bằng cách chú ý đến các biểu diễn đầu ra của bộ mã hóa và các từ đã được tạo ra trước đó.\n\nCơ Chế Che (Masking):\n\nTrong quá trình huấn luyện, cơ chế che được sử dụng để ngăn mô hình chú ý đến các vị trí trong tương lai.\nĐiều này đảm bảo rằng mỗi vị trí chỉ có thể chú ý đến các vị trí trước đó trong chuỗi đầu vào.\n\nTạo Ra Đầu Ra:\n\nLớp cuối cùng của bộ giải mã tạo ra xác suất đầu ra cho mỗi vị trí trong chuỗi đầu ra.\nCác xác suất này thường được tính bằng cách sử dụng hàm kích hoạt softmax.\n\n\nKiến trúc Transformer có một số lợi thế, như khả năng song song hóa, khả năng bắt chước các phụ thuộc xa, và khả năng tổng quát tốt hơn nhờ self-attention. Nó đã đạt được kết quả tiên tiến trên nhiều nhiệm vụ NLP, bao gồm dịch máy, tóm tắt văn bản và hiểu ngôn ngữ.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#self-attention",
    "href": "machine-learning/transformer.html#self-attention",
    "title": "Transformer",
    "section": "",
    "text": "Self-attention, còn được gọi là intra-attention hay scaled dot-product attention (tích vô hướng giữa 2 vector), là một cơ chế tính toán trọng số chú ý cho các phần tử trong cùng một chuỗi đầu vào. Nó cho phép mô hình đánh giá mức độ quan trọng của mỗi phần tử trong chuỗi dựa trên mức liên quan của nó đối với các phần tử khác.\n\n\n\nTrong self-attention, mỗi từ trong chuỗi đầu vào được biểu diễn bằng một vector, thông thường được gọi là nhúng hay vector nhúng (embedding vector). Giả sử nhúng từ cho chuỗi đầu vào như sau:\nVí dụ câu như sau: “Con mèo ngồi trên chiếc thảm.”\nĐể áp dụng self-attention, chúng ta trước tiên biểu diễn mỗi từ trong câu thành một vector nhúng. Hãy giả sử chúng ta có các vector nhúng từ sau đây:\nNhúng từ:\n- \"Con\": [0.2, 0.5, -0.3]\n- \"mèo\": [-0.1, 0.7, 0.2]\n- \"ngồi\": [0.4, -0.2, 0.6]\n- \"trên\": [-0.5, 0.3, -0.1]\n- \"chiếc\": [0.2, 0.5, -0.3]\n- \"thảm\": [0.3, 0.1, 0.8]\n- \".\": [0.0, 0.0, 0.0]\nBây giờ, chúng ta tính toán trọng số self-attention cho mỗi từ trong câu. Trong self-attention, mỗi từ được so sánh với tất cả các từ khác để xác định độ quan trọng hoặc liên quan của nó. Quá trình so sánh được thực hiện bằng cách tính tích vô hướng giữa nhúng từ, sau đó áp dụng phép softmax để thu được trọng số chú ý.\nVí dụ, đối với từ “Con,” tích vô hướng được tính toán với nhúng từ của tất cả các từ khác:\nTrọng số Chú ý cho “Con”:\n- \"Con\" so sánh với \"Con\": 0.3\n- \"Con\" so sánh với \"mèo\": 0.1\n- \"Con\" so sánh với \"ngồi\": 0.2\n- \"Con\" so sánh với \"trên\": 0.05\n- \"Con\" so sánh với \"chiếc\": 0.35\n- \"Con\" so sánh với \"thảm\": 0.0\n- \"Con\" so sánh với \".\": 0.0\nCác trọng số chú ý phản ánh sự quan trọng của mỗi từ đối với “Con” dựa trên sự tương đồng ngữ nghĩa hoặc liên quan ngữ cảnh của chúng. Phép softmax đảm bảo rằng các trọng số tổng cộng bằng 1.\nChúng ta lặp lại quá trình này cho mỗi từ trong câu để thu được trọng số self-attention cho toàn bộ câu. Những trọng số chú ý này sau đó có thể được sử dụng để tổng hợp thông tin từ các từ khác nhau và bắt chước các mối quan hệ quan trọng trong câu để tiếp tục xử lý trong mô hình.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#multi-head-attention",
    "href": "machine-learning/transformer.html#multi-head-attention",
    "title": "Transformer",
    "section": "",
    "text": "Multi-Head Attention là một phần mở rộng của self-attention cho phép mô hình tập trung vào các khía cạnh khác nhau của chuỗi đầu vào cùng một lúc. Đó là việc áp dụng self-attention nhiều lần song song, mỗi attention head tập trung vào một biểu diễn khác nhau của đầu vào. Bằng cách sử dụng nhiều attention head, mô hình có thể nắm bắt các thông tin khác nhau và học các mẫu đa dạng, nâng cao khả năng hiểu và xử lý các chuỗi phức tạp một cách hiệu quả.\n\n\n\n\nXem xét một câu tiếng Anh: “I love cats.” -&gt; Và chúng ta muốn dịch nó sang tiếng Pháp: “J’adore les chats.”\nGiả sử cả hai câu tiếng Anh và tiếng Pháp đã được nhúng thành các vector như sau:\nNhúng từ tiếng Anh:\n\n“I”: [0.1, 0.2, 0.3]\n“love”: [0.4, 0.5, 0.6]\n“cats”: [0.7, 0.8, 0.9]\n\nNhúng từ tiếng Pháp:\n\n“J’adore”: [0.9, 0.8, 0.7]\n“les”: [0.6, 0.5, 0.4]\n“chats”: [0.3, 0.2, 0.1]\n\nTính toán trọng số\n\nGiả sử chúng ta đang sử dụng cơ chế multi-head attention với 2 attention head. Mỗi head sẽ tập trung vào chuỗi đầu vào tiếng Anh (nguồn) và chuỗi đầu vào tiếng Pháp (đích) riêng biệt.\nVới mỗi attention head, chúng ta sẽ tính toán trọng số attention cho cả chuỗi đầu vào tiếng Anh và tiếng Pháp. Các trọng số attention xác định mức độ quan trọng của mỗi từ trong chuỗi nguồn liên quan đến các từ trong chuỗi đích.\nVí dụ,\n\nVới attention head đầu tiên, chúng ta tính toán trọng số attention cho chuỗi đầu vào tiếng Anh:\nAttention_scores_source_head1 = softmax(dot_product(english_source_embeddings, [french_target_embedding1, french_target_embedding2, french_target_embedding3]))\nTương tự, chúng ta tính toán trọng số attention cho chuỗi đích tiếng Pháp:\nAttention_scores_target_head1 = softmax(dot_product(french_target_embeddings, [english_source_embedding1, english_source_embedding2, english_source_embedding3]))\nLặp lại các bước trên cho attention head tiếp theo, kết quả sẽ được các attention score cho cả chuỗi nguồn và chuỗi đích.\nSau khi có các trọng số attention, chúng ta áp dụng chúng vào nhúng tương ứng để có tổng có trọng số cho mỗi chuỗi.\nVí dụ, đối với chuỗi nguồn tiếng Anh và attention head đầu tiên:\nWeighted_sum_source_head1 = attention_score1 * english_source_embedding1 + attention_score2 * english_source_embedding2 + attention_score3 * english_source_embedding3\nTương tự, đối với chuỗi đích tiếng Pháp và attention head đầu tiên:\nWeighted_sum_target_head1 = attention_score1 * french_target_embedding1 + attention_score2 * french_target_embedding2 + attention_score3 * french_target_embedding3\nChúng ta lặp lại các bước trên cho attention head thứ hai.\nCuối cùng, chúng ta nối các đầu ra từ mỗi attention head và thông qua một phép biến đổi tuyến tính để có đầu ra multi-head attention cuối cùng.\nNhư vậy, cơ chế multi-head attention cho phép mô hình nắm bắt các thông tin và mối quan hệ khác nhau giữa chuỗi nguồn và chuỗi đích, tăng khả năng dịch giữa các ngôn ngữ một cách chính xác.\n\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.query_projection = nn.Linear(embed_dim, embed_dim)\n        self.key_projection = nn.Linear(embed_dim, embed_dim)\n        self.value_projection = nn.Linear(embed_dim, embed_dim)\n        self.output_projection = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n        \n        # Apply linear projections for query, key, and value\n        query = self.query_projection(query)\n        key = self.key_projection(key)\n        value = self.value_projection(value)\n        \n        # Reshape query, key, and value to split heads\n        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Compute attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n        attention_weights = torch.softmax(scores, dim=-1)\n        \n        # Apply attention weights to value\n        attended_values = torch.matmul(attention_weights, value)\n        \n        # Reshape and concatenate attended values\n        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n        \n        # Apply linear projection for output\n        output = self.output_projection(attended_values)\n        \n        return output",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#positional-encoding",
    "href": "machine-learning/transformer.html#positional-encoding",
    "title": "Transformer",
    "section": "",
    "text": "Phép mã hóa vị trí (Positional Encoding) là một kỹ thuật được sử dụng trong các mô hình dựa trên chuỗi, đặc biệt là trong kiến trúc Transformer, để đưa thông tin về vị trí tương đối của các yếu tố trong chuỗi đầu vào. Nó giải quyết vấn đề rằng tự chú ý (self-attention) và các cơ chế chú ý khác trong các mô hình này không ngầm định mã hóa thông tin về vị trí.\nTrong các nhiệm vụ xử lý ngôn ngữ tự nhiên, thứ tự của các từ trong một câu mang ý nghĩa và ngữ cảnh quan trọng. Tuy nhiên, mô hình Transformer xử lý chuỗi đầu vào theo cách song song, không rõ ràng ghi nhận thứ tự tuần tự. Phép mã hóa vị trí giúp mô hình phân biệt giữa các yếu tố ở các vị trí khác nhau trong chuỗi.\nMã hóa vị trí được thêm vào nhúng đầu vào của chuỗi trước khi đưa chúng vào mô hình Transformer. Thông thường, điều này được thực hiện bằng cách kết hợp các hàm sin-cos với các nhúng đầu vào. Mỗi chiều của mã hóa vị trí đại diện cho một vị trí cụ thể trong chuỗi.\nSự lựa chọn các hàm sin-cos cho phép mô hình dễ dàng mở rộng cho các độ dài chuỗi dài hơn so với những gì mô hình đã được huấn luyện. Tần số của các hàm sin-cos tuân theo một cấp số học, cho phép mô hình học để chú ý đến các vị trí khác nhau với các tỷ lệ khác nhau.\nBằng cách thêm mã hóa vị trí vào nhúng đầu vào, mô hình Transformer có thể hiệu quả nắm bắt thứ tự và vị trí tương đối của các yếu tố trong chuỗi. Điều này giúp mô hình hiểu các phụ thuộc và mối quan hệ giữa các vị trí khác nhau và cho phép thực hiện các nhiệm vụ yêu cầu xử lý thông tin tuần tự, chẳng hạn như dịch máy hoặc hiểu ngôn ngữ.\n\n\n\n\nGiả sử chúng ta có một chuỗi đầu vào gồm ba từ: [“Tôi”, “yêu”, “mèo”].\nMỗi từ sẽ được biểu diễn bằng một vector nhúng từ, nắm bắt ý nghĩa ngữ nghĩa của nó. Hãy giả định các vector nhúng từ như sau:\n\n“Tôi”: [0.2, 0.3, -0.1]\n“yêu”: [-0.5, 0.7, 0.2]\n“mèo”: [0.7, 0.8, 0.9]\n\nĐể tích hợp thông tin về vị trí, chúng ta sử dụng mã hóa vị trí. Trong ví dụ này, chúng ta sẽ sử dụng các hàm sin và cos để tạo các vector mã hóa vị trí.\nGiả sử chiều nhúng là 3, các vector mã hóa vị trí sẽ được tính như sau:\nCho vị trí 0 (từ đầu tiên): - PE(0) = [sin(0/10000^0), cos(0/10000^0), sin(0/10000^0)]\nCho vị trí 1 (từ thứ hai): - PE(1) = [sin(1/10000^1), cos(1/10000^1), sin(1/10000^1)]\nCho vị trí 2 (từ thứ ba): - PE(2) = [sin(2/10000^0), cos(2/10000^0), sin(2/10000^0)]\nVì sin(0) = 0, sin(1) = 0.8415, sin(2) = 0.9093, cos(0) = 1, cos(1) = 0.5403, cos(2) = -0.4161.\nNên, các vector mã hóa vị trí là:\nPE(0) = [0, 1, 0] PE(1) = [0.8415, 0.5403, 0.8415] PE(2) = [0.9093, -0.4161, 0.9093]\nĐể tích hợp mã hóa vị trí, chúng ta cộng vector mã hóa vị trí tương ứng vào mỗi vector nhúng từ:\n\nTừ “Tôi” với mã hóa vị trí: [0.2, 0.3, -0.1] + [0, 1, 0] = [0.2, 1.3, -0.1]\nTừ “yêu” với mã hóa vị trí: [-0.5, 0.7, 0.2] + [0.8415, 0.5403, 0.8415] =\n\n[0.3415, 1.2403, 1.0415] - Từ “mèo” với mã hóa vị trí: [0.7, 0.8, 0.9] + [0.9093, -0.4161, 0.9093] = [1.6093, 0.3839, 1.8093]\nBằng cách cộng các vector mã hóa vị trí vào các vector nhúng từ, chúng ta giới thiệu thông tin về vị trí tương đối của các từ trong chuỗi đầu vào. Điều này cho phép mô hình Transformer nắm bắt thứ tự tuần tự và các phụ thuộc giữa các từ, điều quan trọng cho các nhiệm vụ như dịch máy hoặc phân tích cảm xúc.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#layer-normalization",
    "href": "machine-learning/transformer.html#layer-normalization",
    "title": "Transformer",
    "section": "",
    "text": "Layer Normalization là một kỹ thuật được sử dụng trong các mô hình học sâu để chuẩn hóa các kích hoạt (kết quả chuyển đổi đầu vào thành đầu ra bằng hàm kích hoạt - tạm gọi là các kích hoạt) của các đơn vị trong một lớp. Nó giải quyết vấn đề của sự thay đổi phân phối đầu vào của lớp trong quá trình huấn luyện, gây khó khăn cho quá trình học.\nCông thức cho Layer Normalization có thể được biểu diễn như sau:\noutput = scale * (input - mean) / sqrt(variance + epsilon) + bias\n\nỞ đây, đầu vào đại diện cho các kích hoạt đầu vào của một lớp, trung bình và phương sai đại diện cho giá trị trung bình và phương sai được tính theo chiều đầu vào, epsilon là một hằng số nhỏ được thêm vào để đảm bảo tính ổn định số học, và scale và bias là các tham số có thể học giúp mô hình điều chỉnh và dịch chuyển các kích hoạt đã được chuẩn hóa.\nLayer Normalization được áp dụng độc lập cho mỗi lô dữ liệu (batch), điều này làm cho nó phù hợp cho các nhiệm vụ với kích thước lô nhỏ. Nó đã được chứng minh là hiệu quả trong ổn định quá trình huấn luyện, tăng tốc quá trình hội tụ và cải thiện hiệu suất tổng quát của các mô hình học sâu.\nTổng quát, Layer Normalization giúp giảm sự phụ thuộc vào tỷ lệ của các kích hoạt đầu vào, thúc đẩy việc truyền gradient ổn định và cải thiện động học huấn luyện tổng thể (overall training dynamics - gồm learning rate, gradient, loss rate) của mạng neural. Nó đã được sử dụng rộng rãi trong các lĩnh vực khác nhau, bao gồm xử lý ngôn ngữ tự nhiên, thị giác máy tính và nhận dạng giọng nói.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#masking",
    "href": "machine-learning/transformer.html#masking",
    "title": "Transformer",
    "section": "",
    "text": "Giả sử chúng ta có một câu: “Tôi thích chơi bóng đá.”\nTrong mô hình ngôn ngữ, mục tiêu là dự đoán từ tiếp theo trong ngữ cảnh đã cho. Tuy nhiên, trong quá trình huấn luyện, chúng ta thường không muốn cho mô hình truy cập vào các từ trong tương lai vì nó có thể dẫn đến rò rỉ dữ liệu và dự đoán không thực tế. Vì vậy, chế che được áp dụng.\nĐể áp dụng cơ chế che, thường được ký hiệu là [MASK], để thay thế một số từ trong chuỗi đầu vào trong quá trình huấn luyện. Trong trường hợp này, chúng ta có thể ngẫu nhiên che một số từ trong câu, dẫn đến:\nCâu Đầu Vào (có cơ chế che): “Tôi thích chơi [MASK] bóng đá.”\nBây giờ, trong quá trình huấn luyện, mục tiêu của mô hình là dự đoán từ bị che dựa trên ngữ cảnh xung quanh. Nó học cách hiểu cấu trúc câu, ý nghĩa và sự phụ thuộc giữa các từ để đưa ra dự đoán chính xác. Mô hình nhận chuỗi đầu vào đã được chỉnh sửa (thêm [MASK]) và cố gắng dự đoán bản gốc, không bị che, ở vị trí đã được che.\nTrong quá trình huấn luyện hoặc đánh giá, cơ chế che không được sử dụng và mô hình được cung cấp với toàn bộ chuỗi đầu vào. Nó có thể tạo ra dự đoán cho từ tiếp theo trong chuỗi dựa trên kiến thức đã học từ quá trình huấn luyện.\nCơ chế che giúp mô hình tổng quát tốt và xử lý các trường hợp mà nó gặp phải các từ chưa được nhìn thấy hoặc thiếu. Nó khuyến khích mô hình dựa vào ngữ cảnh có sẵn để đưa ra dự đoán chính xác và cải thiện khả năng hiểu cấu trúc câu.",
    "crumbs": [
      "Machine learning",
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/ReinforcementLearningConcept.html",
    "href": "machine-learning/ReinforcementLearningConcept.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning (RL) là một lĩnh vực của trí tuệ nhân tạo mà trong đó một “agent” học cách tương tác với một “environment” để đạt được mục tiêu thông qua việc thực hiện “actions”. Agent học từ các phản hồi gọi là “rewards” từ environment và dần dần điều chỉnh hành vi để tối đa hóa tổng số rewards.",
    "crumbs": [
      "Machine learning",
      "Reinforcement Learning"
    ]
  },
  {
    "objectID": "machine-learning/ReinforcementLearningConcept.html#reinforcement-learning-là-gì",
    "href": "machine-learning/ReinforcementLearningConcept.html#reinforcement-learning-là-gì",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning (RL) là một lĩnh vực của trí tuệ nhân tạo mà trong đó một “agent” học cách tương tác với một “environment” để đạt được mục tiêu thông qua việc thực hiện “actions”. Agent học từ các phản hồi gọi là “rewards” từ environment và dần dần điều chỉnh hành vi để tối đa hóa tổng số rewards.",
    "crumbs": [
      "Machine learning",
      "Reinforcement Learning"
    ]
  },
  {
    "objectID": "machine-learning/ReinforcementLearningConcept.html#các-thuật-ngữ-thông-dụng",
    "href": "machine-learning/ReinforcementLearningConcept.html#các-thuật-ngữ-thông-dụng",
    "title": "Reinforcement Learning",
    "section": "Các thuật ngữ thông dụng",
    "text": "Các thuật ngữ thông dụng\nTrong reinforcement learning có rất nhiều các thuật ngữ khác nhau. Sau đây chúng ta cùng liệt kê các thuật ngữ thông dụng và tìm hiểu ý nghĩa của từng thuật ngữ\n\nAgent: Là thực thể thực hiện các hành động trong môi trường để đạt được mục tiêu.\nEnvironment: Là môi trường mà agent tương tác và học từ đó. Nó bao gồm tất cả các yếu tố ảnh hưởng đến agent và có thể thay đổi dựa trên hành động của agent.\nAction: Là các hành động mà agent thực hiện trong môi trường để thay đổi trạng thái hiện tại của nó.\nObservation: Là thông tin mà agent thu thập từ môi trường sau khi thực hiện một hành động.\nState: Là biểu diễn trạng thái hiện tại của môi trường. Nó chứa thông tin cần thiết để quyết định tương lai của agent.\nPolicy: Là chiến lược hoặc kế hoạch mà agent sử dụng để chọn hành động dựa trên trạng thái hiện tại.\nReward: Là phản hồi từ môi trường sau mỗi hành động. Reward định rõ giá trị của hành động và giúp agent học cách tối đa hóa tổng số reward theo thời gian.\nKhai thác và khám phá (exploit or explore): Là quá trình cân bằng giữa việc sử dụng kiến thức hiện có để đạt được reward ngay lập tức (khai thác) và việc thử nghiệm các hành động mới để tìm hiểu thêm về môi trường (khám phá).",
    "crumbs": [
      "Machine learning",
      "Reinforcement Learning"
    ]
  },
  {
    "objectID": "machine-learning/ReinforcementLearningConcept.html#so-sánh-reinforcement-learning-rl-unsupervised-learning-and-supervised-learning",
    "href": "machine-learning/ReinforcementLearningConcept.html#so-sánh-reinforcement-learning-rl-unsupervised-learning-and-supervised-learning",
    "title": "Reinforcement Learning",
    "section": "So sánh Reinforcement Learning (RL), Unsupervised Learning, and Supervised Learning",
    "text": "So sánh Reinforcement Learning (RL), Unsupervised Learning, and Supervised Learning\nHọc tăng cường (RL), Học không giám sát và Học có giám sát là ba mô hình cơ bản trong học máy, mỗi mô hình phục vụ các mục đích khác nhau và giải quyết các loại vấn đề riêng biệt. Dưới đây là so sánh ba cách tiếp cận này:\n\nHọc tăng cường (RL):\n\nMục tiêu: Học cách đưa ra quyết định để tối đa hóa phần thưởng tích lũy trong một môi trường.\nBản chất: Liên quan đến việc tác nhân tương tác với môi trường bằng cách thực hiện hành động và nhận phần thưởng dựa trên hành động của tác nhân đó.\nDữ liệu đào tạo: RL học hỏi từ các tương tác và phản hồi thay vì dữ liệu được gắn nhãn.\nVí dụ: Chơi trò chơi, điều khiển robot, hệ thống đề xuất.\nThách thức: Đánh đổi giữa thăm dò và khai thác, phần thưởng bị trì hoãn, cân bằng giữa thăm dò và khai thác.\n\n\n\nHọc không giám sát:\n\nMục tiêu: Tìm các mẫu hoặc cấu trúc trong dữ liệu chưa được gắn nhãn.\nBản chất: Không có mục tiêu hoặc kết quả cụ thể để dự đoán. Mục tiêu là khám phá các mối quan hệ hoặc cụm ẩn trong dữ liệu.\nDữ liệu huấn luyện: Học từ dữ liệu đầu vào mà không có nhãn đầu ra rõ ràng.\nVí dụ: Phân cụm, giảm kích thước, tạo mô hình tổng quát (ví dụ: GAN).\nThách thức: Xác định số lượng cụm thích hợp, xử lý dữ liệu nhiều chiều.\n\n\n\nHọc tập có giám sát:\n\nMục tiêu: Học cách ánh xạ từ đầu vào đến đầu ra dựa trên dữ liệu huấn luyện được gắn nhãn.\nBản chất: Yêu cầu tập dữ liệu được gắn nhãn trong đó mô hình học cách dự đoán kết quả đầu ra chính xác từ các đầu vào nhất định.\nDữ liệu huấn luyện: Sử dụng các cặp đầu vào-đầu ra để huấn luyện.\nVí dụ: Phân loại, hồi quy.\nThách thức: Quá phù hợp, sai lệch, khái quát hóa đối với dữ liệu không nhìn thấy được.\n\nSự khác biệt chính: - RL tập trung vào việc học các hành động tối ưu để tối đa hóa phần thưởng, trong khi việc học có giám sát và không giám sát tập trung vào các mô hình học tập hoặc các mối quan hệ trong dữ liệu. - Học không giám sát thiếu dữ liệu được dán nhãn, trong khi học có giám sát yêu cầu dữ liệu được dán nhãn. - Học có giám sát phù hợp với các nhiệm vụ đã biết đầu ra mong muốn, trong khi RL phù hợp với các nhiệm vụ liên quan đến việc ra quyết định tuần tự. - RL thường liên quan đến việc khám phá và đánh đổi giữa phần thưởng trước mắt và lâu dài, trong khi học tập có giám sát và không giám sát là tìm kiếm các mô hình hoặc đưa ra dự đoán.",
    "crumbs": [
      "Machine learning",
      "Reinforcement Learning"
    ]
  },
  {
    "objectID": "machine-learning/ReinforcementLearningConcept.html#tham-khảo",
    "href": "machine-learning/ReinforcementLearningConcept.html#tham-khảo",
    "title": "Reinforcement Learning",
    "section": "Tham khảo",
    "text": "Tham khảo\n\ncodelearn\ntechvidvan\nmathworks",
    "crumbs": [
      "Machine learning",
      "Reinforcement Learning"
    ]
  },
  {
    "objectID": "machine-learning/CohortFitDistribution.html",
    "href": "machine-learning/CohortFitDistribution.html",
    "title": "Fit Distribution",
    "section": "",
    "text": "Phân phối bao gồm: Cauchy, Gamma, Exponential, Log-Logistic, Log-Normal Hàm mât đô xác suất (PDF) của phân phối Cauchy:\n\\[\nf\\left(x ; x_{0} ; \\gamma\\right)=\\frac{1}{\\pi \\gamma\\left[1+\\left(\\frac{x-x_{0}}{\\gamma}\\right)^{2}\\right]}=\\frac{1}{\\pi \\gamma}\\left[\\frac{\\gamma^{2}}{\\left(x-x_{0}\\right)^{2}+\\gamma^{2}}\\right]\n\\]\nTrong đó:\n\n\\(x_{0}\\) : là thông số vị trí, chỉ định vị trí đỉnh của phân phối\n\\(\\gamma\\) : là thông số tỷ lệ chỉ định nửa chiều rộng ở nửa tối đa (HWHM), \\(2 \\gamma\\) là toàn bộ chiều rộng ở mức tối đa một nửa (FWHM). \\(\\gamma\\) là một nửa phạm vi liên phần và đôi khi được gọi là lỗi có thể xảy ra\n\nHàm mât đô xác suất (PDF) của phân phối Gamma:\n\\[\nf(x ; k ; \\theta)=\\frac{x^{k-1} e^{-\\frac{x}{\\theta}}}{\\theta^{k} \\Gamma(k)}, \\text { for } x&gt;0 \\text { and } k, \\theta&gt;0\n\\]\n\n\\(k\\) : là tham số hình dạng\n\\(\\theta\\) : là tham số tỷ lệ\n\\(x\\) : là biến ngẫu nhiên\n\\(\\Gamma(k)\\) : là hàm gamma được đánh giá tại \\(\\mathrm{k}\\)\n\nHàm mật đô xác suất (PDF) của phân phối Exponential:\n\\[\nf(x ; \\lambda)=\\left\\{\\begin{array}{rr}\n0, & x&lt;0 \\\\\n\\lambda e^{-\\lambda x}, & x \\geq 0\n\\end{array}\\right.\n\\]\n\n\\(\\lambda\\) : là tham số của phân phối, thường được gọi là tham số tỷ lệ.\n\\(x\\) : là biến ngẫu nhiên\n\nHàm mật độ xác suất (PDF) của phân phối Log-Logistic:\n\\[\nf(x ; \\alpha ; \\beta)=\\frac{\\left(\\frac{\\beta}{\\alpha}\\right)\\left(\\frac{x}{\\alpha}\\right)^{\\beta-1}}{\\left(1+\\left(\\frac{x}{\\alpha}\\right)^{\\beta}\\right)^{2}}, \\text { where } x&gt;0, \\alpha&gt;0, \\beta&gt;0\n\\]\n\n\\(\\alpha\\) : là tham số tỷ lệ và là giá trị trung bình của phân phối\n\\(\\beta\\) : là tham số hình dạng\n\nHàm mât độ xác suất (PDF) của phân phối Lognormal \\(\\left(\\mu, \\sigma^{2}\\right)\\) :\n\\[\nf_{x}(x)=\\frac{d}{d x} \\operatorname{Pr}(X \\leq x)=\\frac{1}{x \\sigma \\sqrt{2 \\pi}} e^{\\left(-\\frac{(\\ln x-\\mu)^{2}}{2 \\sigma^{2}}\\right)}, \\text { where } x&gt;0, \\sigma&gt;0\n\\]\n\n\\(x\\) là biến ngẫu nhiên\nMột biến ngẫu nhiên \\(X\\) tuân theo phân phối Log-normal \\(\\left(X \\sim \\operatorname{Lognormal}\\left(\\mu_{x}, \\sigma_{x}^{2}\\right)\\right)\\) nếu \\(\\ln (X)\\) tuân theo phân phối chuẩn với giá trị trung bình là \\(\\mu\\) và phương sai \\(\\sigma^{2}\\)\n\nĐể lựa chọn phân phối phù hợp cho từng phân nhóm, Nhóm mô hình lựa chọn mô hình có SSE (Sum of square error - tổng của sai số bình phương) nhỏ nhất với SSE được tính theo công thức sau:\n\\[\n\\left.S S E=\\sum \\text { (Giá trị thực tế - Giá trị được tính ra từ mô hình phân phối }\\right)^{2}\n\\]\nCuối cùng, từ mô hình phân phối vừa được lựa chọn (là mô hình có SSE nhỏ nhất).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import cauchy, gamma, expon, lognorm, loglaplace\n\n# Define the x range for the plot\nx = np.linspace(0, 15, 1000)\n\n# Create a single row with 5 subplots\nfig, axs = plt.subplots(1, 5, figsize=(10, 3))\n\n# Cauchy Distribution\ncauchy_pdf = cauchy.pdf(x)\naxs[0].plot(x, cauchy_pdf)\naxs[0].set_title(\"Cauchy\")\n\n# Gamma Distribution\ngamma_pdf = gamma.pdf(x, a=2)\naxs[1].plot(x, gamma_pdf)\naxs[1].set_title(\"Gamma\")\n\n# Exponential Distribution\nexponential_pdf = expon.pdf(x)\naxs[2].plot(x, exponential_pdf)\naxs[2].set_title(\"Exponential\")\n\n# Log Normal Distribution\nlog_normal_pdf = lognorm.pdf(x, s=0.7)\naxs[3].plot(x, log_normal_pdf)\naxs[3].set_title(\"Log Normal\")\n\n# Log Logistic Distribution\nlog_logistic_pdf = loglaplace.pdf(x, c=0.5)\naxs[4].plot(x, log_logistic_pdf)\naxs[4].set_title(\"Log Logistic\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Fit Distribution"
    ]
  },
  {
    "objectID": "machine-learning/CohortFitDistribution.html#dạng-hàm-phân-phối",
    "href": "machine-learning/CohortFitDistribution.html#dạng-hàm-phân-phối",
    "title": "Fit Distribution",
    "section": "",
    "text": "Phân phối bao gồm: Cauchy, Gamma, Exponential, Log-Logistic, Log-Normal Hàm mât đô xác suất (PDF) của phân phối Cauchy:\n\\[\nf\\left(x ; x_{0} ; \\gamma\\right)=\\frac{1}{\\pi \\gamma\\left[1+\\left(\\frac{x-x_{0}}{\\gamma}\\right)^{2}\\right]}=\\frac{1}{\\pi \\gamma}\\left[\\frac{\\gamma^{2}}{\\left(x-x_{0}\\right)^{2}+\\gamma^{2}}\\right]\n\\]\nTrong đó:\n\n\\(x_{0}\\) : là thông số vị trí, chỉ định vị trí đỉnh của phân phối\n\\(\\gamma\\) : là thông số tỷ lệ chỉ định nửa chiều rộng ở nửa tối đa (HWHM), \\(2 \\gamma\\) là toàn bộ chiều rộng ở mức tối đa một nửa (FWHM). \\(\\gamma\\) là một nửa phạm vi liên phần và đôi khi được gọi là lỗi có thể xảy ra\n\nHàm mât đô xác suất (PDF) của phân phối Gamma:\n\\[\nf(x ; k ; \\theta)=\\frac{x^{k-1} e^{-\\frac{x}{\\theta}}}{\\theta^{k} \\Gamma(k)}, \\text { for } x&gt;0 \\text { and } k, \\theta&gt;0\n\\]\n\n\\(k\\) : là tham số hình dạng\n\\(\\theta\\) : là tham số tỷ lệ\n\\(x\\) : là biến ngẫu nhiên\n\\(\\Gamma(k)\\) : là hàm gamma được đánh giá tại \\(\\mathrm{k}\\)\n\nHàm mật đô xác suất (PDF) của phân phối Exponential:\n\\[\nf(x ; \\lambda)=\\left\\{\\begin{array}{rr}\n0, & x&lt;0 \\\\\n\\lambda e^{-\\lambda x}, & x \\geq 0\n\\end{array}\\right.\n\\]\n\n\\(\\lambda\\) : là tham số của phân phối, thường được gọi là tham số tỷ lệ.\n\\(x\\) : là biến ngẫu nhiên\n\nHàm mật độ xác suất (PDF) của phân phối Log-Logistic:\n\\[\nf(x ; \\alpha ; \\beta)=\\frac{\\left(\\frac{\\beta}{\\alpha}\\right)\\left(\\frac{x}{\\alpha}\\right)^{\\beta-1}}{\\left(1+\\left(\\frac{x}{\\alpha}\\right)^{\\beta}\\right)^{2}}, \\text { where } x&gt;0, \\alpha&gt;0, \\beta&gt;0\n\\]\n\n\\(\\alpha\\) : là tham số tỷ lệ và là giá trị trung bình của phân phối\n\\(\\beta\\) : là tham số hình dạng\n\nHàm mât độ xác suất (PDF) của phân phối Lognormal \\(\\left(\\mu, \\sigma^{2}\\right)\\) :\n\\[\nf_{x}(x)=\\frac{d}{d x} \\operatorname{Pr}(X \\leq x)=\\frac{1}{x \\sigma \\sqrt{2 \\pi}} e^{\\left(-\\frac{(\\ln x-\\mu)^{2}}{2 \\sigma^{2}}\\right)}, \\text { where } x&gt;0, \\sigma&gt;0\n\\]\n\n\\(x\\) là biến ngẫu nhiên\nMột biến ngẫu nhiên \\(X\\) tuân theo phân phối Log-normal \\(\\left(X \\sim \\operatorname{Lognormal}\\left(\\mu_{x}, \\sigma_{x}^{2}\\right)\\right)\\) nếu \\(\\ln (X)\\) tuân theo phân phối chuẩn với giá trị trung bình là \\(\\mu\\) và phương sai \\(\\sigma^{2}\\)\n\nĐể lựa chọn phân phối phù hợp cho từng phân nhóm, Nhóm mô hình lựa chọn mô hình có SSE (Sum of square error - tổng của sai số bình phương) nhỏ nhất với SSE được tính theo công thức sau:\n\\[\n\\left.S S E=\\sum \\text { (Giá trị thực tế - Giá trị được tính ra từ mô hình phân phối }\\right)^{2}\n\\]\nCuối cùng, từ mô hình phân phối vừa được lựa chọn (là mô hình có SSE nhỏ nhất).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import cauchy, gamma, expon, lognorm, loglaplace\n\n# Define the x range for the plot\nx = np.linspace(0, 15, 1000)\n\n# Create a single row with 5 subplots\nfig, axs = plt.subplots(1, 5, figsize=(10, 3))\n\n# Cauchy Distribution\ncauchy_pdf = cauchy.pdf(x)\naxs[0].plot(x, cauchy_pdf)\naxs[0].set_title(\"Cauchy\")\n\n# Gamma Distribution\ngamma_pdf = gamma.pdf(x, a=2)\naxs[1].plot(x, gamma_pdf)\naxs[1].set_title(\"Gamma\")\n\n# Exponential Distribution\nexponential_pdf = expon.pdf(x)\naxs[2].plot(x, exponential_pdf)\naxs[2].set_title(\"Exponential\")\n\n# Log Normal Distribution\nlog_normal_pdf = lognorm.pdf(x, s=0.7)\naxs[3].plot(x, log_normal_pdf)\naxs[3].set_title(\"Log Normal\")\n\n# Log Logistic Distribution\nlog_logistic_pdf = loglaplace.pdf(x, c=0.5)\naxs[4].plot(x, log_logistic_pdf)\naxs[4].set_title(\"Log Logistic\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Fit Distribution"
    ]
  },
  {
    "objectID": "machine-learning/CohortFitDistribution.html#python-example",
    "href": "machine-learning/CohortFitDistribution.html#python-example",
    "title": "Fit Distribution",
    "section": "Python Example",
    "text": "Python Example\n\nimport numpy as np\nimport pandas as pd\nimport scipy  \nfrom scipy import stats \nimport scipy.optimize as optimize\n\n\n1. Các hàm liên quan\n\n1.1 Hàm phân phối gamma\n\nclass opt_gamma:\n\n    def __init__(self, actual_pd):\n        self.actual_pd = actual_pd\n        self.x_input = range(1, len(actual_pd) + 1)\n\n    def func(self, x, scale, a, b):        \n        predict = stats.gamma.pdf(x, a = a, scale = b) * scale\n        return predict\n\n    def sse(self, params, xobs, yobs):\n        ynew = self.func(xobs, *params)\n        mse = np.sum((ynew - yobs) ** 2)        \n        return mse\n    \n    def solver(self):\n        p0 = [1,1,1]\n        bounds = [(0.0001, 2), (0.0001, 10), (0.0001, 10)]\n        res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), bounds= bounds)       \n        # res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), method='Nelder-Mead')        \n        return res\n    \n    def predict(self, t=30):\n        res = self.solver()\n        ypred = self.func(range(1, t+1), *res.x)        \n        return ypred        \n\n\n\n1.2 Hàm Phân phối mũ Exponential\n\nclass opt_exponential:\n\n    def __init__(self, actual_pd):\n        self.actual_pd = actual_pd\n        self.x_input = range(1, len(actual_pd) + 1)\n\n    def func(self, x, scale, a):        \n        # change function here        \n        pred = stats.expon.pdf(x, scale = 1/a) * scale\n        return pred\n\n    def sse(self, params, xobs, yobs):\n        ynew = self.func(xobs, *params)\n        mse = np.sum((ynew - yobs) ** 2)        \n        return mse\n    \n    def solver(self):\n        # change initial guess here\n        p0 = [1,2]\n        bounds = [(0.0001, 2), (0.0001, 0.5)]           \n        res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), bounds = bounds)             \n        return res\n    \n    def predict(self, t=30):\n        res = self.solver()\n        ypred = self.func(range(1, t+1), *res.x)        \n        return ypred       \n\n\n\n1.3 Hàm phân phối Cauchy\n\nclass opt_cauchy:\n\n    def __init__(self, actual_pd):\n        self.actual_pd = actual_pd\n        self.x_input = range(1, len(actual_pd) + 1)\n\n    def func(self, x, scale, a, b):        \n        # change function here        \n        predict = stats.cauchy.pdf(x, loc = b, scale = a) * scale\n        return predict\n\n    def sse(self, params, xobs, yobs):\n        ynew = self.func(xobs, *params)\n        mse = np.sum((ynew - yobs) ** 2)        \n        return mse\n    \n    def solver(self):\n        # change initial guess here\n        p0 = [1,1,1]\n        bounds = [(0.0001, 2), (0.0001, 15), (-30, 30)]\n        res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), bounds = bounds)        \n        return res\n    \n    def predict(self, t=30):\n        res = self.solver()\n        ypred = self.func(range(1, t+1), *res.x)        \n        return ypred    \n\n\n\n1.4 Hàm phân phối log logistic\n\nclass opt_log_logistic:\n\n    def __init__(self, actual_pd):\n        self.actual_pd = actual_pd\n        self.x_input = range(1, len(actual_pd) + 1)\n\n    def func(self, x, scale, a, b):        \n        # change function here\n        predict = scale*(a/b)*(x/b)**(a-1)/(1+(x/b)**a)**2\n        return predict\n\n    def sse(self, params, xobs, yobs):\n        ynew = self.func(xobs, *params)\n        mse = np.sum((ynew - yobs) ** 2)        \n        return mse\n    \n    def solver(self):\n        # change initial guess here\n        p0 = [2,3,1]\n        bounds = [(0.0001, 2), (0.0001, 10), (0.0001, 10)]\n        res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), bounds=bounds)        \n        return res\n    \n    def predict(self, t=30):\n        res = self.solver()\n        ypred = self.func(range(1, t+1), *res.x)        \n        return ypred    \n\n\n\n1.5 Hàm phân phối log normal\n\nclass opt_log_normal:\n\n    def __init__(self, actual_pd):\n        self.actual_pd = actual_pd\n        self.x_input = range(1, len(actual_pd) + 1)\n\n    def func(self, x, scale, a, b):        \n        # change function here\n        predict = (np.exp(-(np.log(x) - a)**2 / (2 * b**2)) / (x * b * np.sqrt(2 * np.pi)))*scale\n        return predict\n\n    def sse(self, params, xobs, yobs):\n        ynew = self.func(xobs, *params)\n        mse = np.sum((ynew - yobs) ** 2)        \n        return mse\n    \n    def solver(self):\n        # change initial guess here\n        p0 = [1,1,1]\n        bounds = [(0.0001, 2), (0.0001, 0.5), (0.0001, 0.5)]\n        res = scipy.optimize.minimize(self.sse, p0, args=(self.x_input, self.actual_pd), bounds=bounds)        \n        return res\n    \n    def predict(self, t=30):\n        res = self.solver()\n        ypred = self.func(range(1, t+1), *res.x)        \n        return ypred",
    "crumbs": [
      "Machine learning",
      "Fit Distribution"
    ]
  },
  {
    "objectID": "machine-learning/CohortFitDistribution.html#fit-models",
    "href": "machine-learning/CohortFitDistribution.html#fit-models",
    "title": "Fit Distribution",
    "section": "2. Fit models",
    "text": "2. Fit models\n\n2.1 Đặt giá trị initial\n\nGamma = 1,1,1\nCauchy = 1,1,1\nExpo = 1,2\nLog Logistic = 2,3,1\nLog Normal = 1,1,1\n\n\n\n2.2 Đọc dữ liệu\n\ndata = pd.read_excel('results/Cohort Analysis.xlsb', engine='pyxlsb', sheet_name='1.2 Visualize', usecols = 'O:Z')\ndata.tail()",
    "crumbs": [
      "Machine learning",
      "Fit Distribution"
    ]
  },
  {
    "objectID": "machine-learning/CohortFitDistribution.html#fit-models-1",
    "href": "machine-learning/CohortFitDistribution.html#fit-models-1",
    "title": "Fit Distribution",
    "section": "2.3 Fit models",
    "text": "2.3 Fit models\n\ndef fn_export_by_segment(segment, period):\n    # segment = segment.upper()\n    actual_pd = data[data.Segment_Group.isin([segment])]\n    actual_pd = actual_pd.iloc[0, 2:]\n    actual_pd = actual_pd.values\n    \n    res_gamma = opt_gamma(actual_pd)\n    res_exponential = opt_exponential(actual_pd)\n    res_cauchy = opt_cauchy(actual_pd)\n    res_log_logistic = opt_log_logistic(actual_pd)\n    res_log_normal = opt_log_normal(actual_pd)\n    \n    params = pd.DataFrame({\n        'Segment': segment,\n        'Distribution': ['Gamma', 'Exponential', 'Cauchy', 'Log-Logistic', 'Log-Normal'],\n        'SSE': [res_gamma.solver().fun, res_exponential.solver().fun, res_cauchy.solver().fun, res_log_logistic.solver().fun, res_log_normal.solver().fun],\n        'params': [res_gamma.solver().x, res_exponential.solver().x, res_cauchy.solver().x, res_log_logistic.solver().x, res_log_normal.solver().x]\n    })    \n   \n    params[['scale', 'a', 'b']] = pd.DataFrame(params.params.to_list())    \n    del params['params']\n    \n    dict_predict = {\n            'Segment': segment,\n            'Period': range(1, period+1),\n            'Gamma': res_gamma.predict(period),\n            'Exponential': res_exponential.predict(period),\n            'Cauchy': res_cauchy.predict(period),\n            'Log-Logistic': res_log_logistic.predict(period),\n            'Log-Normal': res_log_normal.predict(period)\n        }\n    \n    best_distribution = params.sort_values('SSE').head(1)\n    best_distribution['Tag best distribution'] = 'Best distribution'    \n\n    print('Best distribution of ' + segment + ' is '+ best_distribution['Distribution'].item())\n\n    return params, best_distribution, pd.DataFrame(dict_predict)\n\n\npred_best_fit = []\nparams_frame = []\npred_all_curve = []\nfor seg in data.Segment_Group:\n    params, df_best_fit, df_all = fn_export_by_segment(seg, 30)\n    pred_best_fit.append(df_best_fit)\n    params_frame.append(params)\n    pred_all_curve.append(df_all)\n\npred_best_fit = pd.concat(pred_best_fit, axis=0)\nparams_frame = pd.concat(params_frame, axis=0)\npred_all_curve = pd.concat(pred_all_curve, axis=0)\n\n\npivot_pred_all_curve = pd.melt(pred_all_curve, \n        id_vars=['Segment', 'Period'], \n        value_vars=['Gamma', 'Exponential', 'Cauchy', 'Log-Logistic', 'Log-Normal'],\n        var_name = 'Distribution').pivot(\n                index = ['Segment', 'Distribution'], columns='Period'\n        )\npivot_pred_all_curve.reset_index(inplace=True, drop=False)\npivot_pred_all_curve.columns = ['Segment', 'Distribution', *range(1,31)]\npivot_pred_all_curve = params_frame.merge(pivot_pred_all_curve, how='inner', on=['Segment', 'Distribution']).merge(\n        pred_best_fit[['Segment', 'Distribution', 'Tag best distribution']], how='left', on=['Segment', 'Distribution']\n)",
    "crumbs": [
      "Machine learning",
      "Fit Distribution"
    ]
  },
  {
    "objectID": "machine-learning/focal-loss.html",
    "href": "machine-learning/focal-loss.html",
    "title": "Focal loss - handle class imbalance",
    "section": "",
    "text": "Hàm mất mát Focal là một sự cải tiến so với hàm mất mát cross-entropy tiêu chuẩn cho phân loại nhị phân và đa lớp. Nó được giới thiệu trong bài báo có tiêu đề “Focal Loss for Dense Object Detection” của Tsung-Yi Lin và cộng sự, và chủ yếu được thiết kế cho các nhiệm vụ phát hiện đối tượng để giải quyết bài toán mất cân bằng giữa các lớp (class imbalance).\nÝ chính đằng sau hàm mất mát Focal là giảm trọng số đóng góp của các ví dụ dễ dàng và tập trung vào những ví dụ khó. Điều này giúp ngăn chặn số lượng lớn các ví dụ tiêu cực dễ dàng từ việc áp đặt lên bộ phát hiện trong quá trình đào tạo.\nCông thức cho hàm mất mát Focal cho phân loại nhị phân là:\n\\[ \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t) \\]\nTrong đó:\n\n\\(p_t\\) là xác suất của lớp đúng (true class). Nếu nhãn lớp đúng là 1, thì \\(p_t\\) là xác suất dự đoán của mô hình cho lớp 1; nếu nhãn lớp đúng là 0, thì $p_t = 1 - $ xác suất dự đoán của mô hình cho lớp 1.\n\\(\\alpha_t\\) là một yếu tố cân bằng. Thông thường đặt nằm giữa 0 và 1. Số này được sử dụng để xử lý mất cân bằng lớp.\n\\(\\gamma\\) là tham số tập trung (focusing parameter) mục đích điều chỉnh mức độ tập trung vào lớp dễ dàng phân loại. Khi \\(\\gamma = 0\\), hàm mất mát Focal tương đương với hàm mất mát cross-entropy. Khi \\(\\gamma\\) tăng, hiệu ứng của yếu tố điều chỉnh trở nên rõ ràng hơn.\n\nLợi thế chính của hàm mất mát Focal là nó đưa ra nhiều trọng số hơn cho các ví dụ bị phân loại sai và ít trọng số hơn cho các ví dụ được phân loại tốt. Điều này giúp trong các tình huống mà một số lớp bị đại diện ít hơn hoặc khi mô hình có khả năng bị áp đặt bởi các ví dụ tiêu cực dễ dàng.\n\n\n\nMục đích: \\(\\alpha\\) được sử dụng trong hàm mất mát Focal để xử lý sự mất cân bằng lớp bằng cách điều chỉnh mất mát cho các lớp dương và âm một cách khác nhau. Nó cung cấp một sự cân bằng giữa tầm quan trọng của lớp dương và lớp âm trong việc tính toán mất mát.\nẢnh hưởng lên Giá trị Mất mát:\n\nĐối với lớp dương (tức là khi nhãn thực \\(y = 1\\)): Giá trị mất mát được nhân với hệ số \\(\\alpha\\).\nĐối với lớp âm (tức là khi nhãn thực \\(y = 0\\)): Giá trị mất mát được nhân với hệ số \\(1 - \\alpha\\).\n\nPhạm vi Giá trị:\n\nThông thường, \\(\\alpha\\) nằm trong khoảng [0, 1].\nGiá trị \\(\\alpha\\) càng gần 1 càng làm cho mất mát lớp dương được tăng cường và mất mát cho lớp âm được giảm đi.\nNgược lại, một giá trị \\(\\alpha\\) gần 0 sẽ nhấn mạnh hơn đến lớp âm.\n\nLợi ích:\n\nBằng cách điều chỉnh \\(\\alpha\\), người ta có thể cung cấp trọng số nhiều hơn cho các lớp được đại diện ít hơn. Điều này có thể đặc biệt hữu ích trong các tình huống có sự mất cân bằng lớp nghiêm trọng, như trong các nhiệm vụ phát hiện đối tượng khi số lượng các ví dụ âm vượt trội so với các ví dụ dương.\nNó đảm bảo rằng mô hình không thiên vị về lớp có nhiều quan sát hơn và xem xét cả hai lớp khi cập nhật trọng số trong quá trình huấn luyện.\n\nThiết lập \\(\\alpha\\):\n\nTrong một số trường hợp, \\(\\alpha\\) có thể được thiết lập dựa trên phân phối lớp nghịch đảo. Ví dụ, nếu 80% các ví dụ là âm và 20% là dương, người ta có thể đặt \\(\\alpha\\) thành 0,2 cho lớp âm và 0,8 cho lớp dương.\nTrong những trường hợp khác, giá trị của \\(\\alpha\\) có thể được xác định thông qua kiểm định chéo (cross-validation) hoặc các phương pháp điều chỉnh tham số khác.\n\n\nTóm lại, tham số \\(\\alpha\\) trong hàm mất mát Focal cung cấp một cơ chế để xử lý sự mất cân bằng lớp bằng cách điều chỉnh mất mát cho các ví dụ dương và âm một cách khác nhau. Nó đảm bảo rằng cả hai lớp chính và phụ đều được đại diện đầy đủ trong quá trình đào tạo của mô hình.\n\n\n\n\\(\\gamma\\) được gọi là “tham số tập trung”. Nó đóng một vai trò quan trọng trong việc xác định mức độ mà mô hình nên tập trung vào các lớp bị phân loại sai so với những lớp được phân loại đúng.\nTác động của nó:\n\nMục đích: Mục đích chính của tham số \\(\\gamma\\) trong hàm mất mát Focal là giảm ảnh hưởng của các lớp dễ phân loại và tăng tầm quan trọng của việc hiệu chỉnh các lớp bị phân loại sai. Điều này đặc biệt hữu ích trong các tình huống mà tập dữ liệu có sự mất cân bằng giữa các lớp.\nTác động của việc Thay đổi \\(\\gamma\\): Thuật ngữ \\((1 - p_t)^\\gamma\\) trong hàm mất mát Focal là yếu tố điều chỉnh. Ở đây, \\(p_t\\) đại diện cho xác suất dự đoán của lớp đúng.\n\nNếu \\(p_t\\) gần bằng 1, nghĩa là ví dụ dễ dàng được phân loại, và \\((1 - p_t)^\\gamma\\) sẽ gần bằng 0, đặc biệt khi \\(\\gamma &gt; 0\\).\nNếu \\(p_t\\) xa 1 (tức là dự đoán là không chính xác hoặc không chắc chắn), thì \\((1 - p_t)^\\gamma\\) sẽ lớn hơn, tăng ảnh hưởng của ví dụ đó lên hàm mất mát.\n\\(\\gamma = 0\\): Hàm mất mát Focal giảm xuống còn bằng hàm mất mát cross-entropy tiêu chuẩn, vì yếu tố điều chỉnh trở thành 1 cho tất cả các ví dụ.\n\\(\\gamma &gt; 0\\): Tăng trọng số của các lớp khó phân loại và giảm trọng số của những lớp dễ phân loại. \\(\\gamma\\) càng lớn, mô hình càng tập trung nhiều vào các lớp khó.\n\nLợi ích: Bằng cách điều chỉnh \\(\\gamma\\), hàm mất mát Focal cho phép các mô hình, đặc biệt trong các nhiệm vụ phát hiện đối tượng, trở nên mạnh mẽ hơn trước số lượng lớn các lớp dễ. Thay vì tiêu tốn tài nguyên tính toán cho các lớp dễ dàng, mô hình tập trung nhiều hơn vào các lớp khó, thường chứa nhiều thông tin hơn.\n\nTóm lại, tham số \\(\\gamma\\) trong hàm mất mát Focal cung cấp một cơ chế để nhấn mạnh việc học từ các lớp bị phân loại sai so với những lớp dễ phân loại. Đó là một công cụ để xử lý sự mất cân bằng lớp và đảm bảo rằng mô hình chú ý nhiều hơn đến các lớp mà nó phân loại sai.",
    "crumbs": [
      "Machine learning",
      "Focal loss - handle class imbalance"
    ]
  },
  {
    "objectID": "machine-learning/focal-loss.html#focal-loss",
    "href": "machine-learning/focal-loss.html#focal-loss",
    "title": "Focal loss - handle class imbalance",
    "section": "",
    "text": "Hàm mất mát Focal là một sự cải tiến so với hàm mất mát cross-entropy tiêu chuẩn cho phân loại nhị phân và đa lớp. Nó được giới thiệu trong bài báo có tiêu đề “Focal Loss for Dense Object Detection” của Tsung-Yi Lin và cộng sự, và chủ yếu được thiết kế cho các nhiệm vụ phát hiện đối tượng để giải quyết bài toán mất cân bằng giữa các lớp (class imbalance).\nÝ chính đằng sau hàm mất mát Focal là giảm trọng số đóng góp của các ví dụ dễ dàng và tập trung vào những ví dụ khó. Điều này giúp ngăn chặn số lượng lớn các ví dụ tiêu cực dễ dàng từ việc áp đặt lên bộ phát hiện trong quá trình đào tạo.\nCông thức cho hàm mất mát Focal cho phân loại nhị phân là:\n\\[ \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t) \\]\nTrong đó:\n\n\\(p_t\\) là xác suất của lớp đúng (true class). Nếu nhãn lớp đúng là 1, thì \\(p_t\\) là xác suất dự đoán của mô hình cho lớp 1; nếu nhãn lớp đúng là 0, thì $p_t = 1 - $ xác suất dự đoán của mô hình cho lớp 1.\n\\(\\alpha_t\\) là một yếu tố cân bằng. Thông thường đặt nằm giữa 0 và 1. Số này được sử dụng để xử lý mất cân bằng lớp.\n\\(\\gamma\\) là tham số tập trung (focusing parameter) mục đích điều chỉnh mức độ tập trung vào lớp dễ dàng phân loại. Khi \\(\\gamma = 0\\), hàm mất mát Focal tương đương với hàm mất mát cross-entropy. Khi \\(\\gamma\\) tăng, hiệu ứng của yếu tố điều chỉnh trở nên rõ ràng hơn.\n\nLợi thế chính của hàm mất mát Focal là nó đưa ra nhiều trọng số hơn cho các ví dụ bị phân loại sai và ít trọng số hơn cho các ví dụ được phân loại tốt. Điều này giúp trong các tình huống mà một số lớp bị đại diện ít hơn hoặc khi mô hình có khả năng bị áp đặt bởi các ví dụ tiêu cực dễ dàng.\n\n\n\nMục đích: \\(\\alpha\\) được sử dụng trong hàm mất mát Focal để xử lý sự mất cân bằng lớp bằng cách điều chỉnh mất mát cho các lớp dương và âm một cách khác nhau. Nó cung cấp một sự cân bằng giữa tầm quan trọng của lớp dương và lớp âm trong việc tính toán mất mát.\nẢnh hưởng lên Giá trị Mất mát:\n\nĐối với lớp dương (tức là khi nhãn thực \\(y = 1\\)): Giá trị mất mát được nhân với hệ số \\(\\alpha\\).\nĐối với lớp âm (tức là khi nhãn thực \\(y = 0\\)): Giá trị mất mát được nhân với hệ số \\(1 - \\alpha\\).\n\nPhạm vi Giá trị:\n\nThông thường, \\(\\alpha\\) nằm trong khoảng [0, 1].\nGiá trị \\(\\alpha\\) càng gần 1 càng làm cho mất mát lớp dương được tăng cường và mất mát cho lớp âm được giảm đi.\nNgược lại, một giá trị \\(\\alpha\\) gần 0 sẽ nhấn mạnh hơn đến lớp âm.\n\nLợi ích:\n\nBằng cách điều chỉnh \\(\\alpha\\), người ta có thể cung cấp trọng số nhiều hơn cho các lớp được đại diện ít hơn. Điều này có thể đặc biệt hữu ích trong các tình huống có sự mất cân bằng lớp nghiêm trọng, như trong các nhiệm vụ phát hiện đối tượng khi số lượng các ví dụ âm vượt trội so với các ví dụ dương.\nNó đảm bảo rằng mô hình không thiên vị về lớp có nhiều quan sát hơn và xem xét cả hai lớp khi cập nhật trọng số trong quá trình huấn luyện.\n\nThiết lập \\(\\alpha\\):\n\nTrong một số trường hợp, \\(\\alpha\\) có thể được thiết lập dựa trên phân phối lớp nghịch đảo. Ví dụ, nếu 80% các ví dụ là âm và 20% là dương, người ta có thể đặt \\(\\alpha\\) thành 0,2 cho lớp âm và 0,8 cho lớp dương.\nTrong những trường hợp khác, giá trị của \\(\\alpha\\) có thể được xác định thông qua kiểm định chéo (cross-validation) hoặc các phương pháp điều chỉnh tham số khác.\n\n\nTóm lại, tham số \\(\\alpha\\) trong hàm mất mát Focal cung cấp một cơ chế để xử lý sự mất cân bằng lớp bằng cách điều chỉnh mất mát cho các ví dụ dương và âm một cách khác nhau. Nó đảm bảo rằng cả hai lớp chính và phụ đều được đại diện đầy đủ trong quá trình đào tạo của mô hình.\n\n\n\n\\(\\gamma\\) được gọi là “tham số tập trung”. Nó đóng một vai trò quan trọng trong việc xác định mức độ mà mô hình nên tập trung vào các lớp bị phân loại sai so với những lớp được phân loại đúng.\nTác động của nó:\n\nMục đích: Mục đích chính của tham số \\(\\gamma\\) trong hàm mất mát Focal là giảm ảnh hưởng của các lớp dễ phân loại và tăng tầm quan trọng của việc hiệu chỉnh các lớp bị phân loại sai. Điều này đặc biệt hữu ích trong các tình huống mà tập dữ liệu có sự mất cân bằng giữa các lớp.\nTác động của việc Thay đổi \\(\\gamma\\): Thuật ngữ \\((1 - p_t)^\\gamma\\) trong hàm mất mát Focal là yếu tố điều chỉnh. Ở đây, \\(p_t\\) đại diện cho xác suất dự đoán của lớp đúng.\n\nNếu \\(p_t\\) gần bằng 1, nghĩa là ví dụ dễ dàng được phân loại, và \\((1 - p_t)^\\gamma\\) sẽ gần bằng 0, đặc biệt khi \\(\\gamma &gt; 0\\).\nNếu \\(p_t\\) xa 1 (tức là dự đoán là không chính xác hoặc không chắc chắn), thì \\((1 - p_t)^\\gamma\\) sẽ lớn hơn, tăng ảnh hưởng của ví dụ đó lên hàm mất mát.\n\\(\\gamma = 0\\): Hàm mất mát Focal giảm xuống còn bằng hàm mất mát cross-entropy tiêu chuẩn, vì yếu tố điều chỉnh trở thành 1 cho tất cả các ví dụ.\n\\(\\gamma &gt; 0\\): Tăng trọng số của các lớp khó phân loại và giảm trọng số của những lớp dễ phân loại. \\(\\gamma\\) càng lớn, mô hình càng tập trung nhiều vào các lớp khó.\n\nLợi ích: Bằng cách điều chỉnh \\(\\gamma\\), hàm mất mát Focal cho phép các mô hình, đặc biệt trong các nhiệm vụ phát hiện đối tượng, trở nên mạnh mẽ hơn trước số lượng lớn các lớp dễ. Thay vì tiêu tốn tài nguyên tính toán cho các lớp dễ dàng, mô hình tập trung nhiều hơn vào các lớp khó, thường chứa nhiều thông tin hơn.\n\nTóm lại, tham số \\(\\gamma\\) trong hàm mất mát Focal cung cấp một cơ chế để nhấn mạnh việc học từ các lớp bị phân loại sai so với những lớp dễ phân loại. Đó là một công cụ để xử lý sự mất cân bằng lớp và đảm bảo rằng mô hình chú ý nhiều hơn đến các lớp mà nó phân loại sai.",
    "crumbs": [
      "Machine learning",
      "Focal loss - handle class imbalance"
    ]
  },
  {
    "objectID": "machine-learning/focal-loss.html#ví-dụ",
    "href": "machine-learning/focal-loss.html#ví-dụ",
    "title": "Focal loss - handle class imbalance",
    "section": "Ví dụ",
    "text": "Ví dụ\n\nTunning alpha and gamma\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nfrom lightgbm.callback import record_evaluation\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# Generate imbalanced data using make_classification\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, weights=[0.99, 0.01], random_state=42)  # 99% of class 0 and 1% of class 1\nX_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n# Define Focal Loss for LightGBM\ndef focal_loss_lgb(y_pred, dtrain, alpha, gamma):\n    a, g = alpha, gamma\n    y_true = dtrain.get_label()\n    p = 1 / (1 + np.exp(-y_pred))\n    loss = -(a * y_true + (1 - a) * (1 - y_true)) * ((1 - (y_true * p + (1 - y_true) * (1 - p))) ** g) * (y_true * np.log(p) + (1 - y_true) * np.log(1 - p))\n    return 'focal_loss', np.mean(loss), False\n\n# Gini coefficient\ndef gini(y_true, y_pred):\n    return 2 * roc_auc_score(y_true, y_pred) - 1\n\n# Optuna study for tuning alpha and gamma\ndef objective(trial):\n    # Parameters to be tuned\n    alpha = trial.suggest_float('alpha', 0.01, 1)\n    gamma = trial.suggest_float('gamma', 0.1, 5)\n    \n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val, reference=train_set)\n\n    param = {\n        'objective': 'binary',\n        'metric': 'custom',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n\n    # Using record_evaluation to capture validation results without printing\n    evals_result = {}\n    model = lgb.train(param, \n                      train_set, \n                      valid_sets=[val_set], \n                      feval=lambda preds, dtrain: focal_loss_lgb(preds, dtrain, alpha, gamma), \n                      callbacks=[record_evaluation(evals_result)])\n    \n    preds = model.predict(X_val)\n    return gini(y_val, preds)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100) \n\n# Results\nbest_params = study.best_params\nbest_score = study.best_value\n\n# Plotting\nalphas = [trial.params['alpha'] for trial in study.trials]\ngammas = [trial.params['gamma'] for trial in study.trials]\nscores_focal = [trial.value for trial in study.trials]\nplt.figure(figsize=(12, 5))\nplt.scatter(alphas, scores_focal, color='blue', label='Alpha')\nplt.scatter(gammas, scores_focal, color='red', label='Gamma')\nplt.xlabel('Parameter Value')\nplt.ylabel('Gini Score')\nplt.title('Effect of Alpha and Gamma on Gini Score')\nplt.legend()\nplt.show()\n\nprint(f\"Best Gini score: {best_score}\")\nprint(f\"Best parameters: {best_params}\")\n\n\n\n\n\n\n\n\nBest Gini score: 0.7631133671742809\nBest parameters: {'alpha': 0.45019195857112326, 'gamma': 0.4919531053413074, 'lambda_l1': 6.640753524365382, 'lambda_l2': 9.081726977645413, 'num_leaves': 21, 'feature_fraction': 0.9362220967011345, 'bagging_fraction': 0.4744110361566192, 'bagging_freq': 2, 'min_child_samples': 12}\n\n\n\n# Optuna study for tuning alpha and gamma\ndef objective(trial):\n    \n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val, reference=train_set)\n\n    params_ce = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n\n    # Using record_evaluation to capture validation results without printing\n    evals_result_ce = {}\n    model_ce = lgb.train(params_ce, train_set, valid_sets=[val_set], callbacks=[record_evaluation(evals_result_ce)])\n    preds_ce = model_ce.predict(X_val)    \n    return gini(y_val, preds_ce)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100) \n\n# Results\nbest_params = study.best_params\nbest_score = study.best_value\nscores_ce = [trial.value for trial in study.trials]\nprint(f\"Best Gini score: {best_score}\")\nprint(f\"Best parameters: {best_params}\")\n\nBest Gini score: 0.7445008460236886\nBest parameters: {'lambda_l1': 5.045686905737897, 'lambda_l2': 8.682377409692112, 'num_leaves': 158, 'feature_fraction': 0.7953399252370615, 'bagging_fraction': 0.43225443468376873, 'bagging_freq': 3, 'min_child_samples': 16}\n\n\n==&gt; Sử dụng focal loss sau 100 lần thử thì tìm được mô hình có gini tốt hơn so với sử dụng cross entropy\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(scores_ce, label='Cross-Entropy', color='red')\nplt.plot(scores_focal, label='Focal Loss', color='blue')\nplt.xlabel('Trial')\nplt.ylabel('ROC AUC Score')\nplt.title('Comparison of Focal Loss and Cross-Entropy over Trials')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Initialize lists to store results\nfocal_scores = []\nce_scores = []\nimbalance_ratios = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n\nfor ratio in imbalance_ratios:\n    # Generate imbalanced data\n    X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, weights=[1-ratio, ratio], random_state=42)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n    \n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val, reference=train_set)\n    \n    # Train with Focal Loss\n    params_focal = {\n        'objective': 'binary',\n        'metric': 'custom',\n        'boosting_type': 'gbdt'\n    }\n    \n    alpha_value = 0.25\n    gamma_value = 2.0\n    evals_result_focal = {}    \n    model_focal = lgb.train(params_focal, train_set, valid_sets=[val_set], \n                        feval=lambda preds, dtrain: focal_loss_lgb(preds, dtrain, alpha_value, gamma_value), \n                        callbacks=[record_evaluation(evals_result_focal)])\n    preds_focal = model_focal.predict(X_val)\n    roc_focal = roc_auc_score(y_val, preds_focal)\n    \n    # Train with Cross-Entropy\n    params_ce = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting_type': 'gbdt'\n    }\n    \n    evals_result_ce = {} \n    model_ce = lgb.train(params_ce, train_set, valid_sets=[val_set], callbacks=[record_evaluation(evals_result_ce)])\n    preds_ce = model_ce.predict(X_val)\n    roc_ce = roc_auc_score(y_val, preds_ce)\n    \n    # Store results\n    focal_scores.append(roc_focal)\n    ce_scores.append(roc_ce)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(imbalance_ratios, focal_scores, marker='o', label='Focal Loss', color='b')\nplt.plot(imbalance_ratios, ce_scores, marker='x', label='Cross-Entropy Loss', color='r')\nplt.xlabel('Imbalance Ratio')\nplt.ylabel('ROC AUC Score')\nplt.title('Effect of Loss Type on Imbalanced Data')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n[LightGBM] [Info] Number of positive: 11, number of negative: 789\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.013750 -&gt; initscore=-4.272871\n[LightGBM] [Info] Start training from score -4.272871\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 11, number of negative: 789\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.013750 -&gt; initscore=-4.272871\n[LightGBM] [Info] Start training from score -4.272871\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 43, number of negative: 757\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.053750 -&gt; initscore=-2.868163\n[LightGBM] [Info] Start training from score -2.868163\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 43, number of negative: 757\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.053750 -&gt; initscore=-2.868163\n[LightGBM] [Info] Start training from score -2.868163\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 82, number of negative: 718\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102500 -&gt; initscore=-2.169750\n[LightGBM] [Info] Start training from score -2.169750\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 82, number of negative: 718\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000299 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102500 -&gt; initscore=-2.169750\n[LightGBM] [Info] Start training from score -2.169750\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 161, number of negative: 639\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.201250 -&gt; initscore=-1.378500\n[LightGBM] [Info] Start training from score -1.378500\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 161, number of negative: 639\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000302 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.201250 -&gt; initscore=-1.378500\n[LightGBM] [Info] Start training from score -1.378500\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 239, number of negative: 561\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298750 -&gt; initscore=-0.853257\n[LightGBM] [Info] Start training from score -0.853257\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 239, number of negative: 561\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000259 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298750 -&gt; initscore=-0.853257\n[LightGBM] [Info] Start training from score -0.853257\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 318, number of negative: 482\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397500 -&gt; initscore=-0.415893\n[LightGBM] [Info] Start training from score -0.415893\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 318, number of negative: 482\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000370 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397500 -&gt; initscore=-0.415893\n[LightGBM] [Info] Start training from score -0.415893\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 396, number of negative: 404\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495000 -&gt; initscore=-0.020001\n[LightGBM] [Info] Start training from score -0.020001\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 396, number of negative: 404\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5100\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495000 -&gt; initscore=-0.020001\n[LightGBM] [Info] Start training from score -0.020001\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n\n\n\n\n\n\n\n\n==&gt; Sử dụng \\(\\alpha\\) và \\(\\gamma\\) chưa tunning trên nhiều kịch bản imbalance khác nhau, gini không có sự khác biệt",
    "crumbs": [
      "Machine learning",
      "Focal loss - handle class imbalance"
    ]
  },
  {
    "objectID": "machine-learning/modelCalibration.html",
    "href": "machine-learning/modelCalibration.html",
    "title": "Model calibration",
    "section": "",
    "text": "Model calibration is a process used in machine learning to ensure that the predicted probabilities from a classification model reflect the true likelihood of the corresponding outcomes. In classification tasks, a well-calibrated model should produce probability estimates that are both reliable and accurate. This is important because many machine learning applications, such as risk assessment, medical diagnosis, and fraud detection, rely on these probability estimates to make decisions.\nCalibration involves adjusting the predicted probabilities from a model to align them with the actual outcomes. There are several methods to calibrate models, and the choice of method depends on the specific algorithm and problem. Some common calibration techniques include:\n\nPlatt Scaling: This is a simple method that fits a logistic regression model to the output of the original model. It essentially learns a transformation to map the model’s raw scores to calibrated probabilities.\nIsotonic Regression: Isotonic regression is a non-parametric approach that fits a piecewise-constant, non-decreasing function to the predicted probabilities. It can be more flexible than Platt scaling in some cases.\nBeta Calibration: This method models the predicted probabilities as Beta distributions and estimates the parameters of these distributions to better match the observed outcomes.\nHistogram Binning: This technique divides the predicted probabilities into bins and estimates the true class frequencies within each bin. It then uses these frequencies to adjust the predicted probabilities.\nBayesian Calibration: Bayesian methods can be used to update the model’s probability estimates based on prior knowledge or data. This approach can be especially useful when you have a limited amount of data.\n\nThe goal of model calibration is to ensure that, for instance, if a model predicts a 70% probability of an event happening, it should be correct approximately 70% of the time in practice. Calibrated models provide more meaningful and reliable probability estimates, making them easier to interpret and use for decision-making. In applications where it’s important to understand the level of confidence associated with predictions, model calibration is a critical step in improving the model’s utility.",
    "crumbs": [
      "Machine learning",
      "Model calibration"
    ]
  },
  {
    "objectID": "machine-learning/modelCalibration.html#model-calibration",
    "href": "machine-learning/modelCalibration.html#model-calibration",
    "title": "Model calibration",
    "section": "",
    "text": "Model calibration is a process used in machine learning to ensure that the predicted probabilities from a classification model reflect the true likelihood of the corresponding outcomes. In classification tasks, a well-calibrated model should produce probability estimates that are both reliable and accurate. This is important because many machine learning applications, such as risk assessment, medical diagnosis, and fraud detection, rely on these probability estimates to make decisions.\nCalibration involves adjusting the predicted probabilities from a model to align them with the actual outcomes. There are several methods to calibrate models, and the choice of method depends on the specific algorithm and problem. Some common calibration techniques include:\n\nPlatt Scaling: This is a simple method that fits a logistic regression model to the output of the original model. It essentially learns a transformation to map the model’s raw scores to calibrated probabilities.\nIsotonic Regression: Isotonic regression is a non-parametric approach that fits a piecewise-constant, non-decreasing function to the predicted probabilities. It can be more flexible than Platt scaling in some cases.\nBeta Calibration: This method models the predicted probabilities as Beta distributions and estimates the parameters of these distributions to better match the observed outcomes.\nHistogram Binning: This technique divides the predicted probabilities into bins and estimates the true class frequencies within each bin. It then uses these frequencies to adjust the predicted probabilities.\nBayesian Calibration: Bayesian methods can be used to update the model’s probability estimates based on prior knowledge or data. This approach can be especially useful when you have a limited amount of data.\n\nThe goal of model calibration is to ensure that, for instance, if a model predicts a 70% probability of an event happening, it should be correct approximately 70% of the time in practice. Calibrated models provide more meaningful and reliable probability estimates, making them easier to interpret and use for decision-making. In applications where it’s important to understand the level of confidence associated with predictions, model calibration is a critical step in improving the model’s utility.",
    "crumbs": [
      "Machine learning",
      "Model calibration"
    ]
  },
  {
    "objectID": "machine-learning/modelCalibration.html#history-of-calibration-models",
    "href": "machine-learning/modelCalibration.html#history-of-calibration-models",
    "title": "Model calibration",
    "section": "History of calibration models",
    "text": "History of calibration models\nThe history of calibration models in the context of machine learning and statistics is closely tied to the need for well-calibrated probability estimates, especially in classification tasks. Here’s a brief overview of the history and development of calibration models:\n\nEarly Days of Probability Estimation (Pre-2000s): In the early years of machine learning and statistics, the primary focus was on developing classification models that predict class labels. Probability estimates were often not a central concern.\nRise of Support Vector Machines (SVMs): In the 1990s and early 2000s, Support Vector Machines gained popularity. These models produced decision functions that were not inherently calibrated probability estimates, leading to a need for calibration techniques.\nPlatt Scaling (1999): John Platt introduced Platt Scaling, a logistic regression-based calibration technique, in his paper “Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.” This method became one of the earliest and widely used approaches for converting SVM scores into well-calibrated probabilities.\nLogistic Regression Calibration: Logistic regression, a well-established statistical technique, was used as an early calibration method for various classifiers. It helped map model scores to calibrated probabilities.\nDevelopment of Calibration Metrics (2000s): During this period, researchers started to develop calibration metrics, such as the Brier Score and calibration plots, to quantitatively measure the calibration performance of models.\nBayesian Calibration (2000s): Bayesian approaches for calibration started to gain attention. These methods allow for the incorporation of prior knowledge and often provide robust calibration.\nIsotonic Regression (2000s): Isotonic regression became a popular non-parametric technique for calibration. It offers flexibility by allowing for monotonic transformations of probability estimates.\nImprovements in Machine Learning Models (2010s): The advancement of machine learning models, such as gradient-boosted trees and neural networks, led to the need for more advanced calibration methods. Techniques like Beta Calibration started to emerge.\nSoftware Libraries and Packages: As machine learning and data science gained prominence, software libraries and packages like scikit-learn and TensorFlow included calibration functionalities, making it easier for practitioners to apply calibration techniques.\nOngoing Research and Development: Research in calibration continues, focusing on more complex models, large-scale applications, and novel techniques. The field is evolving to address challenges in deep learning and ensemble methods.\n\nIn summary, the history of calibration models reflects the growing awareness of the importance of well-calibrated probability estimates in various machine learning applications. Over the years, a variety of techniques and methods have been developed to address calibration issues, enabling machine learning models to provide more reliable and interpretable probability estimates for informed decision-making. The field continues to evolve as new challenges and opportunities emerge in machine learning and data analysis.",
    "crumbs": [
      "Machine learning",
      "Model calibration"
    ]
  },
  {
    "objectID": "machine-learning/modelCalibration.html#beta-calibration",
    "href": "machine-learning/modelCalibration.html#beta-calibration",
    "title": "Model calibration",
    "section": "Beta Calibration",
    "text": "Beta Calibration\nBeta Calibration is one of the more recent calibration techniques that has gained attention in the field of machine learning. Its emergence can be attributed to several factors:\n\nImproved Calibration Needs: As machine learning models have become increasingly popular for a wide range of applications, the need for well-calibrated probability estimates has grown. Many machine learning models, including complex ensemble methods and deep learning models, often produce uncalibrated or poorly calibrated probability estimates. This need for improved calibration led to the exploration of novel calibration techniques, including Beta Calibration.\nFlexibility in Modeling Probability Distributions: Beta Calibration offers flexibility in modeling probability distributions. It assumes that predicted probabilities can be represented by a Beta distribution, which is a flexible parametric distribution. This flexibility allows it to capture a wide range of probability distribution shapes, making it suitable for a variety of real-world scenarios.\nSuitability for Imbalanced Datasets: In many practical applications, datasets are imbalanced, meaning one class significantly outnumbers the other. Traditional calibration techniques may not be effective in such cases. Beta Calibration’s ability to handle imbalanced data by modeling the distribution of predicted probabilities is a valuable feature.\nAddressing Overconfidence or Underconfidence: Beta Calibration is effective in addressing issues of model overconfidence or underconfidence. Some models may produce predicted probabilities that are too extreme (close to 0 or 1) and do not accurately reflect the true likelihood of events occurring. Beta Calibration can moderate these extreme probabilities to provide more reliable estimates.\nGrowing Research and Tools: As researchers and practitioners have recognized the importance of calibration, there has been an increased focus on developing and providing tools and libraries that support calibration techniques. The availability of software packages like the betacal package has made it easier for data scientists to apply Beta Calibration.\nComparative Studies: Comparative studies and evaluations of different calibration techniques have highlighted the effectiveness of Beta Calibration in certain scenarios. This has contributed to its adoption and prominence.\nVersatility in Calibration Challenges: Beta Calibration can be used in a wide range of applications and is not limited to specific types of models or datasets. Its versatility in addressing calibration challenges makes it a valuable addition to the calibration toolbox.\n\nIn summary, Beta Calibration emerged as a response to the growing need for more advanced and flexible calibration techniques that can handle a variety of machine learning models and data scenarios. Its ability to model predicted probabilities as Beta distributions and address issues like overconfidence or underconfidence has made it a valuable tool for improving the reliability and utility of machine learning models in practical applications.\n\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n# Load the German Credit Data\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\nX_cal, X_test, y_cal, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test)\ny_train[y_train == 2] = 0\ny_test[y_test == 2] = 0\ny_cal[y_cal == 2] = 0\n\n\ny_cal.value_counts()\n\n1    175\n0     75\nName: class, dtype: int64\n\n\n\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import brier_score_loss\nimport matplotlib.pyplot as plt\n\n# Load the German Credit Data\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\nX_cal, X_test, y_cal, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test)\n\n# Convert class labels to binary (0 and 1)\ny_train = (y_train == 1).astype(int)\ny_cal = (y_cal == 1).astype(int)\ny_test = (y_test == 1).astype(int)\n\n\n# Train a classifier (LightGBM in this example)\nclf = lgb.LGBMClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Predict probabilities on the test set\npredicted_probabilities = clf.predict_proba(X_test)[:, 1]\ncal_probas = clf.predict_proba(X_cal)[:, 1] \n\n# Isotonic Calibration\niso_calibrator = IsotonicRegression()\niso_calibrator.fit(cal_probas, y_cal)\niso_calibrated_probabilities = iso_calibrator.predict(cal_probas)\niso_brier = brier_score_loss(y_cal, iso_calibrated_probabilities)\nprint(f'Brier Score after Isotonic Calibration: {iso_brier:.4f}')\n\n# Logistic Calibration\nlogistic_calibrator = LogisticRegression(C=99999999999)\nlogistic_calibrator.fit(cal_probas.reshape(-1, 1), y_cal)\nlogistic_calibrated_probabilities = logistic_calibrator.predict_proba(cal_probas.reshape(-1, 1))[:, 1]\nlogistic_brier = brier_score_loss(y_cal, logistic_calibrated_probabilities)\nprint(f'Brier Score after Logistic Calibration: {logistic_brier:.4f}')\n\n# Beta Calibration\nfrom betacal import BetaCalibration\nbeta_calibrator = BetaCalibration()\nbeta_calibrator.fit(cal_probas, y_cal)\nbeta_calibrated_probabilities = beta_calibrator.predict(cal_probas)\nbeta_brier = brier_score_loss(y_cal, beta_calibrated_probabilities)\nprint(f'Brier Score after Beta Calibration: {beta_brier:.4f}')\n\nBrier Score after Isotonic Calibration: 0.1745\nBrier Score after Logistic Calibration: 0.1876\nBrier Score after Beta Calibration: 0.1827\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\n# Calculate Gini coefficient for Isotonic calibration\ngini_iso = 2 * roc_auc_score(y_cal, iso_calibrated_probabilities) - 1\nprint(f'Gini for Isotonic Calibration: {gini_iso:.4f}')\n\n# Calculate Gini coefficient for Logistic calibration\ngini_logistic = 2 * roc_auc_score(y_cal, logistic_calibrated_probabilities) - 1\nprint(f'Gini for Logistic Calibration: {gini_logistic:.4f}')\n\n# Calculate Gini coefficient for Beta calibration\ngini_beta = 2 * roc_auc_score(y_cal, beta_calibrated_probabilities) - 1\nprint(f'Gini for Beta Calibration: {gini_beta:.4f}')\n\nGini for Isotonic Calibration: 0.5006\nGini for Logistic Calibration: 0.4539\nGini for Beta Calibration: 0.4539\n\n\n\nfrom sklearn.calibration import calibration_curve\n# Generate calibration plots\nplt.figure(figsize=(12, 6))\n\n# Calibration Curve (Isotonic)\nplt.subplot(131)\nfraction_of_positives, mean_predicted_value = calibration_curve(y_cal, iso_calibrated_probabilities, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, marker='o', label='Calibrated Probabilities (Isotonic)')\nplt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve (Isotonic)')\nplt.legend()\n\n# Calibration Curve (Logistic)\nplt.subplot(132)\nfraction_of_positives, mean_predicted_value = calibration_curve(y_cal, logistic_calibrated_probabilities, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, marker='o', label='Calibrated Probabilities (Logistic)')\nplt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve (Logistic)')\nplt.legend()\n\n# Calibration Curve (Beta)\nplt.subplot(133)\nfraction_of_positives, mean_predicted_value = calibration_curve(y_cal, beta_calibrated_probabilities, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, marker='o', label='Calibrated Probabilities (Beta)')\nplt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve (Beta)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Model calibration"
    ]
  },
  {
    "objectID": "machine-learning/modelCalibration.html#brier-score",
    "href": "machine-learning/modelCalibration.html#brier-score",
    "title": "Model calibration",
    "section": "Brier Score",
    "text": "Brier Score\nThe Brier Score, also known as the Brier Loss or Brier’s Probability Score, is a commonly used metric to assess the calibration quality of probabilistic predictions generated by a classification model. The Brier Score quantifies the mean squared difference between predicted probabilities and actual outcomes. The formula for the Brier Score is as follows:\n\\[ \\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^{N} (P_i - O_i)^2 \\]\nWhere: - \\(N\\) is the total number of samples. - \\(P_i\\) is the predicted probability for the positive class (usually class 1) for sample i. - \\(O_i\\) is the actual outcome for sample i, which is binary (1 for a positive outcome, 0 for a negative outcome).\nThe Brier Score ranges from 0 to 1, where lower values indicate better calibration. A Brier Score of 0 means perfect calibration, where the predicted probabilities are identical to the actual outcomes. A Brier Score of 1 indicates the worst calibration, where the predicted probabilities are far from the true outcomes.\nThe Brier Score is used for calibration purposes for the following reasons:\n\nMeasuring Calibration Quality: The Brier Score provides a quantitative measure of how well the predicted probabilities align with the true outcomes. It directly assesses the calibration quality of a model’s probability estimates.\nSensitivity to Miscalibration: It is sensitive to both overconfidence and underconfidence in predicted probabilities. If a model is poorly calibrated, the Brier Score will be high, highlighting the need for calibration.\nComparison of Calibration Techniques: The Brier Score allows for the comparison of different calibration techniques. It can help you evaluate the effectiveness of calibration methods in improving the reliability of probability estimates.\nInterpretability: The Brier Score is easy to understand and interpret. Lower Brier Scores indicate better calibration, making it a useful metric for explaining the quality of probability estimates to stakeholders.\nDirect Application to Decision Thresholds: In binary classification tasks, the Brier Score can assist in selecting appropriate decision thresholds. By analyzing the Brier Score at different threshold values, you can make decisions that balance precision and recall based on the calibration performance.\n\nThresholds for Brier Score:\n\nPerfect Calibration (Threshold: 0): Achieving a Brier Score of 0 represents perfect calibration, where the model’s predicted probabilities precisely match the actual outcomes. While this is the ideal calibration, it is rarely attainable in practice.\nGood Calibration (Threshold: &lt; 0.25): Brier Scores below 0.25 are often regarded as a benchmark for good calibration. In such cases, the predicted probabilities are reasonably aligned with the true outcomes, indicating reliable probability estimates.\nFair Calibration (Threshold: 0.25 - 0.5): Brier Scores falling between 0.25 and 0.5 suggest moderately calibrated models. While not perfect, these models provide fairly reliable probability estimates, making them suitable for various applications.\nPoor Calibration (Threshold: &gt; 0.5): Brier Scores exceeding 0.5 typically indicate poor calibration. Such scores imply that the model’s predicted probabilities are unreliable for decision-making, as they deviate significantly from the actual outcomes.\nDecision-Making Threshold (Application-Dependent): The choice of an acceptable Brier Score threshold depends on the specific application’s criticality and requirements. In critical applications where decisions are heavily based on probabilities, a lower threshold (e.g., 0.1) might be necessary for confidence. In less critical scenarios, a threshold closer to 0.5 may be considered acceptable. The chosen threshold should align with the level of risk tolerance and the consequences of incorrect decisions within the application context.\n\nIn summary, the Brier Score is a valuable metric for evaluating the calibration of probabilistic predictions, making it an essential tool for assessing the reliability of classification models and the effectiveness of calibration techniques. Its simplicity and direct connection to calibration quality make it a widely used metric in machine learning and statistics.",
    "crumbs": [
      "Machine learning",
      "Model calibration"
    ]
  },
  {
    "objectID": "machine-learning/Partial_dependence_and_Accumulated_local_Profiles.html",
    "href": "machine-learning/Partial_dependence_and_Accumulated_local_Profiles.html",
    "title": "Partial-dependence Profiles vs Accumulated-local Profiles",
    "section": "",
    "text": "Partial Dependence Profiles (PDP) được tính dựa trên việc ước lượng giá trị trung bình của đầu ra dự đoán khi thay đổi giá trị của một hoặc một nhóm đặc trưng cụ thể, trong khi giữ các đặc trưng còn lại cố định. Phương pháp này giúp hiểu ảnh hưởng trung bình của một đặc trưng đến dự đoán của mô hình.",
    "crumbs": [
      "Machine learning",
      "Partial-dependence Profiles vs Accumulated-local Profiles"
    ]
  },
  {
    "objectID": "machine-learning/Partial_dependence_and_Accumulated_local_Profiles.html#python-snippets-use-dalex",
    "href": "machine-learning/Partial_dependence_and_Accumulated_local_Profiles.html#python-snippets-use-dalex",
    "title": "Partial-dependence Profiles vs Accumulated-local Profiles",
    "section": "Python snippets use dalex",
    "text": "Python snippets use dalex\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom dalex import Explainer\nimport matplotlib.pyplot as plt\n\n# Create Explainer with DALEX\nexplainer = Explainer(model, X_train, y_train, label=\"LightGBM - Wine Quality\")\n\n# Generate Partial Dependence Profile (PDP) for a selected feature (e.g., alcohol)\npdp = explainer.model_profile(variables=\"alcohol\", type='partial')\n\n# Generate Accumulated Local Profile (ALE) for the same feature\nale = explainer.model_profile(variables=\"alcohol\", type='accumulated')\n\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 1279 rows 11 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 1279 values\n  -&gt; model_class       : lightgbm.sklearn.LGBMClassifier (default)\n  -&gt; label             : LightGBM - Wine Quality\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x000001984DBC6E60&gt; will be used (default)\n  -&gt; predict function  : Accepts pandas.DataFrame and numpy.ndarray.\n  -&gt; predicted values  : min = 9.3e-09, mean = 0.0336, max = 0.998\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = 3.0, mean = 5.59, max = 8.0\n  -&gt; model_info        : package lightgbm\n\nA new explainer has been created!\n\n\nCalculating ceteris paribus: 100%|██████████| 1/1 [00:00&lt;00:00,  3.79it/s]\nCalculating ceteris paribus: 100%|██████████| 1/1 [00:00&lt;00:00,  3.68it/s]\nCalculating accumulated dependency: 100%|██████████| 1/1 [00:00&lt;00:00,  8.43it/s]\n\n\n\n# pdp.plot()\n# ale.plot()\n# Plot PDP and ALE in subplots\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot PDP\naxes[0].plot(pdp.result[\"_x_\"], pdp.result[\"_yhat_\"], label=\"PDP\")\naxes[0].set_title(\"Partial Dependence Profile (PDP) for Alcohol\")\naxes[0].set_xlabel(\"Alcohol\")\naxes[0].set_ylabel(\"Predicted Value\")\naxes[0].legend()\n\n# Plot ALE\naxes[1].plot(ale.result[\"_x_\"], ale.result[\"_yhat_\"], label=\"ALE\", color=\"orange\")\naxes[1].set_title(\"Accumulated Local Effect (ALE) for Alcohol\")\naxes[1].set_xlabel(\"Alcohol\")\naxes[1].set_ylabel(\"Predicted Value\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Partial-dependence Profiles vs Accumulated-local Profiles"
    ]
  },
  {
    "objectID": "machine-learning/Partial_dependence_and_Accumulated_local_Profiles.html#custom-lightgbm-shap",
    "href": "machine-learning/Partial_dependence_and_Accumulated_local_Profiles.html#custom-lightgbm-shap",
    "title": "Partial-dependence Profiles vs Accumulated-local Profiles",
    "section": "Custom lightGBM shap",
    "text": "Custom lightGBM shap\n\nimport numpy as np\nimport pandas as pd\nimport shap\nimport matplotlib.pyplot as plt\nimport os\n\nclass ShapExplain:\n    def __init__(self, model):\n        if hasattr(model, 'classes_') and hasattr(model, 'feature_name_'):\n            self.model = model\n            self.raw_scores = None\n            self.proba_scores = None\n            self.labels = None\n            self.shap_values = None\n            self.shap_contributions = None\n\n            # Initialize class attributes from the model\n            self.num_classes = len(model.classes_)\n            self.num_features = len(model.feature_name_)\n            self.feature_names = model.feature_name_ + [\"Bias\"]\n        else:\n            raise ValueError(\"The provided model does not have the required attributes 'classes_' or 'feature_name_'.\")\n\n    def predict_raw_scores(self, X):\n        if hasattr(self.model, 'predict'):\n            self.raw_scores = self.model.predict(X, raw_score=True)\n            return self.raw_scores\n        else:\n            raise AttributeError(\"The model does not support raw score predictions.\")\n\n    def predict_proba(self, X):\n        if hasattr(self.model, 'predict_proba'):\n            self.proba_scores = self.model.predict_proba(X)\n            return self.proba_scores\n        else:\n            raise AttributeError(\"The model does not support probability predictions.\")\n\n    def predict_labels(self, X):\n        self.predict_proba(X)  # Ensure probabilities are always calculated\n        self.labels = [np.argmax(score) for score in self.proba_scores]\n        return self.labels\n\n    def calculate_shap_values(self, X):\n        if hasattr(self.model, 'predict'):\n            self.shap_values = self.model.predict(X, pred_contrib=True)\n            return self.shap_values\n        else:\n            raise AttributeError(\"The model does not support SHAP value predictions.\")\n\n    def split_shap_values(self):\n        if self.shap_values is None:\n            raise ValueError(\"SHAP values have not been calculated. Call calculate_shap_values first.\")\n        \n        shap_contributions = {}\n        for class_idx in range(self.num_classes):\n            start_idx = class_idx * (self.num_features + 1)\n            end_idx = start_idx + (self.num_features + 1)\n            shap_contributions[f\"Class_{class_idx}\"] = pd.DataFrame(\n                self.shap_values[:, start_idx:end_idx], columns=self.feature_names\n            )\n        self.shap_contributions = shap_contributions\n        return self.shap_contributions\n\n    def plot_shap_summary(self, plot_type='bar', output_dir=None, title_prefix=None):        \n\n        # Ensure output directory exists if specified\n        if output_dir and not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        for class_name, shap_values_df in self.shap_contributions.items():\n            shap_values = shap_values_df.iloc[:, :-1].values\n            feature_names = shap_values_df.columns[:-1]  # Exclude 'Bias' from feature names\n\n            plt.figure()\n            shap.summary_plot(shap_values, feature_names=feature_names, plot_type=plot_type, show=False)\n\n            title = f'SHAP Summary Plot for {class_name}'\n            if title_prefix:\n                title = f'{title_prefix} {title}'\n            plt.title(title)\n\n            if output_dir:\n                filename = f'{title_prefix}_{class_name}_shap_summary.png' if title_prefix else f'{class_name}_shap_summary.png'\n                plt.savefig(os.path.join(output_dir, filename), bbox_inches='tight')\n                plt.close()\n            else:\n                plt.show()\n\n\nimport lightgbm as lgb\nimport pandas as pd\n\n# Train an LGBMClassifier\nmodel = lgb.LGBMClassifier(verbosity = -1)\nmodel.fit(X_train, y_train, categorical_feature='auto')\n\n# Instantiate ShapExplain with the trained model\nshap_explain = ShapExplain(model)\n\n# Make predictions and calculate SHAP values\nraw_scores = shap_explain.predict_raw_scores(X_test)\nproba_scores = shap_explain.predict_proba(X_test)\nlabels = shap_explain.predict_labels(X_test)\nshap_values = shap_explain.calculate_shap_values(X_test)\nshap_contributions = shap_explain.split_shap_values()\n\n# Optionally, plot and save SHAP summary\nshap_explain.plot_shap_summary(plot_type='violin', output_dir='shap_plots', title_prefix='Wine Dataset')\n\n# Save results to Excel\nwith pd.ExcelWriter('shap_explain_output.xlsx', engine='openpyxl') as writer:\n    # Convert raw_scores and labels to DataFrames and write to Excel\n    pd.DataFrame(raw_scores).to_excel(writer, sheet_name='Raw Scores')\n    pd.DataFrame(proba_scores, columns=[f\"Class_{i}\" for i in range(shap_explain.num_classes)]).to_excel(writer, sheet_name='Probabilities')\n    pd.DataFrame(labels, columns=['Predicted Labels']).to_excel(writer, sheet_name='Labels')\n\n    # Write each class's SHAP contributions to a separate sheet\n    for class_name, df in shap_explain.shap_contributions.items():\n        df.to_excel(writer, sheet_name=f'SHAP Values {class_name}')",
    "crumbs": [
      "Machine learning",
      "Partial-dependence Profiles vs Accumulated-local Profiles"
    ]
  },
  {
    "objectID": "machine-learning/Partial_dependence_and_Accumulated_local_Profiles.html#references",
    "href": "machine-learning/Partial_dependence_and_Accumulated_local_Profiles.html#references",
    "title": "Partial-dependence Profiles vs Accumulated-local Profiles",
    "section": "References",
    "text": "References\n\nhttps://ema.drwhy.ai/partialDependenceProfiles.html\nhttps://docs.seldon.io/projects/alibi/en/latest/methods/PartialDependence.html\nhttps://www.blog.trainindata.com/partial-dependence-plots-with-python\nhttps://www.analyticsvidhya.com/blog/2020/10/accumulated-local-effects-ale-feature-effects-global-interpretability/",
    "crumbs": [
      "Machine learning",
      "Partial-dependence Profiles vs Accumulated-local Profiles"
    ]
  },
  {
    "objectID": "machine-learning/cal_shap_use_lightgbm.html",
    "href": "machine-learning/cal_shap_use_lightgbm.html",
    "title": "Tính toán giá trị SHAP sử dụng thư viện LightGBM",
    "section": "",
    "text": "Cài đặt thư viện cần thiết:\n\nĐảm bảo rằng bạn đã cài đặt các thư viện shap, lightgbm, pandas, numpy, scikit-learn, và scipy.\n\npip install shap lightgbm pandas numpy scikit-learn scipy\nTải dữ liệu và chuẩn bị:\n\nSử dụng load_breast_cancer từ sklearn.datasets để tải dữ liệu.\nChuyển đổi dữ liệu thành DataFrame và chia thành tập huấn luyện và tập kiểm tra.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport shap\nimport lightgbm as lgbm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom scipy.special import expit\n\nshap.initjs()\ndata = load_breast_cancer()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nIProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n\n\n\n\n\nHuấn luyện mô hình:\n\nKhởi tạo và huấn luyện một mô hình LightGBM với tập huấn luyện.\n\n\n\nmodel = lgbm.LGBMClassifier(verbose=-1)\nmodel.fit(X_train, y_train)\n\nLGBMClassifier(verbose=-1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifierLGBMClassifier(verbose=-1)\n\n\n\nTính toán giá trị SHAP:\n\nKhởi tạo TreeExplainer với mô hình đã huấn luyện.\nTính toán giá trị SHAP cho tập huấn luyện.\n\n\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_train)\n\nLightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n\n\n\nTạo biểu đồ SHAP:\n\nLấy giá trị SHAP cho một hàng cụ thể và tạo biểu đồ force plot.\n\n\n\nclass_idx = 1  # Chỉ số của lớp (class) mà bạn quan tâm\nrow_idx = 0    # Chỉ số của hàng dữ liệu mà bạn quan tâm\nexpected_value = explainer.expected_value[class_idx]\nshap_value = shap_values[class_idx][row_idx]\nshap.force_plot(\n    base_value=expected_value,\n    shap_values=shap_value,\n    features=X_train.iloc[row_idx, :],\n    link=\"logit\",\n)\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nTính toán xác suất SHAP:\n\nDự đoán xác suất với mô hình.\nTính giá trị trung bình của raw scores.\nTính toán giá trị base value và tổng giá trị SHAP cho hàng cụ thể.\nSử dụng hàm sigmoid (expit) để tính toán xác suất SHAP.\n\n\n\nfrom scipy.special import expit\n\n# Dự đoán xác suất\nmodel_proba = model.predict_proba(X_train.iloc[[row_idx]])\n\n# Giá trị raw scores trung bình\nmean_raw_score = model.predict(X_train, raw_score=True).mean()\n\n# Giá trị base value\nbv = explainer.expected_value[class_idx]\n\n# Tổng giá trị SHAP\nsv_0 = shap_values[class_idx][row_idx].sum()\n\n# Tính xác suất SHAP\nshap_proba = expit(bv + sv_0)\n\nprint(\"Model Probability:\", model_proba)\nprint(\"Mean Raw Score:\", mean_raw_score)\nprint(\"Base Value:\", bv)\nprint(\"Summed SHAP Values:\", sv_0)\nprint(\"SHAP Probability:\", shap_proba)\n\nModel Probability: [[0.00275887 0.99724113]]\nMean Raw Score: 2.4839751932445577\nBase Value: 2.4839751932445573\nSummed SHAP Values: 3.40619583933988\nSHAP Probability: 0.9972411289322419",
    "crumbs": [
      "Machine learning",
      "Tính toán giá trị SHAP sử dụng thư viện LightGBM"
    ]
  },
  {
    "objectID": "machine-learning/IsolationForest.html",
    "href": "machine-learning/IsolationForest.html",
    "title": "Isolation Forest - anomaly detection",
    "section": "",
    "text": "Bước 1: Chuẩn bị Dữ liệu - Bắt đầu bằng việc thu thập và tải dữ liệu mà bạn muốn phát hiện các dấu hiệu bất thường.\nBước 2: Tải và Import Thư viện - Trong Python, sử dụng các thư viện sau: numpy, pandas, và sklearn.ensemble.\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nBước 3: Chuẩn bị dữ liệu - Đảm bảo rằng dữ liệu của bạn đã được chuẩn bị và lấy ra từ các cột không mong muốn.\nBước 4: Tạo Mô hình Isolation Forest - Sử dụng IsolationForest để tạo một mô hình phát hiện dấu hiệu bất thường. Bạn có thể tuỳ chỉnh mô hình bằng cách thay đổi các tham số, chẳng hạn như contamination (tỷ lệ dấu hiệu bất thường trong dữ liệu).\nmodel = IsolationForest(contamination=0.05)  # Tuỳ chỉnh contamination tại đây\nBước 5: Huấn luyện Mô hình - Sử dụng dữ liệu huấn luyện của bạn để huấn luyện mô hình Isolation Forest.\nmodel.fit(X)\nBước 6: Dự đoán Dấu hiệu Bất thường - Sử dụng mô hình đã huấn luyện để dự đoán dấu hiệu bất thường trên tập dữ liệu kiểm tra.\npredictions = model.predict(X)\nBước 7: Lấy Điểm Anomaly - Sử dụng phương thức decision_function để lấy điểm anomaly cho mỗi mẫu dữ liệu. Điểm càng thấp càng có khả năng là dấu hiệu bất thường.\nanomaly_scores = model.decision_function(X)\nBước 8: Xác định Dấu hiệu Bất thường - Dùng ngưỡng (threshold) để xác định dấu hiệu bất thường dựa trên điểm anomaly. Bạn có thể tuỳ chỉnh ngưỡng theo ý muốn.\nthreshold = -0.1  # Ngưỡng ngầm định, điều chỉnh tùy theo nhu cầu\nanomalies = X[anomaly_scores &lt; threshold]\nBước 9: Xuất kết quả - Xuất các mẫu bất thường mà bạn đã xác định từ dữ liệu.\nprint(\"Dấu hiệu bất thường:\")\nprint(anomalies)\nLưu ý rằng bạn có thể tùy chỉnh các tham số và ngưỡng để điều chỉnh độ nhạy của việc phát hiện dấu hiệu bất thường dựa trên nhu cầu của bạn.\n\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\n\n\n# Create an Isolation Forest model\ncontamination = 0.05\nmodel = IsolationForest(contamination=contamination)\n\n# Fit the model to your data\nmodel.fit(X)\n\n# Predict anomalies (1 for inliers, -1 for outliers)\npredictions = model.predict(X)\n\n# Get the anomaly scores (the lower the score, the more likely it's an anomaly)\nanomaly_scores = model.decision_function(X)\n\n# Identify anomalies\nanomalies = data[predictions == -1]\n\nc:\\Users\\binhnn2\\anaconda3\\envs\\env_pycaret\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n  warnings.warn(\n\n\n\nanomalies\n\n\n\n\n\n\n\n\nexisting_checking\nduration\ncredit_history\npurpose\ncredit_amount\nsavings\nemployment\ninstallment_rate\npersonal_status\nother_debtors\n...\nproperty\nage\nother_installment_plans\nhousing\nexisting_credits\njob\npeople_liable\ntelephone\nforeign_worker\nclass\n\n\n\n\n44\nA11\n48\nA34\nA41\n6143\nA61\nA75\n4\nA92\nA101\n...\nA124\n58\nA142\nA153\n2\nA172\n1\nA191\nA201\n2\n\n\n55\nA14\n6\nA31\nA40\n783\nA65\nA73\n1\nA93\nA103\n...\nA121\n26\nA142\nA152\n1\nA172\n2\nA191\nA201\n1\n\n\n87\nA12\n36\nA32\nA46\n12612\nA62\nA73\n1\nA93\nA101\n...\nA124\n47\nA143\nA153\n1\nA173\n2\nA192\nA201\n2\n\n\n99\nA12\n20\nA33\nA41\n7057\nA65\nA74\n3\nA93\nA101\n...\nA122\n36\nA141\nA151\n2\nA174\n2\nA192\nA201\n1\n\n\n105\nA12\n24\nA34\nA410\n11938\nA61\nA73\n2\nA93\nA102\n...\nA123\n39\nA143\nA152\n2\nA174\n2\nA192\nA201\n2\n\n\n110\nA12\n6\nA33\nA49\n1449\nA62\nA75\n1\nA91\nA101\n...\nA123\n31\nA141\nA152\n2\nA173\n2\nA191\nA201\n1\n\n\n154\nA12\n24\nA33\nA49\n6967\nA62\nA74\n4\nA93\nA101\n...\nA123\n36\nA143\nA151\n1\nA174\n1\nA192\nA201\n1\n\n\n157\nA11\n12\nA31\nA48\n339\nA61\nA75\n4\nA94\nA101\n...\nA123\n45\nA141\nA152\n1\nA172\n1\nA191\nA201\n1\n\n\n175\nA14\n30\nA31\nA41\n7485\nA65\nA71\n4\nA92\nA101\n...\nA121\n53\nA141\nA152\n1\nA174\n1\nA192\nA201\n2\n\n\n181\nA12\n36\nA33\nA49\n4455\nA61\nA73\n2\nA91\nA101\n...\nA121\n30\nA142\nA152\n2\nA174\n1\nA192\nA201\n2\n\n\n186\nA12\n9\nA31\nA41\n5129\nA61\nA75\n2\nA92\nA101\n...\nA124\n74\nA141\nA153\n1\nA174\n2\nA192\nA201\n2\n\n\n191\nA12\n48\nA30\nA49\n3844\nA62\nA74\n4\nA93\nA101\n...\nA124\n34\nA143\nA153\n1\nA172\n2\nA191\nA201\n2\n\n\n226\nA12\n48\nA32\nA43\n10961\nA64\nA74\n1\nA93\nA102\n...\nA124\n27\nA141\nA152\n2\nA173\n1\nA192\nA201\n2\n\n\n263\nA14\n12\nA34\nA46\n2748\nA61\nA75\n2\nA92\nA101\n...\nA124\n57\nA141\nA153\n3\nA172\n1\nA191\nA201\n1\n\n\n272\nA12\n48\nA31\nA40\n12169\nA65\nA71\n4\nA93\nA102\n...\nA124\n36\nA143\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n287\nA12\n48\nA33\nA410\n7582\nA62\nA71\n2\nA93\nA101\n...\nA124\n31\nA143\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n304\nA14\n48\nA34\nA40\n10127\nA63\nA73\n2\nA93\nA101\n...\nA124\n44\nA141\nA153\n1\nA173\n1\nA191\nA201\n2\n\n\n306\nA14\n30\nA32\nA41\n4811\nA65\nA74\n2\nA92\nA101\n...\nA122\n24\nA142\nA151\n1\nA172\n1\nA191\nA201\n1\n\n\n333\nA14\n48\nA34\nA41\n11590\nA62\nA73\n2\nA92\nA101\n...\nA123\n24\nA141\nA151\n2\nA172\n1\nA191\nA201\n2\n\n\n343\nA12\n18\nA32\nA49\n4439\nA61\nA75\n1\nA93\nA102\n...\nA121\n33\nA141\nA152\n1\nA174\n1\nA192\nA201\n1\n\n\n373\nA14\n60\nA34\nA40\n13756\nA65\nA75\n2\nA93\nA101\n...\nA124\n63\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n374\nA12\n60\nA31\nA410\n14782\nA62\nA75\n3\nA92\nA101\n...\nA124\n60\nA141\nA153\n2\nA174\n1\nA192\nA201\n2\n\n\n395\nA12\n39\nA33\nA46\n11760\nA62\nA74\n2\nA93\nA101\n...\nA124\n32\nA143\nA151\n1\nA173\n1\nA192\nA201\n1\n\n\n449\nA12\n15\nA33\nA45\n1512\nA64\nA73\n3\nA94\nA101\n...\nA122\n61\nA142\nA152\n2\nA173\n1\nA191\nA201\n2\n\n\n496\nA12\n36\nA32\nA42\n9034\nA62\nA72\n4\nA93\nA102\n...\nA124\n29\nA143\nA151\n1\nA174\n1\nA192\nA201\n2\n\n\n539\nA13\n18\nA32\nA42\n3049\nA61\nA72\n1\nA92\nA101\n...\nA122\n45\nA142\nA152\n1\nA172\n1\nA191\nA201\n1\n\n\n549\nA14\n48\nA34\nA41\n8858\nA65\nA74\n2\nA93\nA101\n...\nA124\n35\nA143\nA153\n2\nA173\n1\nA192\nA201\n1\n\n\n551\nA14\n6\nA31\nA43\n1750\nA63\nA75\n2\nA93\nA101\n...\nA122\n45\nA141\nA152\n1\nA172\n2\nA191\nA201\n1\n\n\n595\nA12\n6\nA31\nA40\n931\nA62\nA72\n1\nA92\nA101\n...\nA122\n32\nA142\nA152\n1\nA172\n1\nA191\nA201\n2\n\n\n602\nA12\n24\nA31\nA46\n1837\nA61\nA74\n4\nA92\nA101\n...\nA124\n34\nA141\nA153\n1\nA172\n1\nA191\nA201\n2\n\n\n607\nA12\n36\nA32\nA43\n2671\nA62\nA73\n4\nA92\nA102\n...\nA124\n50\nA143\nA153\n1\nA173\n1\nA191\nA201\n2\n\n\n611\nA13\n10\nA32\nA40\n1240\nA62\nA75\n1\nA92\nA101\n...\nA124\n48\nA143\nA153\n1\nA172\n2\nA191\nA201\n2\n\n\n613\nA11\n24\nA31\nA41\n3632\nA61\nA73\n1\nA92\nA103\n...\nA123\n22\nA141\nA151\n1\nA173\n1\nA191\nA202\n1\n\n\n616\nA12\n60\nA33\nA43\n9157\nA65\nA73\n2\nA93\nA101\n...\nA124\n27\nA143\nA153\n1\nA174\n1\nA191\nA201\n1\n\n\n663\nA12\n6\nA33\nA42\n1050\nA61\nA71\n4\nA93\nA101\n...\nA122\n35\nA142\nA152\n2\nA174\n1\nA192\nA201\n1\n\n\n666\nA12\n30\nA31\nA42\n3496\nA64\nA73\n4\nA93\nA101\n...\nA123\n34\nA142\nA152\n1\nA173\n2\nA192\nA201\n1\n\n\n667\nA14\n48\nA31\nA49\n3609\nA61\nA73\n1\nA92\nA101\n...\nA121\n27\nA142\nA152\n1\nA173\n1\nA191\nA201\n1\n\n\n684\nA12\n36\nA33\nA49\n9857\nA62\nA74\n1\nA93\nA101\n...\nA122\n31\nA143\nA152\n2\nA172\n2\nA192\nA201\n1\n\n\n703\nA12\n30\nA33\nA49\n2503\nA62\nA75\n4\nA93\nA101\n...\nA122\n41\nA142\nA152\n2\nA173\n1\nA191\nA201\n1\n\n\n721\nA12\n6\nA31\nA46\n433\nA64\nA72\n4\nA92\nA101\n...\nA122\n24\nA141\nA151\n1\nA173\n2\nA191\nA201\n2\n\n\n754\nA14\n12\nA33\nA45\n1555\nA64\nA75\n4\nA93\nA101\n...\nA124\n55\nA143\nA153\n2\nA173\n2\nA191\nA201\n2\n\n\n774\nA13\n12\nA34\nA40\n1480\nA63\nA71\n2\nA93\nA101\n...\nA124\n66\nA141\nA153\n3\nA171\n1\nA191\nA201\n1\n\n\n808\nA12\n42\nA31\nA41\n9283\nA61\nA71\n1\nA93\nA101\n...\nA124\n55\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n829\nA12\n48\nA33\nA49\n6681\nA65\nA73\n4\nA93\nA101\n...\nA124\n38\nA143\nA153\n1\nA173\n2\nA192\nA201\n1\n\n\n876\nA11\n18\nA31\nA43\n1940\nA61\nA72\n3\nA93\nA102\n...\nA124\n36\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n889\nA14\n28\nA31\nA41\n7824\nA65\nA72\n3\nA93\nA103\n...\nA121\n40\nA141\nA151\n2\nA173\n2\nA192\nA201\n1\n\n\n895\nA14\n36\nA33\nA41\n8947\nA65\nA74\n3\nA93\nA101\n...\nA123\n31\nA142\nA152\n1\nA174\n2\nA192\nA201\n1\n\n\n915\nA12\n48\nA30\nA410\n18424\nA61\nA73\n1\nA92\nA101\n...\nA122\n32\nA141\nA152\n1\nA174\n1\nA192\nA202\n2\n\n\n927\nA11\n48\nA32\nA41\n10297\nA61\nA74\n4\nA93\nA101\n...\nA124\n39\nA142\nA153\n3\nA173\n2\nA192\nA201\n2\n\n\n935\nA12\n30\nA33\nA43\n1919\nA62\nA72\n4\nA93\nA101\n...\nA124\n30\nA142\nA152\n2\nA174\n1\nA191\nA201\n2\n\n\n\n\n50 rows × 21 columns\n\n\n\nỞ trên đã sử dụng tỷ lệ 5% mẫu để xác định anomaly. Ngoài ra có thể điều chỉnh ngưỡng anomaly scores phù hợp thay vì đưa ra tỷ lệ.\n\n# Identify anomalies based on anomaly scores (you can set a threshold)\nthreshold = -0.01  # Adjust the threshold as needed\nanomalies = X[anomaly_scores &lt; threshold]\nanomalies\n\n\n\n\n\n\n\n\nduration\ncredit_amount\ninstallment_rate\nresidence_since\nage\nexisting_credits\npeople_liable\nexisting_checking_A12\nexisting_checking_A13\nexisting_checking_A14\n...\nproperty_A124\nother_installment_plans_A142\nother_installment_plans_A143\nhousing_A152\nhousing_A153\njob_A172\njob_A173\njob_A174\ntelephone_A192\nforeign_worker_A202\n\n\n\n\n44\n48\n6143\n4\n4\n58\n2\n1\n0\n0\n0\n...\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n55\n6\n783\n1\n2\n26\n1\n2\n0\n0\n1\n...\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n87\n36\n12612\n1\n4\n47\n1\n2\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n99\n20\n7057\n3\n4\n36\n2\n2\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n175\n30\n7485\n4\n1\n53\n1\n1\n0\n0\n1\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n186\n9\n5129\n2\n4\n74\n1\n2\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n191\n48\n3844\n4\n4\n34\n1\n2\n1\n0\n0\n...\n1\n0\n1\n0\n1\n1\n0\n0\n0\n0\n\n\n263\n12\n2748\n2\n4\n57\n3\n1\n0\n0\n1\n...\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n272\n48\n12169\n4\n4\n36\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n287\n48\n7582\n2\n4\n31\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n304\n48\n10127\n2\n2\n44\n1\n1\n0\n0\n1\n...\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n\n\n306\n30\n4811\n2\n4\n24\n1\n1\n0\n0\n1\n...\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n374\n60\n14782\n3\n4\n60\n2\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n449\n15\n1512\n3\n3\n61\n2\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n496\n36\n9034\n4\n1\n29\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n\n\n549\n48\n8858\n2\n1\n35\n2\n1\n0\n0\n1\n...\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n595\n6\n931\n1\n1\n32\n1\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n602\n24\n1837\n4\n4\n34\n1\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n613\n24\n3632\n1\n4\n22\n1\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n616\n60\n9157\n2\n2\n27\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n0\n0\n\n\n663\n6\n1050\n4\n1\n35\n2\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n667\n48\n3609\n1\n1\n27\n1\n1\n0\n0\n1\n...\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n754\n12\n1555\n4\n4\n55\n2\n2\n0\n0\n1\n...\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n\n\n808\n42\n9283\n1\n2\n55\n1\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n889\n28\n7824\n3\n4\n40\n2\n2\n0\n0\n1\n...\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n895\n36\n8947\n3\n2\n31\n1\n2\n0\n0\n1\n...\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n915\n48\n18424\n1\n2\n32\n1\n1\n1\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n\n\n927\n48\n10297\n4\n4\n39\n3\n2\n0\n0\n0\n...\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n\n\n935\n30\n1919\n4\n3\n30\n2\n1\n1\n0\n0\n...\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n\n\n29 rows × 48 columns",
    "crumbs": [
      "Machine learning",
      "Isolation Forest - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/IsolationForest.html#sử-dụng-thuật-toán-isolation-forest-cho-việc-phát-hiện-dấu-hiệu-bất-thường-anomaly-detection",
    "href": "machine-learning/IsolationForest.html#sử-dụng-thuật-toán-isolation-forest-cho-việc-phát-hiện-dấu-hiệu-bất-thường-anomaly-detection",
    "title": "Isolation Forest - anomaly detection",
    "section": "",
    "text": "Bước 1: Chuẩn bị Dữ liệu - Bắt đầu bằng việc thu thập và tải dữ liệu mà bạn muốn phát hiện các dấu hiệu bất thường.\nBước 2: Tải và Import Thư viện - Trong Python, sử dụng các thư viện sau: numpy, pandas, và sklearn.ensemble.\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nBước 3: Chuẩn bị dữ liệu - Đảm bảo rằng dữ liệu của bạn đã được chuẩn bị và lấy ra từ các cột không mong muốn.\nBước 4: Tạo Mô hình Isolation Forest - Sử dụng IsolationForest để tạo một mô hình phát hiện dấu hiệu bất thường. Bạn có thể tuỳ chỉnh mô hình bằng cách thay đổi các tham số, chẳng hạn như contamination (tỷ lệ dấu hiệu bất thường trong dữ liệu).\nmodel = IsolationForest(contamination=0.05)  # Tuỳ chỉnh contamination tại đây\nBước 5: Huấn luyện Mô hình - Sử dụng dữ liệu huấn luyện của bạn để huấn luyện mô hình Isolation Forest.\nmodel.fit(X)\nBước 6: Dự đoán Dấu hiệu Bất thường - Sử dụng mô hình đã huấn luyện để dự đoán dấu hiệu bất thường trên tập dữ liệu kiểm tra.\npredictions = model.predict(X)\nBước 7: Lấy Điểm Anomaly - Sử dụng phương thức decision_function để lấy điểm anomaly cho mỗi mẫu dữ liệu. Điểm càng thấp càng có khả năng là dấu hiệu bất thường.\nanomaly_scores = model.decision_function(X)\nBước 8: Xác định Dấu hiệu Bất thường - Dùng ngưỡng (threshold) để xác định dấu hiệu bất thường dựa trên điểm anomaly. Bạn có thể tuỳ chỉnh ngưỡng theo ý muốn.\nthreshold = -0.1  # Ngưỡng ngầm định, điều chỉnh tùy theo nhu cầu\nanomalies = X[anomaly_scores &lt; threshold]\nBước 9: Xuất kết quả - Xuất các mẫu bất thường mà bạn đã xác định từ dữ liệu.\nprint(\"Dấu hiệu bất thường:\")\nprint(anomalies)\nLưu ý rằng bạn có thể tùy chỉnh các tham số và ngưỡng để điều chỉnh độ nhạy của việc phát hiện dấu hiệu bất thường dựa trên nhu cầu của bạn.\n\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\n\n\n# Create an Isolation Forest model\ncontamination = 0.05\nmodel = IsolationForest(contamination=contamination)\n\n# Fit the model to your data\nmodel.fit(X)\n\n# Predict anomalies (1 for inliers, -1 for outliers)\npredictions = model.predict(X)\n\n# Get the anomaly scores (the lower the score, the more likely it's an anomaly)\nanomaly_scores = model.decision_function(X)\n\n# Identify anomalies\nanomalies = data[predictions == -1]\n\nc:\\Users\\binhnn2\\anaconda3\\envs\\env_pycaret\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n  warnings.warn(\n\n\n\nanomalies\n\n\n\n\n\n\n\n\nexisting_checking\nduration\ncredit_history\npurpose\ncredit_amount\nsavings\nemployment\ninstallment_rate\npersonal_status\nother_debtors\n...\nproperty\nage\nother_installment_plans\nhousing\nexisting_credits\njob\npeople_liable\ntelephone\nforeign_worker\nclass\n\n\n\n\n44\nA11\n48\nA34\nA41\n6143\nA61\nA75\n4\nA92\nA101\n...\nA124\n58\nA142\nA153\n2\nA172\n1\nA191\nA201\n2\n\n\n55\nA14\n6\nA31\nA40\n783\nA65\nA73\n1\nA93\nA103\n...\nA121\n26\nA142\nA152\n1\nA172\n2\nA191\nA201\n1\n\n\n87\nA12\n36\nA32\nA46\n12612\nA62\nA73\n1\nA93\nA101\n...\nA124\n47\nA143\nA153\n1\nA173\n2\nA192\nA201\n2\n\n\n99\nA12\n20\nA33\nA41\n7057\nA65\nA74\n3\nA93\nA101\n...\nA122\n36\nA141\nA151\n2\nA174\n2\nA192\nA201\n1\n\n\n105\nA12\n24\nA34\nA410\n11938\nA61\nA73\n2\nA93\nA102\n...\nA123\n39\nA143\nA152\n2\nA174\n2\nA192\nA201\n2\n\n\n110\nA12\n6\nA33\nA49\n1449\nA62\nA75\n1\nA91\nA101\n...\nA123\n31\nA141\nA152\n2\nA173\n2\nA191\nA201\n1\n\n\n154\nA12\n24\nA33\nA49\n6967\nA62\nA74\n4\nA93\nA101\n...\nA123\n36\nA143\nA151\n1\nA174\n1\nA192\nA201\n1\n\n\n157\nA11\n12\nA31\nA48\n339\nA61\nA75\n4\nA94\nA101\n...\nA123\n45\nA141\nA152\n1\nA172\n1\nA191\nA201\n1\n\n\n175\nA14\n30\nA31\nA41\n7485\nA65\nA71\n4\nA92\nA101\n...\nA121\n53\nA141\nA152\n1\nA174\n1\nA192\nA201\n2\n\n\n181\nA12\n36\nA33\nA49\n4455\nA61\nA73\n2\nA91\nA101\n...\nA121\n30\nA142\nA152\n2\nA174\n1\nA192\nA201\n2\n\n\n186\nA12\n9\nA31\nA41\n5129\nA61\nA75\n2\nA92\nA101\n...\nA124\n74\nA141\nA153\n1\nA174\n2\nA192\nA201\n2\n\n\n191\nA12\n48\nA30\nA49\n3844\nA62\nA74\n4\nA93\nA101\n...\nA124\n34\nA143\nA153\n1\nA172\n2\nA191\nA201\n2\n\n\n226\nA12\n48\nA32\nA43\n10961\nA64\nA74\n1\nA93\nA102\n...\nA124\n27\nA141\nA152\n2\nA173\n1\nA192\nA201\n2\n\n\n263\nA14\n12\nA34\nA46\n2748\nA61\nA75\n2\nA92\nA101\n...\nA124\n57\nA141\nA153\n3\nA172\n1\nA191\nA201\n1\n\n\n272\nA12\n48\nA31\nA40\n12169\nA65\nA71\n4\nA93\nA102\n...\nA124\n36\nA143\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n287\nA12\n48\nA33\nA410\n7582\nA62\nA71\n2\nA93\nA101\n...\nA124\n31\nA143\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n304\nA14\n48\nA34\nA40\n10127\nA63\nA73\n2\nA93\nA101\n...\nA124\n44\nA141\nA153\n1\nA173\n1\nA191\nA201\n2\n\n\n306\nA14\n30\nA32\nA41\n4811\nA65\nA74\n2\nA92\nA101\n...\nA122\n24\nA142\nA151\n1\nA172\n1\nA191\nA201\n1\n\n\n333\nA14\n48\nA34\nA41\n11590\nA62\nA73\n2\nA92\nA101\n...\nA123\n24\nA141\nA151\n2\nA172\n1\nA191\nA201\n2\n\n\n343\nA12\n18\nA32\nA49\n4439\nA61\nA75\n1\nA93\nA102\n...\nA121\n33\nA141\nA152\n1\nA174\n1\nA192\nA201\n1\n\n\n373\nA14\n60\nA34\nA40\n13756\nA65\nA75\n2\nA93\nA101\n...\nA124\n63\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n374\nA12\n60\nA31\nA410\n14782\nA62\nA75\n3\nA92\nA101\n...\nA124\n60\nA141\nA153\n2\nA174\n1\nA192\nA201\n2\n\n\n395\nA12\n39\nA33\nA46\n11760\nA62\nA74\n2\nA93\nA101\n...\nA124\n32\nA143\nA151\n1\nA173\n1\nA192\nA201\n1\n\n\n449\nA12\n15\nA33\nA45\n1512\nA64\nA73\n3\nA94\nA101\n...\nA122\n61\nA142\nA152\n2\nA173\n1\nA191\nA201\n2\n\n\n496\nA12\n36\nA32\nA42\n9034\nA62\nA72\n4\nA93\nA102\n...\nA124\n29\nA143\nA151\n1\nA174\n1\nA192\nA201\n2\n\n\n539\nA13\n18\nA32\nA42\n3049\nA61\nA72\n1\nA92\nA101\n...\nA122\n45\nA142\nA152\n1\nA172\n1\nA191\nA201\n1\n\n\n549\nA14\n48\nA34\nA41\n8858\nA65\nA74\n2\nA93\nA101\n...\nA124\n35\nA143\nA153\n2\nA173\n1\nA192\nA201\n1\n\n\n551\nA14\n6\nA31\nA43\n1750\nA63\nA75\n2\nA93\nA101\n...\nA122\n45\nA141\nA152\n1\nA172\n2\nA191\nA201\n1\n\n\n595\nA12\n6\nA31\nA40\n931\nA62\nA72\n1\nA92\nA101\n...\nA122\n32\nA142\nA152\n1\nA172\n1\nA191\nA201\n2\n\n\n602\nA12\n24\nA31\nA46\n1837\nA61\nA74\n4\nA92\nA101\n...\nA124\n34\nA141\nA153\n1\nA172\n1\nA191\nA201\n2\n\n\n607\nA12\n36\nA32\nA43\n2671\nA62\nA73\n4\nA92\nA102\n...\nA124\n50\nA143\nA153\n1\nA173\n1\nA191\nA201\n2\n\n\n611\nA13\n10\nA32\nA40\n1240\nA62\nA75\n1\nA92\nA101\n...\nA124\n48\nA143\nA153\n1\nA172\n2\nA191\nA201\n2\n\n\n613\nA11\n24\nA31\nA41\n3632\nA61\nA73\n1\nA92\nA103\n...\nA123\n22\nA141\nA151\n1\nA173\n1\nA191\nA202\n1\n\n\n616\nA12\n60\nA33\nA43\n9157\nA65\nA73\n2\nA93\nA101\n...\nA124\n27\nA143\nA153\n1\nA174\n1\nA191\nA201\n1\n\n\n663\nA12\n6\nA33\nA42\n1050\nA61\nA71\n4\nA93\nA101\n...\nA122\n35\nA142\nA152\n2\nA174\n1\nA192\nA201\n1\n\n\n666\nA12\n30\nA31\nA42\n3496\nA64\nA73\n4\nA93\nA101\n...\nA123\n34\nA142\nA152\n1\nA173\n2\nA192\nA201\n1\n\n\n667\nA14\n48\nA31\nA49\n3609\nA61\nA73\n1\nA92\nA101\n...\nA121\n27\nA142\nA152\n1\nA173\n1\nA191\nA201\n1\n\n\n684\nA12\n36\nA33\nA49\n9857\nA62\nA74\n1\nA93\nA101\n...\nA122\n31\nA143\nA152\n2\nA172\n2\nA192\nA201\n1\n\n\n703\nA12\n30\nA33\nA49\n2503\nA62\nA75\n4\nA93\nA101\n...\nA122\n41\nA142\nA152\n2\nA173\n1\nA191\nA201\n1\n\n\n721\nA12\n6\nA31\nA46\n433\nA64\nA72\n4\nA92\nA101\n...\nA122\n24\nA141\nA151\n1\nA173\n2\nA191\nA201\n2\n\n\n754\nA14\n12\nA33\nA45\n1555\nA64\nA75\n4\nA93\nA101\n...\nA124\n55\nA143\nA153\n2\nA173\n2\nA191\nA201\n2\n\n\n774\nA13\n12\nA34\nA40\n1480\nA63\nA71\n2\nA93\nA101\n...\nA124\n66\nA141\nA153\n3\nA171\n1\nA191\nA201\n1\n\n\n808\nA12\n42\nA31\nA41\n9283\nA61\nA71\n1\nA93\nA101\n...\nA124\n55\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n829\nA12\n48\nA33\nA49\n6681\nA65\nA73\n4\nA93\nA101\n...\nA124\n38\nA143\nA153\n1\nA173\n2\nA192\nA201\n1\n\n\n876\nA11\n18\nA31\nA43\n1940\nA61\nA72\n3\nA93\nA102\n...\nA124\n36\nA141\nA153\n1\nA174\n1\nA192\nA201\n1\n\n\n889\nA14\n28\nA31\nA41\n7824\nA65\nA72\n3\nA93\nA103\n...\nA121\n40\nA141\nA151\n2\nA173\n2\nA192\nA201\n1\n\n\n895\nA14\n36\nA33\nA41\n8947\nA65\nA74\n3\nA93\nA101\n...\nA123\n31\nA142\nA152\n1\nA174\n2\nA192\nA201\n1\n\n\n915\nA12\n48\nA30\nA410\n18424\nA61\nA73\n1\nA92\nA101\n...\nA122\n32\nA141\nA152\n1\nA174\n1\nA192\nA202\n2\n\n\n927\nA11\n48\nA32\nA41\n10297\nA61\nA74\n4\nA93\nA101\n...\nA124\n39\nA142\nA153\n3\nA173\n2\nA192\nA201\n2\n\n\n935\nA12\n30\nA33\nA43\n1919\nA62\nA72\n4\nA93\nA101\n...\nA124\n30\nA142\nA152\n2\nA174\n1\nA191\nA201\n2\n\n\n\n\n50 rows × 21 columns\n\n\n\nỞ trên đã sử dụng tỷ lệ 5% mẫu để xác định anomaly. Ngoài ra có thể điều chỉnh ngưỡng anomaly scores phù hợp thay vì đưa ra tỷ lệ.\n\n# Identify anomalies based on anomaly scores (you can set a threshold)\nthreshold = -0.01  # Adjust the threshold as needed\nanomalies = X[anomaly_scores &lt; threshold]\nanomalies\n\n\n\n\n\n\n\n\nduration\ncredit_amount\ninstallment_rate\nresidence_since\nage\nexisting_credits\npeople_liable\nexisting_checking_A12\nexisting_checking_A13\nexisting_checking_A14\n...\nproperty_A124\nother_installment_plans_A142\nother_installment_plans_A143\nhousing_A152\nhousing_A153\njob_A172\njob_A173\njob_A174\ntelephone_A192\nforeign_worker_A202\n\n\n\n\n44\n48\n6143\n4\n4\n58\n2\n1\n0\n0\n0\n...\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n55\n6\n783\n1\n2\n26\n1\n2\n0\n0\n1\n...\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n87\n36\n12612\n1\n4\n47\n1\n2\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n99\n20\n7057\n3\n4\n36\n2\n2\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n175\n30\n7485\n4\n1\n53\n1\n1\n0\n0\n1\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n186\n9\n5129\n2\n4\n74\n1\n2\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n191\n48\n3844\n4\n4\n34\n1\n2\n1\n0\n0\n...\n1\n0\n1\n0\n1\n1\n0\n0\n0\n0\n\n\n263\n12\n2748\n2\n4\n57\n3\n1\n0\n0\n1\n...\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n272\n48\n12169\n4\n4\n36\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n287\n48\n7582\n2\n4\n31\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n304\n48\n10127\n2\n2\n44\n1\n1\n0\n0\n1\n...\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n\n\n306\n30\n4811\n2\n4\n24\n1\n1\n0\n0\n1\n...\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n374\n60\n14782\n3\n4\n60\n2\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n449\n15\n1512\n3\n3\n61\n2\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n496\n36\n9034\n4\n1\n29\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n\n\n549\n48\n8858\n2\n1\n35\n2\n1\n0\n0\n1\n...\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n595\n6\n931\n1\n1\n32\n1\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n602\n24\n1837\n4\n4\n34\n1\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n613\n24\n3632\n1\n4\n22\n1\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n616\n60\n9157\n2\n2\n27\n1\n1\n1\n0\n0\n...\n1\n0\n1\n0\n1\n0\n0\n1\n0\n0\n\n\n663\n6\n1050\n4\n1\n35\n2\n1\n1\n0\n0\n...\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n667\n48\n3609\n1\n1\n27\n1\n1\n0\n0\n1\n...\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n754\n12\n1555\n4\n4\n55\n2\n2\n0\n0\n1\n...\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n\n\n808\n42\n9283\n1\n2\n55\n1\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n\n\n889\n28\n7824\n3\n4\n40\n2\n2\n0\n0\n1\n...\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n895\n36\n8947\n3\n2\n31\n1\n2\n0\n0\n1\n...\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n\n\n915\n48\n18424\n1\n2\n32\n1\n1\n1\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n\n\n927\n48\n10297\n4\n4\n39\n3\n2\n0\n0\n0\n...\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n\n\n935\n30\n1919\n4\n3\n30\n2\n1\n1\n0\n0\n...\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n\n\n29 rows × 48 columns",
    "crumbs": [
      "Machine learning",
      "Isolation Forest - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/IsolationForest.html#nhược-điểm-của-isolation-forest",
    "href": "machine-learning/IsolationForest.html#nhược-điểm-của-isolation-forest",
    "title": "Isolation Forest - anomaly detection",
    "section": "Nhược điểm của Isolation Forest",
    "text": "Nhược điểm của Isolation Forest\nIsolation Forest là một thuật toán mạnh mẽ cho phát hiện dấu hiệu bất thường trong dữ liệu. Tuy nhiên, nó cũng có nhược điểm và hạn chế của riêng nó. Dưới đây là một số nhược điểm của Isolation Forest cùng với ví dụ:\nNhược Điểm:\n\nKhông hiệu quả đối với dấu hiệu bất thường gần nhau: Isolation Forest không hoạt động tốt trong việc phát hiện dấu hiệu bất thường nằm gần nhau hoặc trải dài dọc theo một đường.\nĐộ nhạy với giá trị ngưỡng: Việc đặt ngưỡng quyết định dấu hiệu bất thường có thể khá khó và phụ thuộc vào kiểu dữ liệu và phân phối của dữ liệu. Nếu bạn đặt ngưỡng quá thấp, có thể dẫn đến nhiều dấu hiệu giả mạo; ngược lại, nếu đặt ngưỡng quá cao, có thể bỏ lỡ nhiều dấu hiệu thực sự bất thường.\nKhả năng đối mặt với dữ liệu nhiều chiều: Isolation Forest có thể gặp khó khăn khi xử lý dữ liệu có số chiều rất lớn. Trong các không gian nhiều chiều, việc tạo ra các phân nhánh ngẫu nhiên có thể dẫn đến các cây con có chiều sâu quá thấp, làm giảm khả năng phát hiện dấu hiệu bất thường.\n\nVí dụ về Nhược Điểm của Isolation Forest:\nGiả sử bạn có dữ liệu về giao dịch tài chính trong một ngân hàng và bạn muốn phát hiện gian lận trong các giao dịch. Một số gian lận xảy ra với các mẫu tương tự với các giao dịch thông thường, nhưng với số lượng nhỏ và gần nhau trong không gian đặc trưng. Isolation Forest có thể không hiệu quả trong việc phát hiện các gian lận như vậy vì nó có thể đánh giá chúng là dấu hiệu thông thường do chúng không được cô lập.\nNhược điểm này có thể được khắc phục bằng cách sử dụng các kỹ thuật phát hiện gian lận cơ bản hơn và phù hợp hơn cho loại tình huống này hoặc bằng việc sử dụng các thuật toán khác như Local Outlier Factor (LOF) hoặc One-Class SVM.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(0)\n\n# Generate random data for the first blob (normal data)\nmean1 = [2, 2]\ncov1 = [[1, 0.5], [0.5, 1]]\ndata1 = np.random.multivariate_normal(mean1, cov1, 100)\n\n# Generate random data for the second blob (abnormal data)\nmean2 = [7, 7]\ncov2 = [[1, -0.5], [-0.5, 1]]\ndata2 = np.random.multivariate_normal(mean2, cov2, 100)\n\n# Combine the two sets of data\ndata = np.vstack((data1, data2))\n\n# Create a scatter plot of the data\nplt.scatter(data1[:, 0], data1[:, 1], marker='o', c='b', label='Normal Data')\nplt.scatter(data2[:, 0], data2[:, 1], marker='x', c='r', label='Abnormal Data')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title(\"Can't distinct normal and abnormal\")\nplt.legend()\n\nplt.grid()\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Isolation Forest - anomaly detection"
    ]
  },
  {
    "objectID": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html",
    "href": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html",
    "title": "Overestimate feature importance in Random Forest",
    "section": "",
    "text": "Tree-based models have a strong tendency to overestimate the importance of continuous numerical or high cardinality categorical features\nCác mô hình dựa trên cây như decision trees, random forests, and gradient boosting machines (GBMs) có thể thể hiện sự thiên vị trong việc đánh giá quá cao tầm quan trọng của các các biến continuous numerical hoặc high cardinality categorical features trong một số trường hợp nhất định. Hiện tượng này thường được gọi là “variable bias” hay “variable inflation”. Đây là lý do tại sao nó xảy ra:\nĐể giải quyết sự thiên vị hoặc đánh giá quá cao tầm quan trọng của tính năng, bạn có thể xem xét những điều sau:\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\n# Load the German Credit dataset (assuming it's available in a CSV file)\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = [\"existing_account\", \"duration_month\", \"credit_history\", \"purpose\", \"credit_amount\",\n                \"savings_account\", \"employment_since\", \"installment_rate\", \"personal_status_sex\", \"other_debtors\",\n                \"present_residence\", \"property\", \"age\", \"other_installment_plans\", \"housing\", \"existing_credits\",\n                \"job\", \"people_liable\", \"telephone\", \"foreign_worker\", \"credit_risk\"]\ndf_raw = pd.read_csv(url, sep=\" \", header=None, names=column_names)\ndf_raw.describe()\n\n\n\n\n\n\n\n\nduration_month\ncredit_amount\ninstallment_rate\npresent_residence\nage\nexisting_credits\npeople_liable\ncredit_risk\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n20.903000\n3271.258000\n2.973000\n2.845000\n35.546000\n1.407000\n1.155000\n1.300000\n\n\nstd\n12.058814\n2822.736876\n1.118715\n1.103718\n11.375469\n0.577654\n0.362086\n0.458487\n\n\nmin\n4.000000\n250.000000\n1.000000\n1.000000\n19.000000\n1.000000\n1.000000\n1.000000\n\n\n25%\n12.000000\n1365.500000\n2.000000\n2.000000\n27.000000\n1.000000\n1.000000\n1.000000\n\n\n50%\n18.000000\n2319.500000\n3.000000\n3.000000\n33.000000\n1.000000\n1.000000\n1.000000\n\n\n75%\n24.000000\n3972.250000\n4.000000\n4.000000\n42.000000\n2.000000\n1.000000\n2.000000\n\n\nmax\n72.000000\n18424.000000\n4.000000\n4.000000\n75.000000\n4.000000\n2.000000\n2.000000\ndf_raw.infer_objects()\n\n\n\n\n\n\n\n\nexisting_account\nduration_month\ncredit_history\npurpose\ncredit_amount\nsavings_account\nemployment_since\ninstallment_rate\npersonal_status_sex\nother_debtors\n...\nproperty\nage\nother_installment_plans\nhousing\nexisting_credits\njob\npeople_liable\ntelephone\nforeign_worker\ncredit_risk\n\n\n\n\n0\nA11\n6\nA34\nA43\n1169\nA65\nA75\n4\nA93\nA101\n...\nA121\n67\nA143\nA152\n2\nA173\n1\nA192\nA201\n1\n\n\n1\nA12\n48\nA32\nA43\n5951\nA61\nA73\n2\nA92\nA101\n...\nA121\n22\nA143\nA152\n1\nA173\n1\nA191\nA201\n2\n\n\n2\nA14\n12\nA34\nA46\n2096\nA61\nA74\n2\nA93\nA101\n...\nA121\n49\nA143\nA152\n1\nA172\n2\nA191\nA201\n1\n\n\n3\nA11\n42\nA32\nA42\n7882\nA61\nA74\n2\nA93\nA103\n...\nA122\n45\nA143\nA153\n1\nA173\n2\nA191\nA201\n1\n\n\n4\nA11\n24\nA33\nA40\n4870\nA61\nA73\n3\nA93\nA101\n...\nA124\n53\nA143\nA153\n2\nA173\n2\nA191\nA201\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\nA14\n12\nA32\nA42\n1736\nA61\nA74\n3\nA92\nA101\n...\nA121\n31\nA143\nA152\n1\nA172\n1\nA191\nA201\n1\n\n\n996\nA11\n30\nA32\nA41\n3857\nA61\nA73\n4\nA91\nA101\n...\nA122\n40\nA143\nA152\n1\nA174\n1\nA192\nA201\n1\n\n\n997\nA14\n12\nA32\nA43\n804\nA61\nA75\n4\nA93\nA101\n...\nA123\n38\nA143\nA152\n1\nA173\n1\nA191\nA201\n1\n\n\n998\nA11\n45\nA32\nA43\n1845\nA61\nA73\n4\nA93\nA101\n...\nA124\n23\nA143\nA153\n1\nA173\n1\nA192\nA201\n2\n\n\n999\nA12\n45\nA34\nA41\n4576\nA62\nA71\n3\nA93\nA101\n...\nA123\n27\nA143\nA152\n1\nA173\n1\nA191\nA201\n1\n\n\n\n\n1000 rows × 21 columns",
    "crumbs": [
      "Machine learning",
      "Overestimate feature importance in Random Forest"
    ]
  },
  {
    "objectID": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#lựa-chọn-dữ-liệu-tính-toán",
    "href": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#lựa-chọn-dữ-liệu-tính-toán",
    "title": "Overestimate feature importance in Random Forest",
    "section": "Lựa chọn dữ liệu tính toán",
    "text": "Lựa chọn dữ liệu tính toán\nChọn các biến có các đặc điểm khác nhau như: biến dạng double, int, char\n\ndf = df_raw[['credit_risk','credit_amount', 'age', 'duration_month', 'credit_history', 'other_installment_plans']]\n\n# Encode categorical variables (e.g., using one-hot encoding)\ndf = pd.get_dummies(df, columns=['credit_history', 'other_installment_plans'])\n\n# Define the target variable and features\nX = df.drop(columns=[\"credit_risk\"])\ny = df[\"credit_risk\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Random Forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)",
    "crumbs": [
      "Machine learning",
      "Overestimate feature importance in Random Forest"
    ]
  },
  {
    "objectID": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#tính-feature-importances-với-các-biến-đã-lựa-chọn-ban-đầu",
    "href": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#tính-feature-importances-với-các-biến-đã-lựa-chọn-ban-đầu",
    "title": "Overestimate feature importance in Random Forest",
    "section": "Tính feature importances với các biến đã lựa chọn ban đầu",
    "text": "Tính feature importances với các biến đã lựa chọn ban đầu\n\n# Compute feature importances using MDI\nmdi_feature_importances = clf.feature_importances_\n\n# Compute feature importances using permutation method\nperm_importance = permutation_importance(clf, X_test, y_test, n_repeats=30, random_state=42)\nperm_feature_importances = perm_importance.importances_mean\n\n# Create DataFrames to display feature importances\nmdi_importances_df = pd.DataFrame({\"Feature\": X.columns, \"MDI Importance\": mdi_feature_importances})\nmdi_importances_df = mdi_importances_df.sort_values(by=\"MDI Importance\", ascending=False)\n\nperm_importances_df = pd.DataFrame({\"Feature\": X.columns, \"Permutation Importance\": perm_feature_importances})\nperm_importances_df = perm_importances_df.sort_values(by=\"Permutation Importance\", ascending=False)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Create DataFrames to display feature importances from both methods...\n# (As shown in the previous code)\n\n# Create subplots for visualization\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Visualize MDI feature importances\nsns.barplot(x=\"MDI Importance\", y=\"Feature\", data=mdi_importances_df, ax=axes[0], color=\"lightblue\")\naxes[0].set_title(\"MDI Feature Importance\")\n\n# Visualize permutation feature importances\nsns.barplot(x=\"Permutation Importance\", y=\"Feature\", data=perm_importances_df, ax=axes[1], color=\"lightgreen\")\naxes[1].set_title(\"Permutation Feature Importance\")\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Overestimate feature importance in Random Forest"
    ]
  },
  {
    "objectID": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#thêm-biến-random-để-đánh-giá",
    "href": "machine-learning/FeatureImportance_Permutation_versus_MeanDecrease.html#thêm-biến-random-để-đánh-giá",
    "title": "Overestimate feature importance in Random Forest",
    "section": "Thêm biến random để đánh giá",
    "text": "Thêm biến random để đánh giá\n\nThêm 2 loại biến: 1 loại dạng CONTINUOUS variable, 1 loại DISCRETE variable để đánh giá xem biến nào importance cao hơn và so với các biến khác trong dữ liệu thì như thế nào\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\n# Load and preprocess the German Credit dataset as before...\n\n# Add two new random columns (Continuous_Random and Discrete_Random)\nimport numpy as np\nnp.random.seed(42)  # For reproducibility\n\n# Create random continuous values\ndf['CONTINUOUS_RANDOM'] = np.random.uniform(0,200, size=len(df))\n\n# Create random discrete values\ndf['DISCRETE_RANDOM'] = np.random.randint(0, 5, size=len(df))\n\n# Create random discrete values\ndf['DISCRETE_RANDOM2'] = np.random.randint(0, 10, size=len(df))\n\n# Define the target variable and features\nX = df.drop(columns=[\"credit_risk\"])\ny = df[\"credit_risk\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Random Forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Compute feature importances using MDI\nmdi_feature_importances = clf.feature_importances_\n\n# Compute feature importances using permutation method\nperm_importance = permutation_importance(clf, X_test, y_test, n_repeats=30, random_state=42)\nperm_feature_importances = perm_importance.importances_mean\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef pImportanceWithHighlight(categories, values, highlight_categories, highlight_color='red', base_color='lightblue', title='Feature Importances'):          \n    data = {'Category': categories,\n            'Values': values}\n    df = pd.DataFrame(data)   \n    \n    # Sort the DataFrame in increasing order of the 'Values' column\n    df_sorted = df.sort_values(by='Values', ascending=False)\n    df_sorted.reset_index(inplace=True, drop=True)    \n    \n     # Determine the positions (indices) of the highlighted categories\n    highlight_indices = [idx for idx, cat in enumerate(df_sorted['Category']) if cat in highlight_categories]\n    \n    # Create a list of colors for each row, setting the highlight color for the specific rows\n    colors = [base_color if idx not in highlight_indices else highlight_color for idx in range(len(df_sorted))]\n\n    # Create a bar plot with the specified colors using Seaborn\n    sns.barplot(x='Values', y='Category', data=df_sorted, palette=colors)\n\n    # Add labels and a title\n    plt.xlabel('Feature')\n    plt.ylabel('Importance')\n    plt.title(title)\n\n# Create subplots for visualization\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Visualize MDI feature importances\nplt.sca(axes[0])  # Set the current axes to the first subplot\npImportanceWithHighlight(X.columns, mdi_feature_importances, ['CONTINUOUS_RANDOM', 'DISCRETE_RANDOM', 'DISCRETE_RANDOM2'], title=\"MDI Feature Importance\")\n\n# Visualize permutation feature importances\nplt.sca(axes[1])  # Set the current axes to the second subplot\npImportanceWithHighlight(X.columns, perm_feature_importances, ['CONTINUOUS_RANDOM', 'DISCRETE_RANDOM', 'DISCRETE_RANDOM2'], title=\"Permutation Feature Importance\", base_color='lightgreen')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Machine learning",
      "Overestimate feature importance in Random Forest"
    ]
  },
  {
    "objectID": "machine-learning/StackModelShapExplainer.html",
    "href": "machine-learning/StackModelShapExplainer.html",
    "title": "StackModelShapExplainer Documentation",
    "section": "",
    "text": "StackModelShapExplainer is a specialized explainability tool designed for stacked ensemble models, providing deep insights into how predictions flow through complex multi-model architectures. It extends traditional SHAP (SHapley Additive exPlanations) analysis to work with hierarchical models where base models feed into meta-models.",
    "crumbs": [
      "Machine learning",
      "StackModelShapExplainer Documentation"
    ]
  },
  {
    "objectID": "machine-learning/StackModelShapExplainer.html#overview",
    "href": "machine-learning/StackModelShapExplainer.html#overview",
    "title": "StackModelShapExplainer Documentation",
    "section": "",
    "text": "StackModelShapExplainer is a specialized explainability tool designed for stacked ensemble models, providing deep insights into how predictions flow through complex multi-model architectures. It extends traditional SHAP (SHapley Additive exPlanations) analysis to work with hierarchical models where base models feed into meta-models.",
    "crumbs": [
      "Machine learning",
      "StackModelShapExplainer Documentation"
    ]
  },
  {
    "objectID": "machine-learning/StackModelShapExplainer.html#methodology",
    "href": "machine-learning/StackModelShapExplainer.html#methodology",
    "title": "StackModelShapExplainer Documentation",
    "section": "Methodology",
    "text": "Methodology\n\n1. Multi-level SHAP Approach\nThe StackModelShapExplainer uses a hierarchical approach to calculate feature attributions:\n\nBase Model Explanations: SHAP values are calculated for each base model independently, showing how input features influence each base model’s predictions.\nMeta-Model Explanations: SHAP values are calculated for the meta-model, showing how the outputs from base models (meta-features) influence the final prediction.\nCombined Explanations: The two levels of SHAP values are combined using chain rule principles to trace the influence of original features through the entire model stack to the final prediction.\n\n\n\n2. Influence Chain Calculation\nFor each feature in the input data, the explainer calculates an “influence chain” showing how that feature affects the final prediction:\nOriginal feature → Base model outputs → Meta-model → Final prediction\nThe full attribution calculation follows this formula:\nFor a feature x_i, its total influence on the final prediction is:\n\\[\\phi_{\\text{combined}}(x_i) = \\sum_{j} \\phi_{\\text{meta}}(f_j) \\cdot \\frac{\\partial f_j}{\\partial x_i}\\]\nWhere: - \\(\\phi_{\\text{combined}}(x_i)\\) is the combined SHAP value for feature \\(x_i\\) - \\(\\phi_{\\text{meta}}(f_j)\\) is the meta-model SHAP value for meta-feature \\(f_j\\) - \\(\\frac{\\partial f_j}{\\partial x_i}\\) represents how meta-feature \\(f_j\\) changes with feature \\(x_i\\)\n\n\n3. Background Data Sampling\nTo calculate SHAP values efficiently, the explainer uses a sampling approach:\n\nA subset of the training data is used as background data\nThis provides a reference distribution for feature values\nThe sampling is stratified to maintain class balance (for classification problems)",
    "crumbs": [
      "Machine learning",
      "StackModelShapExplainer Documentation"
    ]
  },
  {
    "objectID": "machine-learning/StackModelShapExplainer.html#user-manual",
    "href": "machine-learning/StackModelShapExplainer.html#user-manual",
    "title": "StackModelShapExplainer Documentation",
    "section": "User Manual",
    "text": "User Manual\n\nInstallation\n# Assuming you have the StackModelShapExplainer class defined\n# No additional packages beyond standard ML libraries and SHAP are required\n\n\nInitializing the Explainer\n# Create an explainer for your stacked model\nexplainer = StackModelShapExplainer(\n    stack_model=stack_model,  # Your trained stacking model\n    background_data=X_train,  # Training data for background distribution\n    n_background=100          # Number of background samples to use\n)\n\n\nBasic Usage\n# Get explanation for a single observation\nexplanation = explainer.explain_instance(\n    X_instance=new_observation,  # DataFrame with a single row\n    target_class=1               # For classification, which class to explain\n)\n\n# Generate a waterfall plot showing feature contributions\nfig, ax = explainer.plot_waterfall(\n    X_instance=new_observation,\n    target_class=1,\n    max_features=10  # Show only top 10 features\n)\n\n\nExploring Feature Influence\n# Get detailed influence information for a specific feature\ninfluence = explainer.get_feature_influence(explanation, 'feature_name')\n\nprint(f\"Total influence: {influence['combined_influence']:.6f}\")\nprint(\"\\nInfluence paths:\")\nfor meta_feature, value in influence['influence_paths'].items():\n    print(f\"  Via {meta_feature}: {value:.6f}\")\n\n\nExploring Model Influence\n# Get detailed influence information for a specific base model\nmodel_influence = explainer.get_model_influence(explanation, 'model_name')\n\nprint(f\"Total model influence: {model_influence['total_influence']}\")\nprint(\"\\nInfluence through meta-features:\")\nfor meta_feature, value in model_influence['meta_features'].items():\n    print(f\"  Via {meta_feature}: {value}\")\n\n\nVisualizing Overall Feature Importance\n# Plot overall feature importance\nfig, ax = explainer.plot_feature_importance(\n    X_data=X_test,         # Data to calculate importance over\n    target_class=1,        # Class to explain (for classification)\n    max_features=15,       # Number of features to show\n    plot_type='bar'        # 'bar' or 'violin'\n)\n\n\nVisualizing the Prediction Path\n# Visualize how a prediction flows through the model\nviz = explainer.visualize_prediction_path(\n    X_instance=new_observation,\n    target_class=1,\n    top_features=5         # Number of top features to highlight\n)",
    "crumbs": [
      "Machine learning",
      "StackModelShapExplainer Documentation"
    ]
  },
  {
    "objectID": "machine-learning/StackModelShapExplainer.html#function-reference",
    "href": "machine-learning/StackModelShapExplainer.html#function-reference",
    "title": "StackModelShapExplainer Documentation",
    "section": "Function Reference",
    "text": "Function Reference\n\nCore Functions\n\nexplain_instance(X_instance, target_class=None)\nGenerates a comprehensive explanation for a single observation, returning a dictionary with SHAP values at each level of the model.\n\n\nplot_waterfall(X_instance, target_class=None, max_features=10)\nCreates a waterfall plot showing how each feature contributes to pushing the prediction away from the baseline value.\n\n\nget_feature_influence(explanation, feature_name)\nExtracts detailed information about how a specific feature influences the final prediction through various paths in the model.\n\n\nget_model_influence(explanation, model_name)\nAnalyzes how a specific base model influences the final prediction through its output meta-features.\n\n\n\nVisualization Functions\n\nplot_feature_importance(X_data, target_class=None, max_features=15, plot_type='bar')\nCreates a bar or violin plot showing the overall importance of features across multiple observations.\n\n\nvisualize_prediction_path(X_instance, target_class=None, top_features=5)\nGenerates a visualization showing how feature values flow through the model to influence the final prediction.",
    "crumbs": [
      "Machine learning",
      "StackModelShapExplainer Documentation"
    ]
  },
  {
    "objectID": "machine-learning/StackModelShapExplainer.html#explanation-structure",
    "href": "machine-learning/StackModelShapExplainer.html#explanation-structure",
    "title": "StackModelShapExplainer Documentation",
    "section": "Explanation Structure",
    "text": "Explanation Structure\nThe explain_instance() function returns a dictionary with the following components:\n{\n    'combined_shap': {\n        'feature1': 0.23,  # Combined influence of feature1 on final prediction\n        'feature2': -0.11, # etc.\n        ...\n    },\n    'feature_influence_chain': {\n        'feature1': {\n            'meta_feature1': 0.15,  # How feature1 influences through meta_feature1\n            'meta_feature2': 0.08,  # How feature1 influences through meta_feature2\n            ...\n        },\n        ...\n    },\n    'meta_model_shap': array(...),  # SHAP values for meta-model\n    'base_model_shap': {\n        'model1': {\n            'shap_values': array(...),  # SHAP values for base model\n            'feature_names': [...]      # Features used by this model\n        },\n        ...\n    },\n    'meta_model_expected_value': 0.5,  # Baseline prediction of meta-model\n    'prediction': 0.73  # Final prediction value\n}",
    "crumbs": [
      "Machine learning",
      "StackModelShapExplainer Documentation"
    ]
  },
  {
    "objectID": "machine-learning/StackModelShapExplainer.html#example-workflow",
    "href": "machine-learning/StackModelShapExplainer.html#example-workflow",
    "title": "StackModelShapExplainer Documentation",
    "section": "Example Workflow",
    "text": "Example Workflow\n\nTrain a stacked ensemble model using your preferred library\nInitialize the explainer with your trained model and background data\nGenerate explanations for observations of interest\nVisualize the results using the provided plotting functions\nDrill down into specific features or models using the get_feature_influence and get_model_influence functions",
    "crumbs": [
      "Machine learning",
      "StackModelShapExplainer Documentation"
    ]
  },
  {
    "objectID": "machine-learning/StackModelShapExplainer.html#best-practices",
    "href": "machine-learning/StackModelShapExplainer.html#best-practices",
    "title": "StackModelShapExplainer Documentation",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse a representative background dataset - The quality of SHAP explanations depends on having a good baseline distribution\nConsider computational resources - SHAP analysis can be computationally intensive, so adjust n_background accordingly\nFocus on important features - Use max_features parameters to focus on the most influential variables\nCompare across instances - Look at explanations for multiple observations to understand model behavior\nCombine with other techniques - Use in conjunction with partial dependence plots and other explanatory techniques",
    "crumbs": [
      "Machine learning",
      "StackModelShapExplainer Documentation"
    ]
  },
  {
    "objectID": "machine-learning/StackModelShapExplainer.html#limitations",
    "href": "machine-learning/StackModelShapExplainer.html#limitations",
    "title": "StackModelShapExplainer Documentation",
    "section": "Limitations",
    "text": "Limitations\n\nApproximation - The explainer uses approximations for efficiency, which may slightly reduce accuracy\nComputational Cost - Full SHAP analysis can be slow for very large models or datasets\nLinear Combination Assumption - The influence chain calculation assumes effects combine linearly\nFeature Independence - Standard SHAP assumes feature independence, which may not hold in all datasets\n\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import cross_val_predict\n\nclass StackModel:\n    def __init__(self, model_configs, final_estimator, cv=None):\n        \"\"\"\n        Initialize the stacking model.\n\n        Args:\n            model_configs (dict): Configuration for base models. Each key is a model name, and \n                the value is a dictionary with the following keys:\n                - 'feature_names': List of feature names used by the model.\n                - 'estimators': The model class (e.g., sklearn classifier or regressor).\n                - 'hyperparameters': Dictionary of hyperparameters for the estimator.\n            final_estimator (BaseEstimator): Meta-model for stacking.\n            cv (int, cross-validation generator, or None): Cross-validation strategy for \n                generating meta-features. Default is None (5-fold CV).\n        \"\"\"\n        self.model_configs = model_configs\n        self.final_estimator = final_estimator\n        self.cv = cv or 5\n        self.models = {}\n        self.classes_ = None  \n        self.feature_names_ = None  \n\n    def fit(self, X, y):\n        \"\"\"\n        Train the stacking model.\n\n        Args:\n            X (pd.DataFrame): Feature matrix.\n            y (pd.Series or np.ndarray): Target vector.\n        \"\"\"\n                \n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"X must be a pandas DataFrame.\")\n        if not isinstance(y, (pd.Series, np.ndarray)):\n            raise ValueError(\"y must be a pandas Series or a numpy array.\")\n\n        # Store class labels\n        if hasattr(self.final_estimator, \"classes_\") or hasattr(self.final_estimator, \"predict_proba\"):\n            self.classes_ = np.unique(y)\n\n        self.models = {}\n        meta_features_list = []\n        meta_feature_names = []\n\n        # Train base models and generate cross-validated meta-features\n        for model_name, config in self.model_configs.items():\n            features = config['feature_names']\n            if config['estimators'] is not None:\n                estimator = config['estimators'](**config['hyperparameters'])\n                \n                # Generate cross-validated meta-features\n                meta_features = cross_val_predict(\n                    estimator, X[features], y, cv=self.cv, method='predict_proba'\n                )\n                meta_features_list.append(meta_features)\n                meta_feature_names.extend(\n                    [f\"{model_name}_class{i}\" for i in range(meta_features.shape[1])]\n                )\n\n                # Train the model on the full dataset\n                estimator.fit(X[features], y)\n\n                self.models[model_name] = {\n                    'features': features,\n                    'model': estimator\n                }\n            else:\n                # Use raw features directly for models without estimators\n                meta_features = X[features].values\n                meta_features_list.append(meta_features)\n                meta_feature_names.extend(features)\n\n                self.models[model_name] = {\n                    'features': features,\n                    'model': None\n                }\n\n        # Combine all meta-features\n        self.meta_features = np.hstack(meta_features_list)\n\n        # Store meta-feature names\n        self.feature_name_ = meta_feature_names\n\n        # Train the final estimator using meta-features\n        self.final_estimator.fit(self.meta_features, y)\n\n    def predict(self, X, raw_score=False, pred_contrib=False):\n        \"\"\"\n        Predict class labels using the stacking model.\n\n        Args:\n            X (pd.DataFrame): Feature matrix.\n            raw_score (bool): Whether to return raw scores (decision function output).\n        \n        Returns:\n            np.ndarray: Predicted class labels or decision function output.\n        \"\"\"\n        meta_features = self.transform(X)\n        \n        # Support raw_score only if the final estimator is LightGBM\n        if raw_score:\n            if isinstance(self.final_estimator, lgb.LGBMClassifier):\n                return self.final_estimator.predict(X=meta_features, raw_score=True)  # Return raw score for LGBM\n            else:\n                raise TypeError(\"The 'raw_score' option is only supported for LGBMClassifier.\")\n        \n        # Support pred_contrib only if the final estimator is LightGBM\n        if pred_contrib:\n            if isinstance(self.final_estimator, lgb.LGBMClassifier):\n                # Return SHAP-like feature contributions (pred_contrib) for LGBMClassifier\n                return self.final_estimator.predict(X=meta_features, pred_contrib=True)\n            else:\n                raise TypeError(\"The 'pred_contrib' option is only supported for LGBMClassifier.\")\n\n\n        return self.final_estimator.predict(meta_features)\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict probabilities using the stacking model.\n\n        Args:\n            X (pd.DataFrame): Feature matrix.\n\n        Returns:\n            np.ndarray: Predicted probabilities.\n        \"\"\"\n        meta_features = self.transform(X)\n        return self.final_estimator.predict_proba(meta_features)\n\n    def transform(self, X):\n        \"\"\"\n        Generate meta-features for a given dataset, transforming the input features.\n\n        Args:\n            X (pd.DataFrame): Feature matrix.\n\n        Returns:\n            np.ndarray: Transformed meta-features as a numpy array.\n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"X must be a pandas DataFrame.\")\n\n        meta_features = []\n\n        for model_name, model_info in self.models.items():\n            features = model_info['features']\n            if model_info['model'] is not None:\n                model = model_info['model']\n                meta_features.append(model.predict_proba(X[features]))\n            else:\n                meta_features.append(X[features].values)\n\n        return np.hstack(meta_features)\n\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import cross_val_predict\nimport shap\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom matplotlib.patches import Patch\nimport seaborn as sns\n\nclass StackModelShapExplainer:\n    \"\"\"\n    Class to explain predictions from a StackModel using SHAP values.\n    This separate class handles the explanation of predictions without modifying the original model.\n    \"\"\"\n    \n    def __init__(self, stack_model, background_data=None, n_background=100):\n        \"\"\"\n        Initialize the SHAP explainer for a StackModel.\n        \n        Args:\n            stack_model (StackModel): The trained stacking model to explain\n            background_data (pd.DataFrame, optional): Background data for SHAP explainers\n            n_background (int): Number of background samples to use if background_data is large\n        \"\"\"\n        self.stack_model = stack_model\n        \n        # Create background data for SHAP explainers\n        if background_data is None:\n            raise ValueError(\"Background data must be provided for SHAP explainers\")\n        \n        if len(background_data) &gt; n_background:\n            self.background_data = background_data.sample(n_background, random_state=42)\n        else:\n            self.background_data = background_data\n            \n        # Initialize explainers for each model\n        self.explainers = {}\n        self._init_base_explainers()\n        \n        # Initialize explainer for meta-model\n        self.meta_background_data = self.stack_model.transform(self.background_data)\n        self._init_meta_explainer()\n\n        # Store expected values for each explainer\n        self.expected_value = self._get_expected_values()\n    \n    def _init_base_explainers(self):\n        \"\"\"Initialize SHAP explainers for each base model\"\"\"\n        for model_name, model_info in self.stack_model.models.items():\n            if model_info['model'] is not None:\n                features = model_info['features']\n                model = model_info['model']\n                \n                # Sample background data for this model\n                bg_data = self.background_data[features].values\n                \n                # Create appropriate explainer based on model type\n                if isinstance(model, lgb.LGBMClassifier):\n                    self.explainers[model_name] = shap.TreeExplainer(model, bg_data)\n                elif hasattr(model, \"predict_proba\"):\n                    # For models with predict_proba but not LightGBM\n                    self.explainers[model_name] = shap.KernelExplainer(\n                        model.predict_proba, bg_data\n                    )\n                else:\n                    # For other models\n                    self.explainers[model_name] = shap.KernelExplainer(\n                        model.predict, bg_data\n                    )\n    \n    def _init_meta_explainer(self):\n        \"\"\"Initialize SHAP explainer for the meta-model\"\"\"\n        if isinstance(self.stack_model.final_estimator, lgb.LGBMClassifier):\n            self.meta_explainer = shap.TreeExplainer(\n                self.stack_model.final_estimator, self.meta_background_data\n            )\n        elif hasattr(self.stack_model.final_estimator, \"predict_proba\"):\n            self.meta_explainer = shap.KernelExplainer(\n                self.stack_model.final_estimator.predict_proba, self.meta_background_data\n            )\n        else:\n            self.meta_explainer = shap.KernelExplainer(\n                self.stack_model.final_estimator.predict, self.meta_background_data\n            )\n    \n    def _get_expected_values(self):\n        \"\"\"\n        Get expected values for all explainers (base models and meta model).\n        \n        Returns:\n            dict: Dictionary with expected values for each model\n        \"\"\"\n        expected_values = {}\n        \n        # Get expected values for base models\n        for model_name, explainer in self.explainers.items():\n            if hasattr(explainer, 'expected_value'):\n                expected_values[model_name] = explainer.expected_value\n        \n        # Get expected value for meta model\n        if hasattr(self.meta_explainer, 'expected_value'):\n            expected_values['meta_model'] = self.meta_explainer.expected_value\n            \n        return expected_values\n\n    def explain_instance(self, X_instance, target_class=None):\n        \"\"\"\n        Explain predictions for a single instance using SHAP values throughout the pipeline.\n        \n        Args:\n            X_instance (pd.DataFrame): Single row dataframe with features\n            target_class (int, optional): Class index to explain (for multi-class problems)\n            \n        Returns:\n            dict: Dictionary with SHAP explanation information\n        \"\"\"\n        if len(X_instance) != 1:\n            raise ValueError(\"X_instance must be a single instance (DataFrame with one row)\")\n            \n        # Step 1: Get SHAP values for each base model\n        base_model_shap_values = {}\n        base_meta_features = {}\n        \n        for model_name, model_info in self.stack_model.models.items():\n            features = model_info['features']\n            X_model = X_instance[features]\n            \n            if model_info['model'] is not None:\n                # Get SHAP values for this base model\n                model_shap = self.explainers[model_name].shap_values(X_model)\n                \n                # For multi-class, handle the list of arrays\n                if isinstance(model_shap, list):\n                    if target_class is not None:\n                        model_shap = model_shap[target_class]\n                    else:\n                        # If no target class specified, average across classes\n                        model_shap = np.mean(np.array(model_shap), axis=0)\n                \n                base_model_shap_values[model_name] = {\n                    'shap_values': model_shap,\n                    'feature_names': features\n                }\n                \n                # Get the output of this model (meta-features it produces)\n                base_meta_features[model_name] = model_info['model'].predict_proba(X_model)\n            else:\n                # For direct features (no model), SHAP values directly affect meta-model\n                base_meta_features[model_name] = X_model.values\n        \n        # Step 2: Get SHAP values for meta-model on transformed features\n        meta_features = self.stack_model.transform(X_instance)\n        meta_model_shap = self.meta_explainer.shap_values(meta_features)\n        \n        # For multi-class, handle the list of arrays\n        if isinstance(meta_model_shap, list):\n            if target_class is not None:\n                meta_model_shap = meta_model_shap[target_class]\n            else:\n                # If no target class specified, average across classes\n                meta_model_shap = np.mean(np.array(meta_model_shap), axis=0)\n        \n        # Step 3: Apply chain rule to get impact of original features on final prediction\n        # Initialize dictionary to store combined SHAP values\n        combined_shap = {}\n        feature_influence_chain = {}\n        \n        # Current meta-feature index in the concatenated meta_features array\n        meta_feature_idx = 0\n        \n        for model_name, model_info in self.stack_model.models.items():\n            if model_info['model'] is not None:\n                # This model processes features and produces meta-features\n                model_contrib = base_model_shap_values[model_name]\n                orig_features = model_info['features']\n                n_classes = base_meta_features[model_name].shape[1]\n                \n                # For each original feature in this model\n                for i, feature in enumerate(orig_features):\n                    # Initialize if not already present\n                    if feature not in combined_shap:\n                        combined_shap[feature] = 0\n                        feature_influence_chain[feature] = {}\n                    \n                    # For each class output from this model\n                    for class_idx in range(n_classes):\n                        # Get current meta-feature name\n                        meta_feat_name = f\"{model_name}_class{class_idx}\"\n                        \n                        # Get SHAP value of this meta-feature in the meta-model\n                        meta_shap_val = meta_model_shap[0, meta_feature_idx + class_idx]\n                        \n                        # Get influence of original feature on this model's output for this class\n                        # For multi-class base models\n                        if isinstance(model_contrib['shap_values'], list):\n                            base_shap_val = model_contrib['shap_values'][class_idx][0, i]\n                        else:\n                            base_shap_val = model_contrib['shap_values'][0, i]\n                        \n                        # Chain rule: multiply SHAP values\n                        chain_effect = base_shap_val * meta_shap_val\n                        combined_shap[feature] += chain_effect\n                        \n                        # Store the chain of influence for detailed explanation\n                        if meta_feat_name not in feature_influence_chain[feature]:\n                            feature_influence_chain[feature][meta_feat_name] = chain_effect\n                \n                # Update meta_feature index\n                meta_feature_idx += n_classes\n            else:\n                # For direct features, get their impact directly from meta-model SHAP values\n                orig_features = model_info['features']\n                for i, feature in enumerate(orig_features):\n                    combined_shap[feature] = meta_model_shap[0, meta_feature_idx + i]\n                    feature_influence_chain[feature] = {\n                        feature: combined_shap[feature]  # Direct influence\n                    }\n                \n                # Update meta_feature index\n                meta_feature_idx += len(orig_features)\n        \n        return {\n            'combined_shap': combined_shap,\n            'feature_influence_chain': feature_influence_chain,\n            'meta_model_shap': meta_model_shap,\n            'base_model_shap': base_model_shap_values,\n            'meta_features': meta_features,\n            'expected_value': self.expected_value\n        }\n    \n    def plot_feature_importance_old(self, X, target_class=None, top_n=10):\n        \"\"\"\n        Plot overall feature importance across the entire stacking model.\n        \n        Args:\n            X (pd.DataFrame): Feature matrix to compute importances\n            target_class (int, optional): Class to explain for multi-class problems\n            top_n (int): Number of top features to display\n        \"\"\"\n        # Compute SHAP values for a sample of instances\n        if len(X) &gt; 100:\n            X_sample = X.sample(100, random_state=42)\n        else:\n            X_sample = X\n            \n        all_shap_values = {}\n        \n        # For each sample, compute combined SHAP values\n        for i in range(len(X_sample)):\n            instance = X_sample.iloc[[i]]\n            explanation = self.explain_instance(instance, target_class)\n            \n            # Aggregate SHAP values across samples\n            for feature, shap_val in explanation['combined_shap'].items():\n                if feature not in all_shap_values:\n                    all_shap_values[feature] = []\n                all_shap_values[feature].append(shap_val)\n        \n        # Compute mean absolute SHAP value for each feature\n        mean_abs_shap = {feat: np.mean(np.abs(vals)) for feat, vals in all_shap_values.items()}\n        \n        # Sort features by importance\n        sorted_features = sorted(mean_abs_shap.items(), key=lambda x: x[1], reverse=True)\n        \n        # Get top N features\n        top_features = sorted_features[:top_n]\n        \n        # Create plot\n        plt.figure(figsize=(10, 6))\n        features, values = zip(*top_features)\n        plt.barh(range(len(features)), values, align='center')\n        plt.yticks(range(len(features)), features)\n        plt.xlabel('Mean |SHAP Value|')\n        plt.title(f'Top {top_n} Features (Stacking Model Pipeline)')\n        plt.tight_layout()\n        plt.show()\n        \n        return mean_abs_shap\n    \n    def plot_feature_importance(self, X, target_class=None, top_n=10):\n        \"\"\"\n        Plot overall feature importance across the entire stacking model.\n        \n        Args:\n            X (pd.DataFrame): Feature matrix to compute importances\n            target_class (int, optional): Class to explain for multi-class problems\n            top_n (int): Number of top features to display\n        \"\"\"\n        # Compute SHAP values for a sample of instances\n        if len(X) &gt; 100:\n            X_sample = X.sample(100, random_state=42)\n        else:\n            X_sample = X\n            \n        all_shap_values = {}\n        \n        # For each sample, compute combined SHAP values\n        for i in range(len(X_sample)):\n            instance = X_sample.iloc[[i]]\n            explanation = self.explain_instance(instance, target_class)\n            \n            # Aggregate SHAP values across samples\n            for feature, shap_val in explanation['combined_shap'].items():\n                if feature not in all_shap_values:\n                    all_shap_values[feature] = []\n                all_shap_values[feature].append(shap_val)\n        \n        # Compute mean absolute SHAP value for each feature\n        mean_abs_shap = {feat: np.mean(np.abs(vals)) for feat, vals in all_shap_values.items()}\n        \n        # Sort features by importance\n        sorted_features = sorted(mean_abs_shap.items(), key=lambda x: x[1], reverse=True)\n        \n        # Get top N features\n        top_features = sorted_features[:top_n]\n        features, values = zip(*top_features)\n        \n        # Assign feature groups based on model_configs\n        feature_to_model = {}\n        for model_name, config in self.stack_model.model_configs.items():\n            for feat in config['feature_names']:\n                feature_to_model[feat] = model_name\n        # Features not belonging to any base model are labeled as \"meta\"\n        feature_groups = [feature_to_model.get(feat, \"meta\") for feat in features]\n        \n        # Create plot\n        plt.figure(figsize=(10, 6))        \n        \n        feature_df = pd.DataFrame({\n            'feature': features,\n            'mean_abs_shap': values,\n            'group': feature_groups\n        })\n        \n        sns.barplot(data=feature_df, x='mean_abs_shap', y='feature', hue='group', dodge=False, palette='Set2')\n        \n        plt.xlabel('Mean |SHAP Value|')\n        plt.title(f'Top {top_n} Features (Stacking Model Pipeline)')\n        plt.tight_layout()\n        plt.show()\n        \n        return mean_abs_shap\n    \n    def visualize_prediction_path(self, X_instance, target_class=None):\n        \"\"\"\n        Create a visualization showing how features flow through the model to affect prediction.\n        \n        Args:\n            X_instance (pd.DataFrame): Single instance to explain\n            target_class (int, optional): Class to explain for multi-class problems\n        \"\"\"\n        if len(X_instance) != 1:\n            raise ValueError(\"X_instance must be a single instance (DataFrame with one row)\")\n            \n        explanation = self.explain_instance(X_instance, target_class)\n        feature_chains = explanation['feature_influence_chain']\n        \n        # Create plot\n        plt.figure(figsize=(12, 12))\n        \n        # Determine feature ordering by importance\n        feature_importance = {f: np.abs(sum(vals.values())) for f, vals in feature_chains.items()}\n        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n        features = [f[0] for f in sorted_features]\n        \n        # Limit to top features for readability\n        if len(features) &gt; 10:\n            features = features[:10]\n        \n        # Define color mapping for legend\n        color_mapping = {\n            'Negative Contribution': '#008BFB',  # Blue for negative SHAP values\n            'Positive Contribution': '#FF0051'  # Red for positive SHAP values\n            }\n\n        # Create a grid of plots - one row per feature\n        for i, feature in enumerate(features):\n            plt.subplot(len(features), 1, i+1)\n            \n            # Get intermediate influences for this feature\n            influences = feature_chains[feature]\n            meta_features = list(influences.keys())\n            values = list(influences.values())\n            \n            # Sort by absolute value\n            sorted_idx = np.argsort(np.abs(values))[::-1]\n            meta_features = [meta_features[j] for j in sorted_idx]\n            values = [values[j] for j in sorted_idx]\n            \n            # Limit to top 5 intermediate influences for readability\n            if len(meta_features) &gt; 5:\n                meta_features = meta_features[:5]\n                values = values[:5]\n                \n            # Create bar chart\n            bars = plt.barh(meta_features, values, align='center')\n            \n            # Color positive and negative contributions differently\n            for j, bar in enumerate(bars):\n                if values[j] &lt; 0:\n                    bar.set_color('#008BFB')\n                else:\n                    bar.set_color('#FF0051')\n                    \n            plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n            plt.title(f\"Feature: {feature}\")            \n            \n        # Add a single legend at the bottom of the figure        \n        legend_elements = [\n            Patch(facecolor=color_mapping['Negative Contribution'], edgecolor='black', label='Negative Contribution'),\n            Patch(facecolor=color_mapping['Positive Contribution'], edgecolor='black', label='Positive Contribution')\n        ]\n        plt.figlegend(handles=legend_elements, loc='lower center', ncol=2, frameon=False, fontsize=12)\n        \n        plt.suptitle(\"SHAP Value Contribution\")\n\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make space for the legend\n        plt.show()\n        \n        return explanation\n    \n    def plot_waterfall(self, X_instance, target_class=None, max_features=10):\n        \"\"\"\n        Create a waterfall plot showing how each feature's SHAP value influences\n        the prediction for a single observation.\n        \n        Args:\n            X_instance (pd.DataFrame): Single row dataframe with features to explain\n            target_class (int, optional): Class index to explain (for multi-class problems)\n            max_features (int): Maximum number of features to display in the plot\n            \n        Returns:\n            tuple: (figure, axes) matplotlib figure and axes objects\n        \"\"\"\n        if len(X_instance) != 1:\n            raise ValueError(\"X_instance must be a single instance (DataFrame with one row)\")\n            \n        # Get explanation for the instance\n        explanation = self.explain_instance(X_instance, target_class)\n        combined_shap = explanation['combined_shap']\n        \n        # Get expected value (baseline) for meta model\n        if target_class is not None and isinstance(self.expected_value['meta_model'], list):\n            baseline = self.expected_value['meta_model'][target_class]\n        else:\n            baseline = self.expected_value['meta_model']\n        \n        if isinstance(baseline, np.ndarray) and baseline.size &gt; 1:\n            baseline = baseline[0]\n        \n        # Sort features by absolute SHAP value\n        sorted_features = sorted(\n            [(feat, val) for feat, val in combined_shap.items()],\n            key=lambda x: abs(x[1]),\n            reverse=True\n        )\n        \n        # Limit to max_features\n        if len(sorted_features) &gt; max_features:\n            top_features = sorted_features[:max_features-1]\n            # Combine remaining features into \"other\"\n            other_impact = sum(val for _, val in sorted_features[max_features-1:])\n            top_features.append((\"Other features\", other_impact))\n        else:\n            top_features = sorted_features\n        \n        # Create lists for plotting\n        features = [item[0] for item in top_features]\n        impacts = [item[1] for item in top_features]\n        \n        # Calculate final prediction\n        final_prediction = baseline + sum(impacts)\n        \n        # Create figure and axes\n        fig, ax = plt.subplots(figsize=(10, 7))\n        \n        # Define colors for positive and negative contributions\n        colors = ['#FF0051' if x &gt; 0 else '#008BFB' for x in impacts]\n        \n        # Calculate positions for waterfall bars\n        positions = np.zeros(len(impacts) + 1)\n        positions[0] = baseline\n        for i in range(len(impacts)):\n            positions[i+1] = positions[i] + impacts[i]\n        \n        # Plot waterfall chart\n        # First bar (baseline)\n        ax.bar(0, baseline, bottom=0, color='#BBBBBB', width=0.6, \n            label='Baseline (Expected Value)')\n        \n        # Feature impact bars\n        for i, impact in enumerate(impacts):\n            # For each feature impact, plot a bar from the previous cumulative position\n            # Bottom of each bar is the previous cumulative position\n            bottom = positions[i] - max(0, impact)\n            height = abs(impact)\n            ax.bar(i+1, height, bottom=bottom, color=colors[i], width=0.6)\n            \n        # Final prediction bar\n        ax.bar(len(impacts)+1, final_prediction, bottom=0, color='#32CD32', width=0.6,\n            label='Final Prediction')\n        \n        # Connect bars with lines\n        for i in range(len(positions)-1):\n            ax.plot([i+0.3, i+0.7], [positions[i], positions[i]], 'k-', lw=0.5)\n        \n        # Annotate bars with feature names and values\n        ax.set_xticks(range(len(impacts) + 2))\n        xlabels = ['Baseline'] + features + ['Prediction']\n        ax.set_xticklabels(xlabels, rotation=45, ha='right')\n        \n        # Add value labels to the bars\n        for i, pos in enumerate(positions):\n            if i == 0:\n                ax.text(i, pos + 0.01, f'{baseline:.3f}', ha='center', va='bottom')\n            elif i == len(positions) - 1:\n                ax.text(i, pos + 0.01, f'{pos:.3f}', ha='center', va='bottom')\n        \n        # Add impact values as text\n        for i, impact in enumerate(impacts):\n            ax.text(i+1, positions[i] + impact/2, f'{impact:.3f}', \n                    ha='center', va='center', weight='bold',\n                    color='white' if abs(impact) &gt; 0.1 else 'black')\n        \n        # Add a title and axis labels\n        if target_class is not None:\n            ax.set_title(f'Feature Contributions to Prediction (Class {target_class})')\n        else:\n            ax.set_title('Feature Contributions to Prediction')\n        \n        ax.set_ylabel('Model Output')\n        ax.grid(axis='y', linestyle='--', alpha=0.7)\n        \n        # Create custom legend\n        legend_elements = [\n            Patch(facecolor='#BBBBBB', edgecolor='black', label='Baseline'),\n            Patch(facecolor='#FF0051', edgecolor='black', label='Positive Impact'),\n            Patch(facecolor='#008BFB', edgecolor='black', label='Negative Impact'),\n            Patch(facecolor='#32CD32', edgecolor='black', label='Final Prediction')\n        ]\n        ax.legend(handles=legend_elements, loc='best')\n        \n        plt.tight_layout()\n        \n        return fig, ax\n    \n    def get_feature_influence(self, explanation, feature_name):\n        \"\"\"\n        Extract the influence of a specific feature to the final prediction\n        \n        Args:\n            explanation (dict): The explanation dict from explain_instance\n            feature_name (str): Name of the feature to get influence for\n            \n        Returns:\n            dict: Dictionary containing influence details\n        \"\"\"\n        if feature_name not in explanation['combined_shap']:\n            print(f\"Feature '{feature_name}' not found in explanation!\")\n            return None\n        \n        # Get the combined SHAP value (total influence on final prediction)\n        combined_shap = explanation['combined_shap'][feature_name]\n        \n        # Get the detailed influence chain (how the feature affects various meta-features)\n        influence_chain = explanation['feature_influence_chain'][feature_name]\n        \n        # Find which base model contains this feature\n        model_name = None\n        feature_index = None\n        \n        # This requires access to the stack_model object, which isn't in the explanation dict\n        # We can extract this information from the base_model_shap if available\n        for m_name, shap_info in explanation['base_model_shap'].items():\n            if feature_name in shap_info['feature_names']:\n                model_name = m_name\n                feature_index = shap_info['feature_names'].index(feature_name)\n                break\n        \n        # Get base model SHAP values for this feature\n        base_model_shap = None\n        if model_name is not None and feature_index is not None:\n            base_shap = explanation['base_model_shap'][model_name]['shap_values']\n            \n            # Handle multi-class base models\n            if isinstance(base_shap, list):\n                base_model_shap = [shap_vals[0, feature_index] for shap_vals in base_shap]\n            else:\n                base_model_shap = base_shap[0, feature_index]\n        \n        return {\n            'feature': feature_name,\n            'combined_influence': combined_shap,\n            'influence_paths': influence_chain,\n            'base_model': model_name,\n            'base_model_shap': base_model_shap\n        }\n    \n    def get_model_influence(self, explanation, model_name):\n        \"\"\"\n        Extract the influence of a specific base model to the final prediction\n        \n        Args:\n            explanation (dict): The explanation dict from explain_instance\n            model_name (str): Name of the base model to get influence for\n            \n        Returns:\n            dict: Dictionary containing influence details\n        \"\"\"\n        # Check if the model exists in the stack model\n        if model_name not in self.stack_model.models:\n            print(f\"Model '{model_name}' not found in stack model!\")\n            return None\n        \n        # Check if the model has SHAP values in the explanation\n        if model_name not in explanation['base_model_shap']:\n            print(f\"Model '{model_name}' not found in explanation's base model SHAP values!\")\n            return None\n        \n        # Get information about this model\n        model_info = self.stack_model.models[model_name]\n        features = model_info['features']\n        \n        # Get SHAP values for this model's features\n        base_shap_info = explanation['base_model_shap'][model_name]\n        \n        # Get meta-features produced by this model\n        # For classification models, this will be class probabilities\n        # For regression models, this will be predictions\n        model_meta_features = []\n        meta_feature_indices = []\n        \n        # Find meta-features corresponding to this model in the meta-model\n        meta_idx = 0\n        for m_name, m_info in self.stack_model.models.items():\n            if m_name == model_name:\n                # If it's our target model, note the indices\n                if m_info['model'] is not None:\n                    # For models (not direct features), get number of outputs\n                    if hasattr(m_info['model'], 'n_classes_'):\n                        n_outputs = m_info['model'].n_classes_\n                    else:\n                        n_outputs = 1\n                    \n                    # Store meta-feature names and indices\n                    for i in range(n_outputs):\n                        meta_feat_name = f\"{model_name}_class{i}\" if n_outputs &gt; 1 else model_name\n                        model_meta_features.append(meta_feat_name)\n                        meta_feature_indices.append(meta_idx + i)\n                else:\n                    # For direct features, each feature is a meta-feature\n                    for feature in features:\n                        model_meta_features.append(feature)\n                        meta_feature_indices.append(meta_idx)\n                        meta_idx += 1\n                break\n            else:\n                # Update meta-feature index based on previous models\n                if m_info['model'] is not None:\n                    if hasattr(m_info['model'], 'n_classes_'):\n                        meta_idx += m_info['model'].n_classes_\n                    else:\n                        meta_idx += 1\n                else:\n                    meta_idx += len(m_info['features'])\n        \n        # Get meta-model SHAP values for this model's outputs\n        meta_shap = explanation['meta_model_shap']\n        \n        # Handle multi-class meta models\n        if isinstance(meta_shap, list):\n            meta_model_shap = []\n            for class_shap in meta_shap:\n                meta_model_shap.append([class_shap[0, idx] for idx in meta_feature_indices])\n        else:\n            meta_model_shap = [meta_shap[0, idx] for idx in meta_feature_indices]\n        \n        # Calculate total influence of this model on final prediction\n        # Sum of all meta-feature SHAP values corresponding to this model\n        if isinstance(meta_model_shap[0], list):\n            # For multi-class meta models\n            total_influence = [sum(class_shap) for class_shap in meta_model_shap]\n        else:\n            total_influence = sum(meta_model_shap)\n        \n        # Get influence of each feature in this model\n        feature_influences = {}\n        for feature in features:\n            # Try to find this feature in the combined SHAP values\n            if feature in explanation['combined_shap']:\n                feature_influences[feature] = explanation['combined_shap'][feature]\n        \n        return {\n            'model': model_name,\n            'total_influence': total_influence,\n            'meta_features': dict(zip(model_meta_features, meta_model_shap)),\n            'features': features,\n            'feature_influences': feature_influences,\n            'base_model_shap_values': base_shap_info['shap_values']\n        }\n\nThis verification code includes two key functions designed to validate and debug the SHAP explanations produced by the StackModelShapExplainer:\n\nverify_chain_rule_calculation Function\nThis function validates whether the chain rule calculations in the explainer are correct for a specific feature. It:\n\nIdentifies which model contains the feature - It searches through the stack model to determine which base model uses the specified feature.\nExtracts base model SHAP values - It retrieves the SHAP values for this feature from the base model, handling both classification and regression cases.\nExamines meta-model SHAP values - It identifies the meta-features that are influenced by this feature and retrieves their SHAP values.\nValidates chain rule calculations - It compares the sum of influence paths (from the feature influence chain) with the combined SHAP value to verify mathematical consistency.\nHandles pass-through features - For features that bypass base models and go directly to the meta-model, it performs a different validation.\n\nThe function provides detailed logging at each step, showing the mathematical components that should add up to the final combined SHAP value. It concludes with a pass/fail indicator showing whether the chain rule calculation is correct (✓) or has a discrepancy (✗).\n\n\nprint_detailed_shap_values Function\nThis function prints a comprehensive breakdown of all SHAP values at each level of the stack model:\n\nOriginal feature SHAP values - Lists the combined influence of each original feature.\nBase model SHAP values - For each base model, it shows how input features influence that model’s output, handling multi-class cases appropriately.\nMeta-feature values - Shows the actual values of the meta-features (outputs from base models) that feed into the meta-model.\nMeta-model SHAP values - Details how each meta-feature influences the final prediction.\nChain rule calculations - For each original feature, it shows how its influence propagates through different meta-features, culminating in its combined effect.\n\nThese functions are valuable debugging and verification tools that:\n\nEnsure the mathematical correctness of the combined SHAP value calculations\nHelp users understand the flow of influence through the model stack\nValidate that the chain rule implementation properly accounts for all paths of influence\nProvide transparency into how feature attributions are derived\n\nThey would typically be used during development of the explainer or when users want to deeply understand or verify the explanations for particularly important predictions.\n\ndef verify_chain_rule_calculation(explanation, stack_model, feature_name, target_class=None):\n    \"\"\"\n    Verify the chain rule calculation for a specific feature\n    \n    Args:\n        explanation (dict): The explanation dict from explain_instance\n        stack_model (StackModel): The trained stacking model\n        feature_name (str): Name of the feature to verify\n        target_class (int, optional): Class to focus on for multi-class problems\n    \"\"\"\n    print(f\"\\n===== VERIFYING CHAIN RULE FOR FEATURE: {feature_name} =====\\n\")\n    \n    # Find which model contains this feature\n    model_name = None\n    feature_index = None\n    \n    for m_name, model_info in stack_model.models.items():\n        if feature_name in model_info['features']:\n            model_name = m_name\n            feature_index = model_info['features'].index(feature_name)\n            break\n    \n    if model_name is None:\n        print(f\"Feature {feature_name} not found in any model!\")\n        return\n    \n    print(f\"Feature {feature_name} belongs to model {model_name} at index {feature_index}\")\n    \n    # Get base model SHAP values for this feature\n    if stack_model.models[model_name]['model'] is not None:\n        base_shap = explanation['base_model_shap'][model_name]['shap_values']\n        \n        if isinstance(base_shap, list) and target_class is not None:\n            base_feature_shap = base_shap[target_class][0, feature_index]\n            print(f\"Base model SHAP value (class {target_class}): {base_feature_shap:.6f}\")\n        elif isinstance(base_shap, list):\n            for class_idx in range(len(base_shap)):\n                base_feature_shap = base_shap[class_idx][0, feature_index]\n                print(f\"Base model SHAP value (class {class_idx}): {base_feature_shap:.6f}\")\n        else:\n            base_feature_shap = base_shap[0, feature_index]\n            print(f\"Base model SHAP value: {base_feature_shap:.6f}\")\n        \n        # Get meta-model SHAP values for outputs of this model\n        meta_shap = explanation['meta_model_shap']\n        meta_feature_names = stack_model.feature_name_\n        \n        meta_indices = [i for i, name in enumerate(meta_feature_names) if name.startswith(f\"{model_name}_class\")]\n        \n        print(\"\\nMeta-model SHAP values for this model's outputs:\")\n        \n        if isinstance(meta_shap, list) and target_class is not None:\n            for idx in meta_indices:\n                meta_feat_name = meta_feature_names[idx]\n                meta_feat_shap = meta_shap[target_class][0, idx]\n                print(f\"  {meta_feat_name}: {meta_feat_shap:.6f}\")\n        elif isinstance(meta_shap, list):\n            for class_idx in range(len(meta_shap)):\n                for idx in meta_indices:\n                    meta_feat_name = meta_feature_names[idx]\n                    meta_feat_shap = meta_shap[class_idx][0, idx]\n                    print(f\"    {meta_feat_name}: {meta_feat_shap:.6f}\")\n        else:\n            for idx in meta_indices:\n                meta_feat_name = meta_feature_names[idx]\n                meta_feat_shap = meta_shap[0, idx]\n                print(f\"  {meta_feat_name}: {meta_feat_shap:.6f}\")\n        \n        # Verify chain rule calculation\n        print(\"\\nChain rule calculation:\")\n        combined_effect = 0\n        \n        for meta_feature, value in explanation['feature_influence_chain'][feature_name].items():\n            print(f\"  {meta_feature}: {value:.6f}\")\n            combined_effect += value\n        \n        print(f\"\\nSum of all paths: {combined_effect:.6f}\")\n        print(f\"Combined SHAP value: {explanation['combined_shap'][feature_name]:.6f}\")\n        \n        if abs(combined_effect - explanation['combined_shap'][feature_name]) &lt; 1e-6:\n            print(\"✓ Chain rule calculation is correct!\")\n        else:\n            print(\"✗ Chain rule calculation has discrepancy!\")\n    else:\n        # For pass-through features\n        meta_shap = explanation['meta_model_shap']\n        meta_feature_names = stack_model.feature_name_\n        meta_idx = meta_feature_names.index(feature_name)\n        \n        print(\"This is a pass-through feature, so SHAP value comes directly from meta-model.\")\n        \n        if isinstance(meta_shap, list) and target_class is not None:\n            meta_shap_val = meta_shap[target_class][0, meta_idx]\n            print(f\"Meta-model SHAP value (class {target_class}): {meta_shap_val:.6f}\")\n        elif isinstance(meta_shap, list):\n            for class_idx in range(len(meta_shap)):\n                meta_shap_val = meta_shap[class_idx][0, meta_idx]\n                print(f\"Meta-model SHAP value (class {class_idx}): {meta_shap_val:.6f}\")\n        else:\n            meta_shap_val = meta_shap[0, meta_idx]\n            print(f\"Meta-model SHAP value: {meta_shap_val:.6f}\")\n        \n        # Check if calculation is correct\n        if abs(meta_shap_val - explanation['combined_shap'][feature_name]) &lt; 1e-6:\n            print(\"✓ Direct pass-through SHAP value is correct!\")\n        else:\n            print(\"✗ Direct pass-through SHAP value has discrepancy!\")\n\n\ndef print_detailed_shap_values(explanation, stack_model, target_class=None):\n    \"\"\"\n    Print detailed SHAP values for both base models and meta-model.\n    \n    Args:\n        explanation (dict): The explanation dict from explain_instance.\n        stack_model (StackModel): The trained stacking model.\n        target_class (int, optional): Class to focus on for multi-class problems.\n    \"\"\"\n    print(\"\\n===== DETAILED SHAP VALUES =====\\n\")\n    \n    # Get original features\n    orig_features = []\n    for model_name, model_info in stack_model.models.items():\n        orig_features.extend(model_info['features'])\n    \n    # Print SHAP values for original features\n    for feature in orig_features:\n        if feature in explanation['combined_shap']:\n            print(f\"  {feature}: {explanation['combined_shap'][feature]:.6f}\")\n    \n    print(\"\\nBASE MODELS SHAP VALUES:\")\n    for model_name, shap_info in explanation['base_model_shap'].items():\n        print(f\"  Model: {model_name}\")\n        features = shap_info['feature_names']\n        shap_values = shap_info['shap_values']\n        \n        # Handle multi-class SHAP values\n        if isinstance(shap_values, list) and target_class is not None:\n            print(f\"    Class {target_class}:\")\n            for i, feature in enumerate(features):\n                print(f\"      {feature}: {shap_values[target_class][0, i]:.6f}\")\n        elif isinstance(shap_values, list):\n            for class_idx in range(len(shap_values)):\n                print(f\"    Class {class_idx}:\")\n                for i, feature in enumerate(features):\n                    print(f\"      {feature}: {shap_values[class_idx][0, i]:.6f}\")\n        else:\n            for i, feature in enumerate(features):\n                print(f\"    {feature}: {shap_values[0, i]:.6f}\")\n    \n    # Check if feature names are None before proceeding\n    meta_feature_names = stack_model.feature_name_\n    if meta_feature_names is None:\n        print(\"Error: meta_feature_names is None. Please check if the stacking model has been properly fitted.\")\n        return\n\n    print(\"\\nMETA-FEATURES VALUES:\")\n    meta_features = explanation['meta_features'][0]\n    \n    for i, name in enumerate(meta_feature_names):\n        print(f\"  {name}: {meta_features[i]:.6f}\")\n    \n    print(\"\\nMETA-MODEL SHAP VALUES:\")\n    meta_shap = explanation['meta_model_shap']\n    \n    if isinstance(meta_shap, list) and target_class is not None:\n        print(f\"  Class {target_class}:\")\n        for i, name in enumerate(meta_feature_names):\n            print(f\"    {name}: {meta_shap[target_class][0, i]:.6f}\")\n    elif isinstance(meta_shap, list):\n        for class_idx in range(len(meta_shap)):\n            print(f\"  Class {class_idx}:\")\n            for i, name in enumerate(meta_feature_names):\n                print(f\"    {name}: {meta_shap[class_idx][0, i]:.6f}\")\n    else:\n        for i, name in enumerate(meta_feature_names):\n            print(f\"  {name}: {meta_shap[0, i]:.6f}\")\n    \n    print(\"\\nCHAIN RULE CALCULATIONS:\")\n    for feature, influences in explanation['feature_influence_chain'].items():\n        print(f\"  Feature: {feature}\")\n        for meta_feature, value in influences.items():\n            print(f\"    via {meta_feature}: {value:.6f}\")\n        print(f\"    Combined effect: {explanation['combined_shap'][feature]:.6f}\")\n\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport os\n\ndef wine_quality_stacking_example(save_plots=True, output_dir='outputs'):\n    \"\"\"\n    End-to-end example of using StackModelShapExplainer with a wine quality dataset.\n    \n    Args:\n        save_plots (bool): Whether to save plots to disk instead of displaying\n        output_dir (str): Directory to save plots and results\n    \n    Returns:\n        tuple: (stack_model, explainer, X_test, y_test, label_encoder, explanation)\n    \"\"\"\n    print(\"=\" * 80)\n    print(\"Wine Quality Stacking Model Example with SHAP Explanations\")\n    print(\"=\" * 80)\n    \n    # Create output directory if needed\n    if save_plots and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate synthetic wine quality data\n    np.random.seed(42)\n    n_samples = 1599  # Similar to the real wine quality dataset size\n    \n    # Feature names for wine quality dataset\n    features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n               'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n               'pH', 'sulphates', 'alcohol']\n    \n    # Generate synthetic data with realistic distributions\n    wine_data = {\n        'fixed acidity': np.random.normal(7.2, 1.3, n_samples),\n        'volatile acidity': np.random.normal(0.34, 0.17, n_samples),\n        'citric acid': np.random.normal(0.32, 0.14, n_samples),\n        'residual sugar': np.clip(np.random.exponential(2.0, n_samples), 0.9, 15.5),\n        'chlorides': np.clip(np.random.exponential(0.05, n_samples), 0.01, 0.6),\n        'free sulfur dioxide': np.clip(np.random.normal(30, 17, n_samples), 1, 120),\n        'total sulfur dioxide': np.clip(np.random.normal(115, 56, n_samples), 6, 300),\n        'density': np.random.normal(0.995, 0.002, n_samples),\n        'pH': np.random.normal(3.2, 0.16, n_samples),\n        'sulphates': np.random.normal(0.53, 0.15, n_samples),\n        'alcohol': np.clip(np.random.normal(10.4, 1.1, n_samples), 8.0, 14.0),\n    }\n    \n    # Generate quality based on feature values to make it more realistic\n    # Wine quality is higher with higher alcohol, lower volatile acidity, etc.\n    quality = 6 + (wine_data['alcohol'] - 10.4) / 1.1 - (wine_data['volatile acidity'] - 0.34) / 0.17\n    quality = np.round(np.clip(quality, 3, 8)).astype(int)\n    wine_data['quality'] = quality\n    \n    # Create DataFrame\n    data = pd.DataFrame(wine_data)\n    \n    # Convert target to multi-class (low, medium, high quality)\n    print(\"\\nConverting wine quality to categorical labels...\")\n    def quality_label(q):\n        if q &lt;= 5:\n            return 'low'\n        elif q == 6:\n            return 'medium'\n        else:\n            return 'high'\n    \n    data['quality_label'] = data['quality'].apply(quality_label)\n    data.drop(columns=['quality'], inplace=True)\n    \n    # Display class distribution\n    class_counts = data['quality_label'].value_counts()\n    print(\"Class distribution:\")\n    print(class_counts)\n    \n    # Encode target labels\n    label_encoder = LabelEncoder()\n    data['quality_label'] = label_encoder.fit_transform(data['quality_label'])\n    print(f\"Encoded classes: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n    \n    # Features and target\n    X = data.drop(columns=['quality_label'])\n    y = data['quality_label']\n    \n    # Split data\n    print(\"\\nSplitting data into train and test sets...\")\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n    \n    # Define model configuration\n    print(\"\\nConfiguring stacked model architecture...\")\n    model_configs = {\n        'acidity_model': {\n            'feature_names': ['fixed acidity', 'volatile acidity', 'citric acid', 'pH'],\n            'estimators': lgb.LGBMClassifier,\n            'hyperparameters': {'n_estimators': 100, 'max_depth': 3, 'verbosity': -1}\n        },\n        'chemical_model': {\n            'feature_names': ['chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates'],\n            'estimators': lgb.LGBMClassifier,\n            'hyperparameters': {'n_estimators': 100, 'max_depth': 4, 'verbosity': -1}\n        },\n        'physical_properties': {\n            'feature_names': ['residual sugar', 'density', 'alcohol'],\n            'estimators': lgb.LGBMClassifier,\n            'hyperparameters': {'n_estimators': 100, 'max_depth': 3, 'verbosity': -1}\n        }\n    }\n    \n    # Create and train the stacking model\n    print(\"\\nTraining stacked model...\")\n    final_estimator = lgb.LGBMClassifier(n_estimators=50, verbosity=-1)\n    stack_model = StackModel(model_configs, final_estimator, cv=5)\n    stack_model.fit(X_train, y_train)\n    \n    # Evaluate model\n    print(\"\\nEvaluating model performance...\")\n    y_pred = stack_model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Stacking Model Accuracy: {accuracy:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n    \n    # Create SHAP explainer\n    print(\"\\nCreating SHAP explainer...\")\n    explainer = StackModelShapExplainer(stack_model, background_data=X_train, n_background=100)\n    \n    # Get explanation for a single instance\n    instance_idx = 25\n    instance = X_test.iloc[[instance_idx]]\n    true_class = y_test.iloc[instance_idx]\n    class_name = label_encoder.inverse_transform([true_class])[0]\n    print(f\"\\nExplaining prediction for instance with true class: {class_name}\")\n    \n    # Display feature values for selected instance\n    print(\"\\nFeature values for the explained instance:\")\n    for feature in X.columns:\n        print(f\"{feature}: {instance[feature].values[0]:.4f}\")\n    \n    # Get explanation for the instance\n    print(\"\\nGenerating SHAP explanation...\")\n    explanation = explainer.explain_instance(instance, target_class=true_class)\n    \n    # Print combined SHAP values\n    print(\"\\nTop 5 combined SHAP values (impact on final prediction):\")\n    for feature, value in sorted(explanation['combined_shap'].items(), \n                               key=lambda x: abs(x[1]), reverse=True)[:5]:\n        print(f\"{feature}: {value:.6f}\")\n    \n    # Plot feature importance for the dataset\n    print(\"\\nPlotting overall feature importance...\")\n    fig_importance = explainer.plot_feature_importance(\n        X_test, \n        target_class=1\n    )\n    # plt.title(f\"Feature Importance for Class: {class_name}\")\n    \n    if save_plots:\n        plt.savefig(os.path.join(output_dir, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n    else:\n        plt.show()\n    \n    # Visualize prediction path\n    print(\"\\nVisualizing prediction path...\")\n    try:\n        fig_path = explainer.visualize_prediction_path(\n            instance, \n            target_class=true_class\n        )\n        if save_plots:\n            plt.savefig(os.path.join(output_dir, 'prediction_path.png'), dpi=300, bbox_inches='tight')\n        else:\n            plt.show()\n    except Exception as e:\n        print(f\"Error visualizing prediction path: {e}\")\n    \n    # Create waterfall plot\n    print(\"\\nCreating waterfall plot...\")\n    fig_waterfall, ax_waterfall = explainer.plot_waterfall(\n        instance, \n        target_class=true_class,\n        max_features=10\n    )\n    plt.title(f\"Feature Contributions for Class: {class_name}\")\n    \n    if save_plots:\n        plt.savefig(os.path.join(output_dir, 'waterfall_plot.png'), dpi=300, bbox_inches='tight')\n    else:\n        plt.show()\n    \n    # Print detailed SHAP values\n    print(\"\\nPrinting detailed SHAP values...\")\n    print_detailed_shap_values(explanation, stack_model, target_class=true_class)\n    \n    # Verify chain rule calculation for top features\n    print(\"\\nVerifying chain rule calculations...\")\n    important_features = [x[0] for x in sorted(\n        explanation['combined_shap'].items(), key=lambda x: abs(x[1]), reverse=True\n    )[:3]]\n    \n    for feature in important_features:\n        verify_chain_rule_calculation(explanation, stack_model, feature, target_class=true_class)\n    \n    # Feature influence analysis\n    print(\"\\nAnalyzing feature influence paths...\")\n    for feature in important_features:\n        influence = explainer.get_feature_influence(explanation, feature)\n        print(f\"\\nFeature: {feature}\")\n        print(f\"Total influence: {influence['combined_influence']:.6f}\")\n        print(\"Influence paths:\")\n        for meta_feature, value in sorted(influence['influence_paths'].items(), \n                                         key=lambda x: abs(x[1]), reverse=True):\n            print(f\"  Via {meta_feature}: {value:.6f}\")\n    \n    # Model influence analysis\n    print(\"\\nAnalyzing model contributions...\")\n    for model_name in model_configs.keys():\n        model_influence = explainer.get_model_influence(explanation, model_name)\n        print(f\"\\nModel: {model_name}\")\n        print(f\"Total influence: {model_influence['total_influence']}\")\n        print(\"Influence through meta-features:\")\n        for meta_feature, value in model_influence['meta_features'].items():\n            print(f\"  Via {meta_feature}: {value}\")\n    \n    print(\"\\nExample completed successfully!\")\n    return stack_model, explainer, X_test, y_test, label_encoder, explanation\n\n\nif __name__ == \"__main__\":\n    stack_model, explainer, X_test, y_test, label_encoder, explanation = wine_quality_stacking_example()\n\n================================================================================\nWine Quality Stacking Model Example with SHAP Explanations\n================================================================================\n\nConverting wine quality to categorical labels...\nClass distribution:\nquality_label\nhigh      576\nlow       548\nmedium    475\nName: count, dtype: int64\nEncoded classes: {'high': 0, 'low': 1, 'medium': 2}\n\nSplitting data into train and test sets...\nTraining set size: 1279, Test set size: 320\n\nConfiguring stacked model architecture...\n\nTraining stacked model...\n\nEvaluating model performance...\nStacking Model Accuracy: 0.8187\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        high       0.87      0.88      0.87       115\n         low       0.86      0.87      0.87       110\n      medium       0.70      0.68      0.69        95\n\n    accuracy                           0.82       320\n   macro avg       0.81      0.81      0.81       320\nweighted avg       0.82      0.82      0.82       320\n\n\nCreating SHAP explainer...\n\nExplaining prediction for instance with true class: medium\n\nFeature values for the explained instance:\nfixed acidity: 6.1331\nvolatile acidity: 0.3528\ncitric acid: 0.0838\nresidual sugar: 0.9000\nchlorides: 0.0420\nfree sulfur dioxide: 26.1666\ntotal sulfur dioxide: 20.7110\ndensity: 0.9939\npH: 3.2429\nsulphates: 0.1829\nalcohol: 10.1959\n\nGenerating SHAP explanation...\n\nTop 5 combined SHAP values (impact on final prediction):\nalcohol: 0.179514\nvolatile acidity: 0.079854\nfixed acidity: -0.061502\ndensity: -0.050285\ncitric acid: -0.036569\n\nPlotting overall feature importance...\n\n\n\n\n\n\n\n\n\n\nVisualizing prediction path...\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nCreating waterfall plot...\n\nPrinting detailed SHAP values...\n\n===== DETAILED SHAP VALUES =====\n\n  fixed acidity: -0.061502\n  volatile acidity: 0.079854\n  citric acid: -0.036569\n  pH: -0.024314\n  chlorides: -0.004809\n  free sulfur dioxide: -0.017936\n  total sulfur dioxide: 0.008719\n  sulphates: -0.002468\n  residual sugar: 0.007396\n  density: -0.050285\n  alcohol: 0.179514\n\nBASE MODELS SHAP VALUES:\n  Model: acidity_model\n    fixed acidity: -0.079397\n    volatile acidity: 0.103088\n    citric acid: -0.047208\n    pH: -0.031388\n  Model: chemical_model\n    chlorides: 0.074946\n    free sulfur dioxide: 0.279515\n    total sulfur dioxide: -0.135886\n    sulphates: 0.038459\n  Model: physical_properties\n    residual sugar: 0.014496\n    density: -0.098555\n    alcohol: 0.351834\n\nMETA-FEATURES VALUES:\n  acidity_model_class0: 0.227219\n  acidity_model_class1: 0.474671\n  acidity_model_class2: 0.298110\n  chemical_model_class0: 0.459772\n  chemical_model_class1: 0.197444\n  chemical_model_class2: 0.342784\n  physical_properties_class0: 0.307260\n  physical_properties_class1: 0.338261\n  physical_properties_class2: 0.354479\n\nMETA-MODEL SHAP VALUES:\n  acidity_model_class0: 0.263017\n  acidity_model_class1: 0.287869\n  acidity_model_class2: 0.223734\n  chemical_model_class0: -0.082719\n  chemical_model_class1: 0.209395\n  chemical_model_class2: -0.190844\n  physical_properties_class0: 0.514815\n  physical_properties_class1: 0.251906\n  physical_properties_class2: -0.256497\n\nCHAIN RULE CALCULATIONS:\n  Feature: fixed acidity\n    via acidity_model_class0: -0.020883\n    via acidity_model_class1: -0.022856\n    via acidity_model_class2: -0.017764\n    Combined effect: -0.061502\n  Feature: volatile acidity\n    via acidity_model_class0: 0.027114\n    via acidity_model_class1: 0.029676\n    via acidity_model_class2: 0.023064\n    Combined effect: 0.079854\n  Feature: citric acid\n    via acidity_model_class0: -0.012417\n    via acidity_model_class1: -0.013590\n    via acidity_model_class2: -0.010562\n    Combined effect: -0.036569\n  Feature: pH\n    via acidity_model_class0: -0.008255\n    via acidity_model_class1: -0.009036\n    via acidity_model_class2: -0.007022\n    Combined effect: -0.024314\n  Feature: chlorides\n    via chemical_model_class0: -0.006199\n    via chemical_model_class1: 0.015693\n    via chemical_model_class2: -0.014303\n    Combined effect: -0.004809\n  Feature: free sulfur dioxide\n    via chemical_model_class0: -0.023121\n    via chemical_model_class1: 0.058529\n    via chemical_model_class2: -0.053344\n    Combined effect: -0.017936\n  Feature: total sulfur dioxide\n    via chemical_model_class0: 0.011240\n    via chemical_model_class1: -0.028454\n    via chemical_model_class2: 0.025933\n    Combined effect: 0.008719\n  Feature: sulphates\n    via chemical_model_class0: -0.003181\n    via chemical_model_class1: 0.008053\n    via chemical_model_class2: -0.007340\n    Combined effect: -0.002468\n  Feature: residual sugar\n    via physical_properties_class0: 0.007463\n    via physical_properties_class1: 0.003652\n    via physical_properties_class2: -0.003718\n    Combined effect: 0.007396\n  Feature: density\n    via physical_properties_class0: -0.050738\n    via physical_properties_class1: -0.024827\n    via physical_properties_class2: 0.025279\n    Combined effect: -0.050285\n  Feature: alcohol\n    via physical_properties_class0: 0.181129\n    via physical_properties_class1: 0.088629\n    via physical_properties_class2: -0.090244\n    Combined effect: 0.179514\n\nVerifying chain rule calculations...\n\n===== VERIFYING CHAIN RULE FOR FEATURE: alcohol =====\n\nFeature alcohol belongs to model physical_properties at index 2\nBase model SHAP value: 0.351834\n\nMeta-model SHAP values for this model's outputs:\n  physical_properties_class0: 0.514815\n  physical_properties_class1: 0.251906\n  physical_properties_class2: -0.256497\n\nChain rule calculation:\n  physical_properties_class0: 0.181129\n  physical_properties_class1: 0.088629\n  physical_properties_class2: -0.090244\n\nSum of all paths: 0.179514\nCombined SHAP value: 0.179514\n✓ Chain rule calculation is correct!\n\n===== VERIFYING CHAIN RULE FOR FEATURE: volatile acidity =====\n\nFeature volatile acidity belongs to model acidity_model at index 1\nBase model SHAP value: 0.103088\n\nMeta-model SHAP values for this model's outputs:\n  acidity_model_class0: 0.263017\n  acidity_model_class1: 0.287869\n  acidity_model_class2: 0.223734\n\nChain rule calculation:\n  acidity_model_class0: 0.027114\n  acidity_model_class1: 0.029676\n  acidity_model_class2: 0.023064\n\nSum of all paths: 0.079854\nCombined SHAP value: 0.079854\n✓ Chain rule calculation is correct!\n\n===== VERIFYING CHAIN RULE FOR FEATURE: fixed acidity =====\n\nFeature fixed acidity belongs to model acidity_model at index 0\nBase model SHAP value: -0.079397\n\nMeta-model SHAP values for this model's outputs:\n  acidity_model_class0: 0.263017\n  acidity_model_class1: 0.287869\n  acidity_model_class2: 0.223734\n\nChain rule calculation:\n  acidity_model_class0: -0.020883\n  acidity_model_class1: -0.022856\n  acidity_model_class2: -0.017764\n\nSum of all paths: -0.061502\nCombined SHAP value: -0.061502\n✓ Chain rule calculation is correct!\n\nAnalyzing feature influence paths...\n\nFeature: alcohol\nTotal influence: 0.179514\nInfluence paths:\n  Via physical_properties_class0: 0.181129\n  Via physical_properties_class2: -0.090244\n  Via physical_properties_class1: 0.088629\n\nFeature: volatile acidity\nTotal influence: 0.079854\nInfluence paths:\n  Via acidity_model_class1: 0.029676\n  Via acidity_model_class0: 0.027114\n  Via acidity_model_class2: 0.023064\n\nFeature: fixed acidity\nTotal influence: -0.061502\nInfluence paths:\n  Via acidity_model_class1: -0.022856\n  Via acidity_model_class0: -0.020883\n  Via acidity_model_class2: -0.017764\n\nAnalyzing model contributions...\n\nModel: acidity_model\nTotal influence: 0.7746197501622374\nInfluence through meta-features:\n  Via acidity_model_class0: 0.26301690588647036\n  Via acidity_model_class1: 0.28786877665785143\n  Via acidity_model_class2: 0.22373406761791556\n\nModel: chemical_model\nTotal influence: -0.06416729296499396\nInfluence through meta-features:\n  Via chemical_model_class0: -0.08271907892834861\n  Via chemical_model_class1: 0.2093953155260533\n  Via chemical_model_class2: -0.19084352956269868\n\nModel: physical_properties\nTotal influence: 0.5102236171328696\nInfluence through meta-features:\n  Via physical_properties_class0: 0.5148149915394606\n  Via physical_properties_class1: 0.25190587860473895\n  Via physical_properties_class2: -0.25649725301133003\n\nExample completed successfully!\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;",
    "crumbs": [
      "Machine learning",
      "StackModelShapExplainer Documentation"
    ]
  },
  {
    "objectID": "machine-learning/StackModelShapExplainer.html#a-new-observation",
    "href": "machine-learning/StackModelShapExplainer.html#a-new-observation",
    "title": "StackModelShapExplainer Documentation",
    "section": "A new observation",
    "text": "A new observation\n\n# Assuming you already have a trained StackModel called 'stack_model'\n# and a DataFrame 'X_train' that was used to train the model\n\n# Create a new observation for a wine sample\nnew_observation = pd.DataFrame([{\n    'fixed acidity': 7.0,\n    'volatile acidity': 0.35,\n    'citric acid': 0.30,\n    'residual sugar': 2.0,\n    'chlorides': 0.05,\n    'free sulfur dioxide': 25.0,\n    'total sulfur dioxide': 120.0,\n    'density': 0.996,\n    'pH': 3.2,\n    'sulphates': 0.5,\n    'alcohol': 10.5\n}])\n\n# Initialize the SHAP explainer with your trained model and training data\nexplainer = StackModelShapExplainer(\n    stack_model=stack_model,\n    background_data=X_test,\n    n_background=100  # Use 100 background samples for efficiency\n)\n\n# Get the prediction for the new observation\nprediction = stack_model.predict(new_observation)[0]\nprobabilities = stack_model.predict_proba(new_observation)[0]\n\nprint(f\"Predicted wine quality: {prediction}\")\nprint(f\"Prediction probabilities: {probabilities}\")\n\n# Explain the prediction using the waterfall plot\n# If stack_model is a binary classifier, target_class=1 would represent the positive class\nfig, ax = explainer.plot_waterfall(new_observation, target_class=1)\nplt.show()\n\n# You can also visualize feature importance across the entire model\nfeature_importance = explainer.plot_feature_importance(X_test, target_class=1)\n\n# For a deeper understanding of how features flow through the model\nexplanation = explainer.visualize_prediction_path(new_observation, target_class=1)\n\n# If you want to inspect the raw SHAP values\nraw_explanation = explainer.explain_instance(new_observation, target_class=1)\nprint(\"\\nTop 5 most influential features for this prediction:\")\nfor feature, shap_value in sorted(\n    raw_explanation['combined_shap'].items(), \n    key=lambda x: abs(x[1]), \n    reverse=True\n)[:5]:\n    print(f\"{feature}: {shap_value:.4f}\")\n\n# You can also examine how each feature influences the prediction through different paths\nprint(\"\\nInfluence paths for 'alcohol':\")\nif 'alcohol' in raw_explanation['feature_influence_chain']:\n    for meta_feature, influence in raw_explanation['feature_influence_chain']['alcohol'].items():\n        print(f\"  → {meta_feature}: {influence:.4f}\")\n\nPredicted wine quality: 0\nPrediction probabilities: [0.65455803 0.00402556 0.34141641]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop 5 most influential features for this prediction:\nalcohol: 0.4244\ndensity: 0.3190\nfixed acidity: 0.2301\ncitric acid: -0.1939\npH: 0.0958\n\nInfluence paths for 'alcohol':\n  → physical_properties_class0: 0.2445\n  → physical_properties_class1: 0.1653\n  → physical_properties_class2: 0.0146\n\n\n\nimport pandas as pd\n\ndef store_influence_to_predicted_class(explainer, stack_model, new_observation, predicted_class):\n    \"\"\"\n    Store the influence of features and models on the predicted class.\n    \n    Args:\n        explainer (StackModelShapExplainer): The SHAP explainer.\n        stack_model (StackModel): The trained stacking model.\n        new_observation (pd.DataFrame): The feature matrix for a single instance to explain.\n        predicted_class (int): The predicted class index for the instance.\n    \n    Returns:\n        tuple: \n            pd.DataFrame: DataFrame containing feature influences on the predicted class.\n            pd.DataFrame: DataFrame containing model influences on the predicted class.\n    \"\"\"\n    \n    # Get the SHAP explanation for the given observation and predicted class\n    explanation = explainer.explain_instance(new_observation, target_class=predicted_class)\n\n    # Initialize lists to store feature and model influence data\n    feature_influence_data = []\n    model_influence_data = []\n    \n    # ---- Store feature influence ----\n    for feature in new_observation.columns:  # Loop through all features\n        feature_influence = explainer.get_feature_influence(explanation, feature)\n        total_influence = feature_influence['combined_influence']\n        influence_paths = feature_influence['influence_paths']\n        \n        # Store feature influence data\n        for meta_feature, influence in sorted(influence_paths.items(), key=lambda x: abs(x[1]), reverse=True):\n            feature_influence_data.append({\n                'Feature': feature,\n                'Meta Feature': meta_feature,\n                'Influence': influence,\n                'Total Influence': total_influence,\n                'Predicted Class': predicted_class\n            })\n    \n    # ---- Store model influence ----\n    for model_name in stack_model.models.keys():  # Loop through all models\n        model_influence = explainer.get_model_influence(explanation, model_name)\n        total_influence = model_influence['total_influence']\n        meta_feature_influences = model_influence['meta_features']\n        \n        # Store model influence data\n        for meta_feature, influence in meta_feature_influences.items():\n            model_influence_data.append({\n                'Model': model_name,\n                'Meta Feature': meta_feature,\n                'Influence': influence,\n                'Total Influence': total_influence,\n                'Predicted Class': predicted_class\n            })\n    \n    # Convert the lists into separate DataFrames\n    feature_influence_df = pd.DataFrame(feature_influence_data)\n    model_influence_df = pd.DataFrame(model_influence_data)\n    \n    return feature_influence_df, model_influence_df\n\n\n# Example Usage\n# Assuming you already have a trained 'stack_model', 'explainer', and a new observation\n# Let's assume the predicted class is 1 (medium quality wine in the example dataset)\nnew_observation = pd.DataFrame([{\n    'fixed acidity': 7.0,\n    'volatile acidity': 0.35,\n    'citric acid': 0.30,\n    'residual sugar': 2.0,\n    'chlorides': 0.05,\n    'free sulfur dioxide': 25.0,\n    'total sulfur dioxide': 120.0,\n    'density': 0.996,\n    'pH': 3.2,\n    'sulphates': 0.5,\n    'alcohol': 10.5\n}])\n\npredicted_class = 1  # For example, class 1 (e.g., medium quality in the wine dataset)\n\n# Call the function to store influences for the predicted class\nfeature_influence_df, model_influence_df = store_influence_to_predicted_class(explainer, stack_model, new_observation, predicted_class)\n\n\n\n\nfile_name = 'influence_df.xlsx'\nwith pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:\n    # Save each DataFrame to a different sheet\n    feature_influence_df.to_excel(writer, sheet_name='Feature Influence', index=False)\n    model_influence_df.to_excel(writer, sheet_name='Model Influence', index=False)\n\n\npredicted_class = 0 \nfeature_influence_df, model_influence_df = store_influence_to_predicted_class(explainer, stack_model, new_observation, predicted_class)\nfile_name = 'influence_df0.xlsx'\nwith pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:\n    # Save each DataFrame to a different sheet\n    feature_influence_df.to_excel(writer, sheet_name='Feature Influence', index=False)\n    model_influence_df.to_excel(writer, sheet_name='Model Influence', index=False)\n\n\nexplanation = explainer.explain_instance(new_observation, target_class=1)\nexplanation\n\n{'combined_shap': {'fixed acidity': 0.23006162160601437,\n  'volatile acidity': -0.006290676600541171,\n  'citric acid': -0.1938829505432772,\n  'pH': 0.0957712563895823,\n  'chlorides': -0.004115580133319979,\n  'free sulfur dioxide': 0.0035060000889678335,\n  'total sulfur dioxide': -0.0008107644553777969,\n  'sulphates': 0.002053618847751592,\n  'residual sugar': -0.07281704415414081,\n  'density': 0.31899564561470933,\n  'alcohol': 0.4244219215434195},\n 'feature_influence_chain': {'fixed acidity': {'acidity_model_class0': 0.1834103026442602,\n   'acidity_model_class1': 0.045779502543218616,\n   'acidity_model_class2': 0.0008718164185355561},\n  'volatile acidity': {'acidity_model_class0': -0.005015068967558123,\n   'acidity_model_class1': -0.0012517691713319287,\n   'acidity_model_class2': -2.383846165111903e-05},\n  'citric acid': {'acidity_model_class0': -0.15456785181494598,\n   'acidity_model_class1': -0.03858038105409343,\n   'acidity_model_class2': -0.000734717674237794},\n  'pH': {'acidity_model_class0': 0.07635100107707458,\n   'acidity_model_class1': 0.019057331009178228,\n   'acidity_model_class2': 0.0003629243033294928},\n  'chlorides': {'chemical_model_class0': -0.0040471153273282275,\n   'chemical_model_class1': -0.007165484100469425,\n   'chemical_model_class2': 0.007097019294477673},\n  'free sulfur dioxide': {'chemical_model_class0': 0.0034476759625694953,\n   'chemical_model_class1': 0.006104166868323785,\n   'chemical_model_class2': -0.006045842741925446},\n  'total sulfur dioxide': {'chemical_model_class0': -0.000797276968961715,\n   'chemical_model_class1': -0.0014115919569154152,\n   'chemical_model_class2': 0.0013981044704993333},\n  'sulphates': {'chemical_model_class0': 0.002019455835141534,\n   'chemical_model_class1': 0.0035754796955242024,\n   'chemical_model_class2': -0.0035413166829141437},\n  'residual sugar': {'physical_properties_class0': -0.041955866451511795,\n   'physical_properties_class1': -0.028352813796893587,\n   'physical_properties_class2': -0.002508363905735435},\n  'density': {'physical_properties_class0': 0.18379953294579657,\n   'physical_properties_class1': 0.12420751552326484,\n   'physical_properties_class2': 0.010988597145647901},\n  'alcohol': {'physical_properties_class0': 0.24454425013016837,\n   'physical_properties_class1': 0.16525740439789674,\n   'physical_properties_class2': 0.014620267015354403}},\n 'meta_model_shap': array([[-1.0238338 , -0.25555054, -0.00486666, -0.02725937, -0.04826316,\n          0.04780201, -0.92701782, -0.62645742, -0.05542248]]),\n 'base_model_shap': {'acidity_model': {'shap_values': array([[-0.1791407 ,  0.00489832,  0.15096967, -0.07457363]]),\n   'feature_names': ['fixed acidity',\n    'volatile acidity',\n    'citric acid',\n    'pH']},\n  'chemical_model': {'shap_values': array([[ 0.14846695, -0.12647674,  0.02924781, -0.074083  ]]),\n   'feature_names': ['chlorides',\n    'free sulfur dioxide',\n    'total sulfur dioxide',\n    'sulphates']},\n  'physical_properties': {'shap_values': array([[ 0.04525896, -0.19826969, -0.26379671]]),\n   'feature_names': ['residual sugar', 'density', 'alcohol']}},\n 'meta_features': array([[0.39118684, 0.24643168, 0.36238148, 0.34323587, 0.3007726 ,\n         0.35599153, 0.35208745, 0.18631852, 0.46159403]]),\n 'expected_value': {'acidity_model': array([-1.31959511, -1.33981237, -1.12389672]),\n  'chemical_model': array([-1.0379243 , -1.11911687, -1.27916369]),\n  'physical_properties': array([-1.35701102, -1.35865684, -1.22170098]),\n  'meta_model': array([-2.18849207, -1.81391848, -1.36151378])}}",
    "crumbs": [
      "Machine learning",
      "StackModelShapExplainer Documentation"
    ]
  },
  {
    "objectID": "explore.html",
    "href": "explore.html",
    "title": "Explore and setup",
    "section": "",
    "text": "With this tutorial, we have a working example website that we will explore together. We’ll learn a few rules and look for patterns to get an understanding of what things to do to help you start customizing and making it your own. And you can continue to use this website as a reference after the tutorial, along with Quarto documentation.\nWe’ll start our exploration online looking at the website architecture and GitHub repository. Then we’ll setup a copy for ourselves so that we can modify from a working example, which is a great way to learn something new. We’ll set it up so that any modifications (commits) will automatically be republished via GitHub Actions. Subsequent chapters will describe how to modify your repo using different tools (browser, RStudio, Jupyter).",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#exploring-online",
    "href": "explore.html#exploring-online",
    "title": "Explore and setup",
    "section": "Exploring online",
    "text": "Exploring online\n\nThe website itself\nThis website has 5 things you can see on the left sidebar:\n\nWelcome\nExploring and setup\nQuarto workflows\nLearning more\nTransition from Rmd\n\nMost of these are pages, but you’ll see that “Quarto Workflows” has an arrow; it is a folder with additional pages inside.\n\n\nThe website’s repo\nLet’s go to this website’s GitHub repository (also called a “repo”), https://github.com/openscapes/quarto-website-tutorial. You can also click there from any page in this tutorial website by clicking the GitHub octocat icon underneath the Openscapes logo in the left navbar (click it holding command on Mac, or control on a PC to open it in a different tab in your browser).\nHave a look at the filenames. We can recognize the names of the webpages we’ve seen above, and they have red arrows marking them in the image below. You’ll see the “quarto-workflows” folder and the rest in this site are .qmd files, which are plain text Quarto files that can combine Markdown text with code. index.qmd is the home page. If you click inside “quarto-workflows” you’ll see a mix of filetypes!\n\n\n\nquarto-website-tutorial GitHub repository with files for webpages marked with red arrows\n\n\nThe _site folder has html files with names that should be familiar: they match the .md files we were just exploring. This folder is where Quarto stores files to build the website.",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#quarto.yml-intro",
    "href": "explore.html#quarto.yml-intro",
    "title": "Explore and setup",
    "section": "_quarto.yml intro",
    "text": "_quarto.yml intro\nThere is also a _quarto.yml file, which is the website’s configuration file. It is essentially metadata for the website that includes the order that the pages/chapters will be in. This is where you update the organization of your website: which page comes before another. If we compare side-by-side, you’ll see that the pages that appear on our website are listed there.\n\n\n\n_quarto.yml and website side-by-side\n\n\nWe’ll learn more about how to interact with _quarto.yml in Quarto Workflows.",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#fork-to-your-account",
    "href": "explore.html#fork-to-your-account",
    "title": "Explore and setup",
    "section": "Fork to your account",
    "text": "Fork to your account\nLet’s start with an existing Quarto site and copy it into your space to edit. You’ll need a free GitHub account that you create at github.com (follow this advice about choosing your username).\nFirst, choose an existing website to copy. The simplest option is to start with this site: quarto-website-tutorial.\nOther options of potential interest:\n\n2021-Cloud-Hackathon\n2022-SWOT-Ocean-Cloud-Workshop\nOpenscapes Approach-Guide\n\nNext, follow these steps to fork and setup your repo with GitHub Actions from Gavin Fay, using the repo you chose. These instructions will take ~5 minutes.\nNow you’ve got a copy of your repo of choice in your own GitHub account, and you’re set to start making your own edits. Your GitHub repo is set up with a GitHub Action that will use Quarto to rebuild and republish your site anytime you make a commit: committing will trigger the GitHub Action to rebuild and republish the book.\nNote that the GitHub Action for this book does not include R or Python so those will need to be added if your website relies on code. See https://github.com/r-lib/actions for more details and examples.\n\nDownload instead of fork\nForking might not always be the way to go - you can’t fork into the same GitHub user account or organization so if for example you want to make a copy of 2021-Cloud-Hackathon repo within the same NASA-Openscapes GitHub Organization, you’ll need to download instead of fork. In this case, follow these steps to download and copy into a new repository, and set up the GitHub Action separately.\n\nDownload github repo files\nNavigate to https://github.com/openscapes/quarto-website-tutorial (or any other quarto site repo of choice). Click the green “Code” button and select “Download ZIP”. When it downloads on your computer, unzip the files.\n\n\nCreate a new GitHub repo\nNavigate to your GitHub account or organization, and create a new repository, naming it what you’d like. You’ll need a free GitHub account that you create at github.com (follow this advice about choosing your username). When you’re logged in, github.com will show a green button that says “New” which you’ll also see as you navigate to your username’s repository page.\n\n\nAdd original site files\nTo use the GitHub file uploader, click the button next to the green “Code” button that says “Add file”. Add file &gt; Upload files. Then, on your computer, select all the files in unzipped folder (command-A or control-A), and drag them to the GitHub uploader page. Scroll down to write a commit message, which effectively saves your files when you’re working in the browser.\nNote: if you’re comfortable cloning the new repository and copying files into it locally before committing and pushing back to GitHub, that can be preferable to the uploader, which does have limitations with complex repos (although the uploader works fine with this tutorial repo).",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#setup-github-action",
    "href": "explore.html#setup-github-action",
    "title": "Explore and setup",
    "section": "Set up GitHub publishing",
    "text": "Set up GitHub publishing\nIf you’ve used the GitHub uploader, you’ll need to set up GitHub publishing separately. We’ll do this in a few steps: we’ll set up a GitHub Action within your repo, and create a gh-pages branch.\nFirst, the GitHub Action. Go back to your main view of your GitHub repository by clicking on the name of your repository in blue at the top-left (the url in your browser window should say https://github.com/username/repo-name).\nNext to the green code button, click Add file &gt; Create new file. Name it exactly this: .github/workflows/quarto-publish.yml . In detail: start by typing the . with github and when you type the / it will give you a new text box to type workflows (plural!), then another /, and finally, quarto-publish.yml.\nNow you’ll have an empty new file. Paste the following in this empty file - you can click on the top-right of this box to copy all the code inside this code box:\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      # add software dependencies here\n\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # this secret is always available for github actions\nCommit this to save your new quarto-publish.yml file. This is your GitHub Action.\nNext, we’ll create a new gh-pages branch. Go back to the main view of your GitHub repository. On the far left from the green “Code” button, click the button that says “main”. In the pull-down menu, type gh-pages - all lowercase, with a hyphen. Click the bold text that says “Create branch: gh-pages from main”.\nNow click on the Settings tab in the top right of your repository. On the left sidebar, click Pages. At the top of Pages under “Source”, select gh-pages root, and press Save. You’ll then see a green highlighted text saying that your site is published at a “github.io” url.",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#confirm",
    "href": "explore.html#confirm",
    "title": "Explore and setup",
    "section": "Confirm your website is published",
    "text": "Confirm your website is published\nTo confirm that your website is published, go back to your main repository page. You’ll now see an orange dot showing that the GitHub Action is beginning to publish the page.\n\n\n\nOur repo with orange dot indicating in-progress GitHub Action build\n\n\nIf you do not see this orange dot, you might need to make a small commit to trigger the GitHub Actions build. If this is the case, click the pencil on the top-right of the README.md file as circled in the image below, add some small edit (like a space after a period), and scroll down to click commit. Now you should see the orange dot.\n\n\n\n\n\nWhen your orange do becomes a green check, you can go inspect your published site at “https://username.github.io/your-repo). For example: https://openscapes.github.io/quarto-website-tutorial.\n\n\n\nOur repo with green check indicating successful GitHub Action build",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#renaming-your-repo",
    "href": "explore.html#renaming-your-repo",
    "title": "Explore and setup",
    "section": "Renaming your repo",
    "text": "Renaming your repo\nIf you’d like to rename your repo, go to Settings and the option to rename is on the top of the main settings page.",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "explore.html#onward",
    "href": "explore.html#onward",
    "title": "Explore and setup",
    "section": "Onward!",
    "text": "Onward!\nNow you are ready to start editing and publishing! The next chapter describes how starting off from the browser, using Markdown.",
    "crumbs": [
      "Explore and setup"
    ]
  },
  {
    "objectID": "statistic/KS-PSI_ENTROPY.html",
    "href": "statistic/KS-PSI_ENTROPY.html",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "",
    "text": "Chỉ số Ổn định tổng thể (PSI - Population Stability Index) là một phép đo thường được sử dụng trong ngành ngân hàng, đặc biệt trong việc xếp hạng tín dụng, để giám sát sự ổn định và hiệu suất của các mô hình xếp hạng theo thời gian. Đây là cách để định lượng sự thay đổi trong phân phối tổng thể, điều này có thể ảnh hưởng đến sức mạnh dự đoán của một mô hình.\n\nPSI được tính như sau:\n\\[ PSI = \\sum_{i=1}^{N} (Actual_{i} - Expected_{i}) \\log \\left( \\frac{Actual_{i}}{Expected_{i}} \\right) \\]\nTrong đó:\n\n\\(Actual_{i}\\) và \\(Expected_{i}\\) là tỷ lệ quan sát rơi vào bin (i) cho dữ liệu thực tế (mới hoặc kiểm tra) và dữ liệu mong đợi (cũ hoặc đào tạo), tương ứng. - (N) là tổng số bins.\n\nCách giải thích thông thường về các giá trị PSI như sau:\n\nPSI &lt; 0.1: Không có sự thay đổi đáng kể về tổng thể. Mô hình thường được coi là ổn định.\n0.1 ≤ PSI &lt; 0.25: Có một số thay đổi nhỏ trong tổng thể, có thể cần được điều tra thêm.\nPSI ≥ 0.25: Sự thay đổi đáng kể về tổng thể. Mô hình có thể không còn phù hợp và cần được cập nhật hoặc xây dựng lại.\n\nLưu ý rằng những ngưỡng này không phải là cố định và có thể thay đổi tùy thuộc vào đặc điểm cụ thể của tình huống và mức độ rủi ro bạn sẵn sàng chấp nhận.\n\nimport numpy as np\ndef calculate_psi(expected, actual, bins=10, categorical=False):\n    \"\"\"\n    Calculate the PSI (Population Stability Index) between expected and actual data.\n    \n    Args:\n    expected: numpy array of original values\n    actual: numpy array of new values, same size as expected\n    bins: number of bins to use in calculation, defaults to 10\n    categorical: boolean, if True indicates that the input variables are categorical\n    \n    # Example usage for categorical variables:\n    expected_categorical = np.random.choice(['A', 'B', 'C'], size=500, p=[0.4, 0.5, 0.1])\n    actual_categorical = np.random.choice(['A', 'B', 'C'], size=500, p=[0.42, 0.48, 0.1]) \n    \n    psi_value_categorical = calculate_psi(expected_categorical, actual_categorical, categorical=True)\n    psi_value_categorical\n    \n    Returns:\n    psi_value: calculated PSI value\n    \"\"\"\n\n    # Check if the variables are categorical\n    if categorical:\n        # Get unique categories\n        categories = np.unique(np.concatenate([expected, actual]))\n        \n        # Calculate the expected and actual proportions for each category\n        expected_probs = np.array([np.sum(expected == cat) for cat in categories]) / len(expected)\n        actual_probs = np.array([np.sum(actual == cat) for cat in categories]) / len(actual)\n    else:\n        # Define the bin edges for the histogram\n        bin_edges = np.histogram_bin_edges(expected, bins=bins)\n\n        # Calculate the expected and actual proportions for each bin\n        expected_probs, _ = np.histogram(expected, bins=bin_edges)\n        actual_probs, _ = np.histogram(actual, bins=bin_edges)\n\n        # Normalize to get proportions\n        expected_probs = expected_probs / len(expected)\n        actual_probs = actual_probs / len(actual)\n\n    # Initialize PSI\n    psi_value = 0\n\n    # Loop over each bin or category\n    for bin in range(len(expected_probs)):\n        # Avoid division by zero and log of zero\n        if expected_probs[bin] == 0 or actual_probs[bin] == 0:\n            continue\n        # Calculate the PSI for this bin or category\n        psi_value += (expected_probs[bin] - actual_probs[bin]) * np.log(expected_probs[bin] / actual_probs[bin])\n\n    return psi_value"
  },
  {
    "objectID": "statistic/KS-PSI_ENTROPY.html#psi",
    "href": "statistic/KS-PSI_ENTROPY.html#psi",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "",
    "text": "Chỉ số Ổn định tổng thể (PSI - Population Stability Index) là một phép đo thường được sử dụng trong ngành ngân hàng, đặc biệt trong việc xếp hạng tín dụng, để giám sát sự ổn định và hiệu suất của các mô hình xếp hạng theo thời gian. Đây là cách để định lượng sự thay đổi trong phân phối tổng thể, điều này có thể ảnh hưởng đến sức mạnh dự đoán của một mô hình.\n\nPSI được tính như sau:\n\\[ PSI = \\sum_{i=1}^{N} (Actual_{i} - Expected_{i}) \\log \\left( \\frac{Actual_{i}}{Expected_{i}} \\right) \\]\nTrong đó:\n\n\\(Actual_{i}\\) và \\(Expected_{i}\\) là tỷ lệ quan sát rơi vào bin (i) cho dữ liệu thực tế (mới hoặc kiểm tra) và dữ liệu mong đợi (cũ hoặc đào tạo), tương ứng. - (N) là tổng số bins.\n\nCách giải thích thông thường về các giá trị PSI như sau:\n\nPSI &lt; 0.1: Không có sự thay đổi đáng kể về tổng thể. Mô hình thường được coi là ổn định.\n0.1 ≤ PSI &lt; 0.25: Có một số thay đổi nhỏ trong tổng thể, có thể cần được điều tra thêm.\nPSI ≥ 0.25: Sự thay đổi đáng kể về tổng thể. Mô hình có thể không còn phù hợp và cần được cập nhật hoặc xây dựng lại.\n\nLưu ý rằng những ngưỡng này không phải là cố định và có thể thay đổi tùy thuộc vào đặc điểm cụ thể của tình huống và mức độ rủi ro bạn sẵn sàng chấp nhận.\n\nimport numpy as np\ndef calculate_psi(expected, actual, bins=10, categorical=False):\n    \"\"\"\n    Calculate the PSI (Population Stability Index) between expected and actual data.\n    \n    Args:\n    expected: numpy array of original values\n    actual: numpy array of new values, same size as expected\n    bins: number of bins to use in calculation, defaults to 10\n    categorical: boolean, if True indicates that the input variables are categorical\n    \n    # Example usage for categorical variables:\n    expected_categorical = np.random.choice(['A', 'B', 'C'], size=500, p=[0.4, 0.5, 0.1])\n    actual_categorical = np.random.choice(['A', 'B', 'C'], size=500, p=[0.42, 0.48, 0.1]) \n    \n    psi_value_categorical = calculate_psi(expected_categorical, actual_categorical, categorical=True)\n    psi_value_categorical\n    \n    Returns:\n    psi_value: calculated PSI value\n    \"\"\"\n\n    # Check if the variables are categorical\n    if categorical:\n        # Get unique categories\n        categories = np.unique(np.concatenate([expected, actual]))\n        \n        # Calculate the expected and actual proportions for each category\n        expected_probs = np.array([np.sum(expected == cat) for cat in categories]) / len(expected)\n        actual_probs = np.array([np.sum(actual == cat) for cat in categories]) / len(actual)\n    else:\n        # Define the bin edges for the histogram\n        bin_edges = np.histogram_bin_edges(expected, bins=bins)\n\n        # Calculate the expected and actual proportions for each bin\n        expected_probs, _ = np.histogram(expected, bins=bin_edges)\n        actual_probs, _ = np.histogram(actual, bins=bin_edges)\n\n        # Normalize to get proportions\n        expected_probs = expected_probs / len(expected)\n        actual_probs = actual_probs / len(actual)\n\n    # Initialize PSI\n    psi_value = 0\n\n    # Loop over each bin or category\n    for bin in range(len(expected_probs)):\n        # Avoid division by zero and log of zero\n        if expected_probs[bin] == 0 or actual_probs[bin] == 0:\n            continue\n        # Calculate the PSI for this bin or category\n        psi_value += (expected_probs[bin] - actual_probs[bin]) * np.log(expected_probs[bin] / actual_probs[bin])\n\n    return psi_value"
  },
  {
    "objectID": "statistic/KS-PSI_ENTROPY.html#ks-kolmogorov-smirnov",
    "href": "statistic/KS-PSI_ENTROPY.html#ks-kolmogorov-smirnov",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "KS (Kolmogorov-Smirnov)",
    "text": "KS (Kolmogorov-Smirnov)\n\nKiểm định KS (Kolmogorov-Smirnov) là một kiểm định thống kê phi tham số dùng để so sánh hai phân phối tích lũy (CDFs) hoặc một mẫu dữ liệu với một phân phối lý thuyết. Nó có hai ứng dụng chính:\n\nKiểm tra sự phù hợp của mẫu: Được sử dụng để kiểm tra xem một tập dữ liệu có tuân theo một phân phối lý thuyết cụ thể (như phân phối chuẩn, phân phối đều, v.v.) hay không.\nSo sánh hai mẫu dữ liệu: Được sử dụng để kiểm tra xem hai tập dữ liệu có xuất phát từ cùng một phân phối gốc hay không.\n\nCông thức tính toán cho chỉ số KS là:\n\\[ D = \\max |F_1(x) - F_2(x)| \\]\nTrong đó:\n\n$ F_1(x) $ và $F_2(x) $ là hai hàm phân phối tích lũy cần so sánh.\nD là giá trị lớn nhất của sự khác biệt tuyệt đối giữa hai hàm phân phối tích lũy trên toàn bộ phạm vi x.\n\nMột đặc điểm quan trọng của kiểm định KS là nó không yêu cầu giả định về dạng của phân phối, làm cho nó trở thành một công cụ mạnh mẽ và linh hoạt khi so sánh phân phối.\n\nfrom scipy.stats import ks_2samp\n\n# Generate two sample datasets\ndata1 = np.random.normal(0, 1, 1000)\ndata2 = np.random.normal(0.5, 1.5, 1000)\n\n# Compute the KS statistic and p-value\nks_statistic, p_value = ks_2samp(data1, data2)\n\nks_statistic, p_value\n\n(np.float64(0.214), np.float64(1.8518210519255763e-20))"
  },
  {
    "objectID": "statistic/KS-PSI_ENTROPY.html#divergence-test-cross-entropy",
    "href": "statistic/KS-PSI_ENTROPY.html#divergence-test-cross-entropy",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "Divergence Test (cross-entropy)",
    "text": "Divergence Test (cross-entropy)\n\nKiểm định Divergence, thường được gọi là Divergence Kullback-Leibler (KL), là một chỉ số đo sự khác biệt giữa một phân phối xác suất so với một phân phối xác suất thứ hai mong đợi. Nó được sử dụng để so sánh hai phân phối xác suất cho cùng một sự kiện.\nCho hai phân phối xác suất, P và Q, Divergence Kullback-Leibler của Q so với P được định nghĩa như sau:\n\\[ D_{KL}(P||Q) = \\sum_{i} P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)\\]\nTrong đó:\n\n\\(P(i)\\) là xác suất của sự kiện i theo phân phối P,\n\\(Q(i)\\) là xác suất của sự kiện i theo phân phối Q,\nTổng được tính trên tất cả các sự kiện i có thể xảy ra.\n\nMột số điểm quan trọng về KL Divergence:\n\nKhông đối xứng: \\(D_{KL}(P||Q)\\) không bằng \\(D_{KL}(Q||P)\\). Điều này có nghĩa là Divergence KL của Q so với P không giống như Divergence KL của P so với Q.\nKhông âm: Divergence KL luôn không âm, và nó bằng không chỉ khi P và Q là cùng một phân phối.\nĐơn vị: Divergence KL được đo bằng bit nếu logarithm có cơ số 2 (log2), hoặc bằng nats nếu logarithm có cơ số e (logarithm tự nhiên).\n\nTrên thực tế, KL Divergence có thể được sử dụng để đo sự khác biệt giữa phân phối thực tế và dự đoán, hoặc giữa một phân phối quan sát và một phân phối lý thuyết. Nó đặc biệt phổ biến trong các lĩnh vực như lý thuyết thông tin và học máy.\n\nimport numpy as np\n\ndef kl_divergence(p, q):\n    \"\"\"Compute KL divergence of two probability distributions.\"\"\"\n    return np.sum(p * np.log(p / q))\n\n# Example distributions\np = np.array([0.4, 0.5, 0.1])\nq = np.array([0.3, 0.4, 0.3])\n\n# Ensure the distributions are valid (i.e., sum to 1 and non-negative)\nassert np.all(p &gt;= 0) and np.all(q &gt;= 0)\nassert np.isclose(p.sum(), 1) and np.isclose(q.sum(), 1)\n\n# Calculate KL Divergence\ndivergence_value = kl_divergence(p, q)\nprint(f\"KL Divergence between p and q: {divergence_value:.4f}\")\n\nKL Divergence between p and q: 0.1168\n\n\n\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef kl_divergence(p, q):\n    \"\"\"Compute KL divergence of two probability distributions.\"\"\"\n    return entropy(p, q)\n\n# Example distributions\np = np.array([0.4, 0.5, 0.1])\nq = np.array([0.3, 0.4, 0.3])\n\n# Calculate KL Divergence from p to q\ndivergence_value = kl_divergence(p, q)\n\nprint(f\"Cross entropy: {divergence_value:.4f}\")\n\nCross entropy: 0.1168"
  },
  {
    "objectID": "statistic/KS-PSI_ENTROPY.html#so-sánh-ks-và-divergence",
    "href": "statistic/KS-PSI_ENTROPY.html#so-sánh-ks-và-divergence",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "So sánh KS và Divergence",
    "text": "So sánh KS và Divergence\n\nMục đích:\n\nKiểm định KS: Đây là một kiểm định phi tham số được sử dụng để xác định xem hai mẫu có xuất phát từ cùng một phân phối hay không. Thống kê KS đo sự khác biệt lớn nhất giữa các hàm phân phối tích lũy (CDFs) của hai mẫu.\nKiểm định Divergence (KL Divergence): Nó đo cách một phân phối xác suất khác biệt so với một phân phối xác suất thứ hai mong đợi. Nó thường được sử dụng trong lý thuyết thông tin để đo “khoảng cách” giữa hai phân phối.\n\nKết quả:\n\nKiểm định KS: Kết quả là một chỉ số (D) đại diện cho sự khác biệt lớn nhất giữa hai CDFs và một giá trị p kiểm tra giả thuyết rằng hai mẫu được rút ra từ cùng một phân phối.\nKiểm định Divergence (KL Divergence): Kết quả là một giá trị không âm, trong đó giá trị 0 chỉ ra rằng hai phân phối là giống nhau. Lưu ý rằng KL Divergence không đối xứng, tức là \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\).\n\nGiả định:\n\nKiểm định KS: Không giả định về phân phối của dữ liệu.\nKiểm định Divergence (KL Divergence): Giả định \\(Q(i) &gt; 0\\) cho bất kỳ i nào sao cho \\(P(i) &gt; 0\\), nếu không sự khác biệt sẽ vô cùng.\n\nỨng dụng:\n\nKiểm định KS: Thường được sử dụng trong kiểm định giả thuyết để xác định xem một mẫu dữ liệu có tuân theo một phân phối cụ thể hay không.\nKiểm định Divergence (KL Divergence): Rộng rãi được sử dụng trong lý thuyết thông tin, học máy và thống kê, đặc biệt khi so sánh một phân phối thực nghiệm với một phân phối lý thuyết.\n\nGiải thích:\n\nKiểm định KS: Một giá trị p nhỏ cho thấy rằng hai mẫu đến từ các phân phối khác nhau.\nKiểm định Divergence (KL Divergence): Một Divergence KL lớn hơn chỉ ra rằng hai phân phối khác biệt hơn so với nhau.\n\n\nTóm lại, mặc dù cả Kiểm định KS và KL Divergence đều được sử dụng để so sánh các phân phối, nhưng chúng có các phương pháp, giải thích và ứng dụng khác nhau. Sự lựa chọn giữa chúng phụ thuộc vào vấn đề cụ thể và bản chất của dữ liệu."
  },
  {
    "objectID": "statistic/Wine_Quality_Monotonic_Binning.html",
    "href": "statistic/Wine_Quality_Monotonic_Binning.html",
    "title": "Monotonic Trend with Wine Quality Dataset",
    "section": "",
    "text": "Monotonic Trend (xu hướng đơn điệu) là một khái niệm quan trọng trong quá trình xử lý dữ liệu và mô hình hóa, đặc biệt khi làm việc với các biến mục tiêu có thứ tự (ordinal target). Trong bài viết này, chúng ta sẽ sử dụng dữ liệu Wine Quality từ thư viện seaborn để minh họa cách áp dụng OptimalBinning với ràng buộc monotonic.",
    "crumbs": [
      "Statistic",
      "Monotonic Trend with Wine Quality Dataset"
    ]
  },
  {
    "objectID": "statistic/Wine_Quality_Monotonic_Binning.html#transform-data-and-review-results",
    "href": "statistic/Wine_Quality_Monotonic_Binning.html#transform-data-and-review-results",
    "title": "Monotonic Trend with Wine Quality Dataset",
    "section": "Transform Data and Review Results",
    "text": "Transform Data and Review Results\n\n# Transform alcohol feature into bins\nX_train_check = X_train.copy()\nX_train_check[f'{variable}_bins'] = optb.transform(X_train[variable], metric='bins')\n\n# Display the binning table\nbinning_table = optb.binning_table\nbinning_table.build()\n\n\n\n\n\n\n\n\nBin\nCount\nCount (%)\nEvent_0\nEvent_1\nEvent_2\nEvent_3\nEvent_4\nEvent_5\nEvent_rate_0\nEvent_rate_1\nEvent_rate_2\nEvent_rate_3\nEvent_rate_4\nEvent_rate_5\n\n\n\n\n0\n(-inf, 7.85)\n691\n0.480195\n5\n28\n316\n275\n60\n7\n0.007236\n0.040521\n0.457308\n0.397974\n0.086831\n0.010130\n\n\n1\n[7.85, inf)\n748\n0.519805\n4\n20\n297\n299\n119\n9\n0.005348\n0.026738\n0.397059\n0.399733\n0.159091\n0.012032\n\n\n2\nSpecial\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n3\nMissing\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nTotals\n\n1439\n1.000000\n9\n48\n613\n574\n179\n16\n0.006254\n0.033356\n0.425990\n0.398888\n0.124392\n0.011119\n\n\n\n\n\n\n\n\ntype(binning_table)\n\noptbinning.binning.binning_statistics.MulticlassBinningTable\n\n\n\nPhân tích\n\nDữ liệu đã được chia thành 3 nhóm (bins) dựa trên giá trị của alcohol.\nChất lượng trung bình (event mean) tăng dần theo các nhóm, thể hiện mối quan hệ xu hướng đơn điệu tăng dần (monotonic ascending).\nĐiều này phù hợp với giả thuyết rằng hàm lượng cồn cao hơn thường dẫn đến chất lượng rượu tốt hơn.\n\n\n\n\nTại sao cần Monotonic Trend?\n\nTuân theo kiến thức miền: Ví dụ, rượu vang có hàm lượng cồn cao thường được đánh giá chất lượng cao hơn.\nGiúp mô hình dễ diễn giải hơn: Kết quả binned đảm bảo logic và phù hợp với thực tế.\nHỗ trợ các mô hình tuyến tính: Ràng buộc monotonic giúp duy trì tính tuyến tính và thứ tự trong phân tích và dự báo.\n\n\n\n\nĐiều chỉnh và Ứng dụng\n\nThay đổi tham số: Bạn có thể điều chỉnh min_prebin_size, min_n_bins, và max_n_bins để kiểm soát số lượng và kích thước nhóm.\nỨng dụng thực tế:\n\nTrong mô hình hóa rủi ro tài chính, áp dụng monotonic trend để đảm bảo rằng các biến số như thu nhập, dư nợ có quan hệ hợp lý với mức độ rủi ro.\nTrong phân tích tiếp thị, đảm bảo rằng các chỉ số như mức chi tiêu tương quan với mức độ trung thành của khách hàng.\n\n\n\n\n\nKết luận\nRàng buộc monotonic trend không chỉ giúp cải thiện tính hợp lý của việc chia nhóm dữ liệu mà còn tăng tính diễn giải của mô hình. Dữ liệu Wine Quality minh họa rõ cách áp dụng monotonic trend để xử lý biến mục tiêu có thứ tự và duy trì mối quan hệ logic với các đặc trưng đầu vào.\n\nimport pandas as pd\nfrom optbinning import MulticlassOptimalBinning\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split\nimport logging\n\n# Configure logging for better visibility\nlogging.basicConfig(level=logging.INFO)\n\n# Custom class that inherits from BaseEstimator and TransformerMixin\nclass MulticlassBinningTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, monotonic_trend='auto', max_n_bins=5, min_prebin_size=0.05):\n        \"\"\"\n        Initialize the MulticlassOptimalBinning object.\n\n        Parameters:\n        monotonic_trend : str, list of str or None, default 'auto'\n            Specifies the trend assumption for the binning. If it's a string, \n            it applies to all features. If it's a list, each feature will get\n            its corresponding monotonic trend.\n        max_n_bins : int, default 5\n            Maximum number of bins to use.\n        min_prebin_size : float, default 0.05\n            Minimum percentage of data required in each pre-bin.\n        \"\"\"\n        self.monotonic_trend = monotonic_trend   \n        self.max_n_bins = max_n_bins\n        self.min_prebin_size = min_prebin_size\n        self.binning_process = {}   # Initialize an empty dictionary to store binning objects\n    \n    def fit(self, X, y):\n        \"\"\"\n        Fit the binning model on the provided feature(s) X and target y.\n        \n        Parameters:\n        X : pd.DataFrame\n            The feature(s) used for binning.\n        y : array-like\n            The target variable (multiclass).\n        \n        Returns:\n        self : object\n            Fitted transformer.\n        \"\"\"\n        # Separate features into categorical and numerical features\n        self.feats = X.columns.tolist()\n        self.cat_feats = X.select_dtypes(include=['object']).columns.tolist()\n        self.num_feats = [feat for feat in self.feats if feat not in self.cat_feats]\n        \n        logging.info(f'Processing Information Value, Total features: {len(self.feats)}. '\n                     f'Categorical features: {len(self.cat_feats)}. '\n                     f'Numeric features: {len(self.num_feats)}')\n\n        # Fit the binning model for each numerical feature\n        for feat in self.num_feats:\n            optb = MulticlassOptimalBinning(\n                monotonic_trend=self.monotonic_trend,\n                max_n_bins=self.max_n_bins,\n                min_prebin_size=self.min_prebin_size\n            )\n            logging.info(f'Fitting binning for feature: {feat}')\n            self.binning_process[feat] = optb.fit(X[feat], y)  # Store the fitted binning object\n        \n        return self\n    \n    def transform(self, X, metric='mean_woe'):\n        \"\"\"\n        Transform the features using the learned binning model.\n        \n        Parameters:\n        X : pd.DataFrame\n            The feature(s) to transform.\n        metric : str, default='mean_woe'\n            The transformation metric, either 'bin' (binned values) or 'woe' (Weight of Evidence).\n        \n        Returns:\n        transformed_X : pd.DataFrame\n            The transformed feature(s) using the specified metric.\n        \"\"\"\n        transformed_data = pd.DataFrame()\n\n        # Transform each feature separately using the learned binning model\n        for feat in self.num_feats:\n            transformed_data[feat] = self.binning_process[feat].transform(X[feat], metric=metric)\n        \n        return transformed_data\n\n    def get_binning_table(self):\n        \"\"\"\n        Extract the binning details for each numerical feature after fitting the model.\n        \n        Parameters:\n        X : pd.DataFrame\n            The feature(s) to extract binning details for.\n        \n        Returns:\n        binning_process : pd.DataFrame\n            A DataFrame with binning details (bin edges, WOE, etc.) for each feature.\n        \"\"\"\n        binning_tables = []\n        for feat in self.num_feats:\n            # Access the binning information for each feature\n            tbl = self.binning_process[feat].binning_table.build()  # Get binning table\n            tbl['Feature Name'] = feat  # Add feature name for identification\n            binning_tables.append(tbl)  # Add the binning table to the list\n\n        # Concatenate all binning tables into a single DataFrame\n        all_binning_tables = pd.concat(binning_tables, ignore_index=True)\n        \n        return all_binning_tables\n\n# Create an instance of the custom transformer\nbinning_transformer = MulticlassBinningTransformer()\n\n# Fit the binning transformer on the train set\nbinning_transformer.fit(X_train, y_train)\n\n# Transform the features (train and test sets) into their binned representation\nX_train_binned = binning_transformer.transform(X_train, metric='mean_woe')\nX_test_binned = binning_transformer.transform(X_test, metric='mean_woe')\n\n# Transform the features into their WOE values (train and test sets)\nX_train_woe = binning_transformer.transform(X_train, metric='mean_woe')\nX_test_woe = binning_transformer.transform(X_test, metric='mean_woe')\n\n\n# Print transformed results\nprint(\"Transformed (Binned) values for 'fixed acidity' - Train set:\")\nprint(X_train_binned)\nprint(\"Transformed (WOE) values for 'fixed acidity' - Train set:\")\nprint(X_train_woe)\n\nINFO:root:Processing Information Value, Total features: 11. Categorical features: 0. Numeric features: 11\nINFO:root:Fitting binning for feature: fixed acidity\nINFO:root:Fitting binning for feature: volatile acidity\nINFO:root:Fitting binning for feature: citric acid\nINFO:root:Fitting binning for feature: residual sugar\nINFO:root:Fitting binning for feature: chlorides\nINFO:root:Fitting binning for feature: free sulfur dioxide\nINFO:root:Fitting binning for feature: total sulfur dioxide\nINFO:root:Fitting binning for feature: density\nINFO:root:Fitting binning for feature: pH\nINFO:root:Fitting binning for feature: sulphates\nINFO:root:Fitting binning for feature: alcohol\n\n\nTransformed (Binned) values for 'fixed acidity' - Train set:\n      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0          0.022531          0.149226     0.020826   -1.850372e-17  -0.250851   \n1          0.334963          0.149226     0.020826   -1.850372e-17   0.250514   \n2          0.022531          0.149226     0.020826   -1.850372e-17   0.138367   \n3          0.022531          0.149226     0.020826   -1.850372e-17   0.250514   \n4          0.024204          0.114208     0.020826   -1.850372e-17   0.074211   \n...             ...               ...          ...             ...        ...   \n1434       0.022531          0.114208     0.020826   -1.850372e-17   0.138367   \n1435       0.334963          0.149226     0.020826   -1.850372e-17   0.250514   \n1436       0.022531          0.149226     0.020826   -1.850372e-17   0.138367   \n1437       0.022531          0.149226     0.020826   -1.850372e-17   0.138367   \n1438       0.022531          0.149226     0.020826   -1.850372e-17   0.074211   \n\n      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n0                0.220624         -1.850372e-17 -0.090830  0.105966   \n1                0.220624         -1.850372e-17  0.140992  0.027339   \n2                0.220624         -1.850372e-17  0.140992  0.105966   \n3                0.220624         -1.850372e-17  0.140992  0.027339   \n4                0.220624         -1.850372e-17 -0.090830  0.027339   \n...                   ...                   ...       ...       ...   \n1434             0.220624         -1.850372e-17  0.216215  0.105966   \n1435             0.220624         -1.850372e-17  0.140992  0.105966   \n1436             0.220624         -1.850372e-17  0.140992  0.027339   \n1437            -0.380540         -1.850372e-17  0.140992  0.105966   \n1438             0.220624         -1.850372e-17 -0.090830  0.105966   \n\n      sulphates   alcohol  \n0      0.246809 -0.098624  \n1      0.349307  0.527039  \n2      0.349307  0.527039  \n3      0.246809  0.527039  \n4      0.246809 -0.098624  \n...         ...       ...  \n1434   0.349307  0.527039  \n1435   0.349307  0.527039  \n1436   0.349307  0.527039  \n1437   0.246809 -0.098624  \n1438   0.246809 -0.098624  \n\n[1439 rows x 11 columns]\nTransformed (WOE) values for 'fixed acidity' - Train set:\n      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0          0.022531          0.149226     0.020826   -1.850372e-17  -0.250851   \n1          0.334963          0.149226     0.020826   -1.850372e-17   0.250514   \n2          0.022531          0.149226     0.020826   -1.850372e-17   0.138367   \n3          0.022531          0.149226     0.020826   -1.850372e-17   0.250514   \n4          0.024204          0.114208     0.020826   -1.850372e-17   0.074211   \n...             ...               ...          ...             ...        ...   \n1434       0.022531          0.114208     0.020826   -1.850372e-17   0.138367   \n1435       0.334963          0.149226     0.020826   -1.850372e-17   0.250514   \n1436       0.022531          0.149226     0.020826   -1.850372e-17   0.138367   \n1437       0.022531          0.149226     0.020826   -1.850372e-17   0.138367   \n1438       0.022531          0.149226     0.020826   -1.850372e-17   0.074211   \n\n      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n0                0.220624         -1.850372e-17 -0.090830  0.105966   \n1                0.220624         -1.850372e-17  0.140992  0.027339   \n2                0.220624         -1.850372e-17  0.140992  0.105966   \n3                0.220624         -1.850372e-17  0.140992  0.027339   \n4                0.220624         -1.850372e-17 -0.090830  0.027339   \n...                   ...                   ...       ...       ...   \n1434             0.220624         -1.850372e-17  0.216215  0.105966   \n1435             0.220624         -1.850372e-17  0.140992  0.105966   \n1436             0.220624         -1.850372e-17  0.140992  0.027339   \n1437            -0.380540         -1.850372e-17  0.140992  0.105966   \n1438             0.220624         -1.850372e-17 -0.090830  0.105966   \n\n      sulphates   alcohol  \n0      0.246809 -0.098624  \n1      0.349307  0.527039  \n2      0.349307  0.527039  \n3      0.246809  0.527039  \n4      0.246809 -0.098624  \n...         ...       ...  \n1434   0.349307  0.527039  \n1435   0.349307  0.527039  \n1436   0.349307  0.527039  \n1437   0.246809 -0.098624  \n1438   0.246809 -0.098624  \n\n[1439 rows x 11 columns]\n\n\n\nbinning_tables = binning_transformer.get_binning_table()\n\n\nfrom optbinning import BinningProcess\nimport logging\nfrom sklearn.base import TransformerMixin, BaseEstimator\n# default use ContinuousOptimalBinning\nclass OrdinalBinning(BaseEstimator, TransformerMixin):\n    def __init__(self, monotonic_trend='auto_asc_desc', max_n_bins=5, min_prebin_size = 0.05):  \n        self.monotonic_trend = monotonic_trend   \n        self.max_n_bins = max_n_bins\n        self.min_prebin_size = min_prebin_size\n        self.binning_process = None   \n\n    def fit(self, X, y):        \n        self.feats = X.columns.tolist()\n        self.cat_feats = X.select_dtypes(include=['object']).columns.tolist()          \n        # self.num_feats = list(set(self.feats) - set(self.cat_feats))\n        self.num_feats = [item for item in self.feats if item not in self.cat_feats]\n        logging.info('Processing Information Value, Total features {}. Categorical features {}. Numeric features {}'.format(len(self.feats), len(self.cat_feats), len(self.num_feats)))\n        # Set the binning fit parameters for each feature\n        _binning_fit_params = {}\n        for fs in self.feats:\n            _binning_fit_params[fs] = {'monotonic_trend':self.monotonic_trend,\n                                'max_n_bins': self.max_n_bins,\n                                'min_prebin_size':self.min_prebin_size,\n                                'solver': 'mip'}\n            \n        # Initialize the BinningProcess with the specified variables and fit parameters    \n        _binning_process  = BinningProcess(variable_names = self.feats                                                                    \n                                    , binning_fit_params=_binning_fit_params)\n        \n        # Transform the training data using the BinningProcess\n        _binning_process.fit(X, y) \n        self.binning_process = _binning_process        \n        \n\n        return self \n    \n    def transform(self, X):\n        # Transform and convert to DataFrame with original column names\n        X_binned = self.binning_process.transform(X)\n        return pd.DataFrame(X_binned, columns=X.columns, index=X.index)\n    \n    def get_feature_names_in(self):\n        return self.feats\n\n    def get_feature_names_out(self):\n        return self.feats\n    \n    def get_summary(self):\n        # Get the BinningProcess object from the fitted pipeline\n        binning_process = self.binning_process\n\n        # Initialize an empty list to hold binning tables for each variable\n        binning_tables = []\n\n        # Iterate through the binned variables and build binning tables\n        binned_variables = binning_process.variable_names\n        for variable in binned_variables:\n            print(f\"Processing binning for: {variable}\")\n            \n            # Get the binning object for each variable\n            binning_object = binning_process.get_binned_variable(variable)\n            \n            # Build the binning table for the current variable\n            binning_table = binning_object.binning_table.build()\n            \n            # Add the variable name as a column to identify the source column\n            binning_table['Feature Name'] = variable\n            \n            # Append the binning table to the list\n            binning_tables.append(binning_table)\n\n        # Concatenate all binning tables into a single DataFrame\n        all_binning_tables = pd.concat(binning_tables, ignore_index=True)\n        \n        return all_binning_tables\n\n\nordinal_binning = OrdinalBinning(monotonic_trend='auto', max_n_bins=10, min_prebin_size=0.05)\n# Fit and transform data\nordinal_binning.fit(X_train, y_train)\nX_train_transformed = ordinal_binning.transform(X_train)\nX_test_transformed = ordinal_binning.transform(X_test)\n\nINFO:root:Processing Information Value, Total features 11. Categorical features 0. Numeric features 11\n\n\n\nbinning_tables = ordinal_binning.get_summary()\n\nProcessing binning for: fixed acidity\nProcessing binning for: volatile acidity\nProcessing binning for: citric acid\nProcessing binning for: residual sugar\nProcessing binning for: chlorides\nProcessing binning for: free sulfur dioxide\nProcessing binning for: total sulfur dioxide\nProcessing binning for: density\nProcessing binning for: pH\nProcessing binning for: sulphates\nProcessing binning for: alcohol\n\n\n\nimport os\nfor fs in ordinal_binning.binning_process.summary().name:\n    binning_table = ordinal_binning.binning_process.get_binned_variable(fs).binning_table\n    # Save the plot as an image file in the export folder\n    export_path = os.path.join('D:tmp/', f'WoE/{fs}_binning_plot.png')\n    binning_table.plot(show_bin_labels=True, add_special=False, add_missing=False, savefig = export_path)\n\n\nbinning_table.plot(show_bin_labels=True, add_special=False, add_missing=False)\n\n\n\n\n\n\n\n\n\nbinning_table.build()\n\n\n\n\n\n\n\n\nBin\nCount\nCount (%)\nEvent_0\nEvent_1\nEvent_2\nEvent_3\nEvent_4\nEvent_5\nEvent_rate_0\nEvent_rate_1\nEvent_rate_2\nEvent_rate_3\nEvent_rate_4\nEvent_rate_5\n\n\n\n\n0\n(-inf, 10.15)\n712\n0.494788\n5\n25\n449\n213\n19\n1\n0.007022\n0.035112\n0.630618\n0.299157\n0.026685\n0.001404\n\n\n1\n[10.15, inf)\n727\n0.505212\n4\n23\n164\n361\n160\n15\n0.005502\n0.031637\n0.225585\n0.496561\n0.220083\n0.020633\n\n\n2\nSpecial\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n3\nMissing\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nTotals\n\n1439\n1.000000\n9\n48\n613\n574\n179\n16\n0.006254\n0.033356\n0.425990\n0.398888\n0.124392\n0.011119\n\n\n\n\n\n\n\n\nbinning_table.analysis()\n\n-------------------------------------------------\nOptimalBinning: Multiclass Binning Table Analysis\n-------------------------------------------------\n\n  General metrics\n\n    JS (Jensen-Shannon)      0.13742779\n    HHI                      0.50005433\n    HHI (normalized)         0.33340577\n    Cramer's V               0.45204457\n    Quality score            0.20904416\n\n  Monotonic trend\n\n    Class  0                 descending\n    Class  1                 descending\n    Class  2                 descending\n    Class  3                  ascending\n    Class  4                  ascending\n    Class  5                  ascending\n\n  Significance tests\n\n    Bin A  Bin B  t-statistic      p-value\n        0      1   294.051433 1.902916e-61\n\n\n\n\ntype(binning_table)\n\noptbinning.binning.binning_statistics.MulticlassBinningTable",
    "crumbs": [
      "Statistic",
      "Monotonic Trend with Wine Quality Dataset"
    ]
  },
  {
    "objectID": "statistic/ordinal_regression.html",
    "href": "statistic/ordinal_regression.html",
    "title": "Mô Hình Hồi Quy Logistic Thứ Tự",
    "section": "",
    "text": "1. Mô Hình Hồi Quy Logistic Thứ Tự\n\nKhái niệm:\n\nMô hình hồi quy logistic thứ tự là gì?: Hồi quy logistic thứ tự là một phương pháp phân tích thống kê được sử dụng khi biến phụ thuộc có thứ tự, nghĩa là các giá trị của biến phụ thuộc có một thứ tự tự nhiên nhưng không nhất thiết có khoảng cách đều nhau giữa các giá trị. Ví dụ về biến phụ thuộc có thứ tự bao gồm mức độ hài lòng của khách hàng (rất không hài lòng, không hài lòng, trung lập, hài lòng, rất hài lòng) hoặc trình độ học vấn (tiểu học, trung học, đại học).\nVí dụ: Giả sử chúng ta muốn nghiên cứu các yếu tố ảnh hưởng đến mức độ hài lòng của khách hàng sử dụng dịch vụ của một công ty. Biến phụ thuộc là mức độ hài lòng của khách hàng với 5 mức độ: “Rất không hài lòng”, “Không hài lòng”, “Trung lập”, “Hài lòng”, “Rất hài lòng”.\n\n\n\n\n2. Mô Hình Hồi Quy Logistic Thứ Tự\n\nCông thức mô hình:\n\nTrình bày công thức toán học của mô hình hồi quy logistic thứ tự:\n\\[ \\log\\left(\\frac{\\pi_{ij}}{1 - \\pi_{ij}}\\right) = \\alpha_j + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_p x_{ip} \\]\nTrong đó:\n\n\\(\\pi_{ij} = P(Y_i \\leq j)\\) là xác suất tích lũy của mức độ \\(j\\) hoặc thấp hơn.\n\\(\\alpha_j\\) là các ngưỡng (cutpoints) cho các mức độ khác nhau.\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) là các hệ số hồi quy.\n\n\n\n\n\nSo sánh Mô hình Ordinal Logit, Multinomial Logit và Binary Logit\n\n1. Mô hình Binary Logit\n\nĐặc điểm:\n\nSử dụng khi biến phụ thuộc chỉ có hai nhóm phân loại.\nMô hình tính toán xác suất thành công (1) so với thất bại (0).\nCông thức mô hình: \\[ \\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p \\]\nXác suất thành công được tính bằng: \\[ P(Y=1|X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}} \\]\n\n\n\n\n2. Mô hình Multinomial Logit\n\nĐặc điểm:\n\nSử dụng khi biến phụ thuộc có ba hoặc nhiều hơn các nhóm phân loại không có thứ tự.\nMô hình tính toán xác suất cho từng nhóm so với một nhóm tham chiếu.\nCông thức mô hình: \\[ \\log\\left(\\frac{P(Y=k)}{P(Y=K)}\\right) = \\beta_{k0} + \\beta_{k1} X_1 + \\ldots + \\beta_{kp} X_p \\]\nXác suất cho mỗi nhóm được tính bằng: \\[ P(Y=k|X) = \\frac{e^{\\beta_{k0} + \\beta_{k1} X_1 + \\ldots + \\beta_{kp} X_p}}{1 + \\sum_{j=1}^{K-1} e^{\\beta_{j0} + \\beta_{j1} X_1 + \\ldots + \\beta_{jp} X_p}} \\]\n\n\n\n\n3. Mô hình Ordinal Logit\n\nĐặc điểm:\n\nSử dụng khi biến phụ thuộc có nhiều nhóm phân loại có thứ tự.\nMô hình tính toán xác suất cho mỗi nhóm phân loại dựa trên thứ tự.\nCông thức mô hình: \\[ \\log\\left(\\frac{P(Y \\leq j)}{P(Y &gt; j)}\\right) = \\alpha_j + \\beta_1 X_1 + \\ldots + \\beta_p X_p \\]\nXác suất cho mỗi nhóm phân loại được tính bằng: \\[ P(Y = j|X) = P(Y \\leq j|X) - P(Y \\leq j-1|X) \\]\n\n\n\n\nSo sánh:\n\nSố lượng nhóm phân loại:\n\nBinary Logit: 2 nhóm.\nMultinomial Logit: 3 hoặc nhiều nhóm không có thứ tự.\nOrdinal Logit: 3 hoặc nhiều nhóm có thứ tự.\n\nCách tính xác suất:\n\nBinary Logit tính toán xác suất thành công so với thất bại.\nMultinomial Logit tính toán xác suất cho từng nhóm so với một nhóm tham chiếu.\nOrdinal Logit tính toán xác suất cho mỗi nhóm phân loại dựa trên thứ tự.\n\nĐặc điểm dữ liệu:\n\nBinary Logit thường được sử dụng cho các biến phụ thuộc có hai trạng thái như có hoặc không.\nMultinomial Logit được sử dụng khi biến phụ thuộc có ba hoặc nhiều hơn các nhóm không có thứ tự.\nOrdinal Logit thích hợp khi biến phụ thuộc có nhiều nhóm có thứ tự.\n\n\n\n\n\nPrediction\n\nMô hình Logit\nXác suất của một quan sát cho $ y_j = i $ là: \\[ p_{ij} = \\Pr(y_j = i) = \\Pr(\\kappa_{i-1} &lt; x_j \\beta + u \\leq \\kappa_i)  = \\frac{1}{1 + \\exp(-\\kappa_i + x_j \\beta)} - \\frac{1}{1 + \\exp(-\\kappa_{i-1} + x_j \\beta)} \\]\nTrong đó: - $ _0 $ được định nghĩa là $ -$ và $ _k $ là $ +$.\n\n\nMô hình Probit\nXác suất của một quan sát cho $ y_j = i $ là: \\[ p_{ij} = \\Pr(y_j = i) = \\Pr(\\kappa_{i-1} &lt; x_j \\beta + u \\leq \\kappa_i) = = \\Phi(\\kappa_i - x_j \\beta) - \\Phi(\\kappa_{i-1} - x_j \\beta) \\]\nTrong đó: - $ () $ là hàm phân phối tích lũy chuẩn (standard normal cumulative distribution function).\n\n\nHàm Log-Likelihood\nHàm log-likelihood là: \\[ \\ln L = \\sum_{j=1}^N Wj \\sum_{i=1}^k l_i(y_j) \\ln p_{ij} \\]\nTrong đó: - $ w_j $ là một trọng số tùy chọn. - $ l_i(y_j) $ : \\[\n  l_i(y_j) =\n  \\begin{cases}\n  1 & \\text{nếu } y_j = i \\\\\n  0 & \\text{ngược lại}\n  \\end{cases}\n  \\]\nNguồn: rologit\n\n\n\nCách giải thích kết quả của mô hình ordinal logistic\nĐể minh họa cách giải thích kết quả của mô hình ordinal logistic, hãy xem xét một ví dụ cụ thể. Giả sử chúng ta đã chạy một mô hình ordinal logistic để dự đoán mức độ hài lòng của khách hàng (1: Rất không hài lòng, 2: Không hài lòng, 3: Bình thường, 4: Hài lòng, 5: Rất hài lòng) dựa trên hai biến độc lập: thời gian chờ đợi (WaitTime) và chất lượng dịch vụ (ServiceQuality).\nDưới đây là kết quả từ mô hình ordinal logistic:\n                            Coefficient    Std. Error     Z-value    P-value\nIntercept 1 (cutpoint 1)        -2.50          0.30       -8.33      &lt;0.001\nIntercept 2 (cutpoint 2)        -1.50          0.25       -6.00      &lt;0.001\nIntercept 3 (cutpoint 3)        -0.50          0.20       -2.50       0.012\nIntercept 4 (cutpoint 4)         1.00          0.15        6.67      &lt;0.001\nWaitTime                        -0.75          0.10       -7.50      &lt;0.001\nServiceQuality                   1.20          0.08       15.00      &lt;0.001\n\n\nGiải thích Kết quả\n\n1. Các Hệ số Cắt (Cutpoints)\n\nIntercept 1 (cutpoint 1) = -2.50:\n\nĐây là điểm cắt giữa mức độ hài lòng “Rất không hài lòng” (1) và “Không hài lòng hoặc cao hơn” (2, 3, 4, 5).\nGiá trị âm cho thấy rằng với giá trị trung bình của các biến độc lập (WaitTime và ServiceQuality), khả năng là khách hàng sẽ không hài lòng hơn là rất không hài lòng.\n\nIntercept 2 (cutpoint 2) = -1.50:\n\nĐây là điểm cắt giữa mức độ hài lòng “Không hài lòng” (2) và “Bình thường hoặc cao hơn” (3, 4, 5).\nGiá trị âm nhưng lớn hơn -2.50, cho thấy rằng với giá trị trung bình của các biến độc lập, khả năng là khách hàng sẽ bình thường hoặc cao hơn là không hài lòng.\n\nIntercept 3 (cutpoint 3) = -0.50:\n\nĐây là điểm cắt giữa mức độ hài lòng “Bình thường” (3) và “Hài lòng hoặc cao hơn” (4, 5).\nGiá trị gần bằng 0 cho thấy rằng với giá trị trung bình của các biến độc lập, khả năng là khách hàng sẽ hài lòng hoặc cao hơn là bình thường.\n\nIntercept 4 (cutpoint 4) = 1.00:\n\nĐây là điểm cắt giữa mức độ hài lòng “Hài lòng” (4) và “Rất hài lòng” (5).\nGiá trị dương cho thấy rằng với giá trị trung bình của các biến độc lập, khả năng là khách hàng sẽ rất hài lòng hơn là hài lòng.\n\n\n\n\n2. Các Hệ số Hồi quy\n\nWaitTime = -0.75:\n\nHệ số này âm, cho thấy rằng thời gian chờ đợi dài hơn sẽ làm giảm mức độ hài lòng của khách hàng.\nCụ thể, mỗi đơn vị tăng thêm trong thời gian chờ đợi sẽ làm giảm log-odds của việc khách hàng đạt mức độ hài lòng cao hơn so với mức độ hài lòng hiện tại.\nHệ số này có ý nghĩa thống kê (P-value &lt; 0.001), cho thấy rằng thời gian chờ đợi có ảnh hưởng đáng kể đến mức độ hài lòng của khách hàng.\n\nServiceQuality = 1.20:\n\nHệ số này dương, cho thấy rằng chất lượng dịch vụ tốt hơn sẽ tăng mức độ hài lòng của khách hàng.\nCụ thể, mỗi đơn vị tăng thêm trong chất lượng dịch vụ sẽ tăng log-odds của việc khách hàng đạt mức độ hài lòng cao hơn so với mức độ hài lòng hiện tại.\nHệ số này cũng có ý nghĩa thống kê (P-value &lt; 0.001), cho thấy rằng chất lượng dịch vụ có ảnh hưởng đáng kể đến mức độ hài lòng của khách hàng.\n\n\n\n\n\nCách Tính Xác Suất\nGiả sử chúng ta muốn tính xác suất rằng một khách hàng với thời gian chờ đợi là 3 đơn vị và chất lượng dịch vụ là 4 đơn vị sẽ thuộc vào từng mức độ hài lòng.\n\nĐầu tiên, chúng ta tính giá trị tiềm ẩn \\(Y^*\\): \\[\nY^* = -0.75 \\times 3 + 1.20 \\times 4 = -2.25 + 4.8 = 2.55\n\\]\nSau đó, chúng ta tính xác suất cho từng mức độ hài lòng:\n\\[\nP(Y \\leq 1) = \\frac{1}{1 + \\exp(-(-2.50 - 2.55))} = \\frac{1}{1 + \\exp(5.05)} \\approx 0.006\n\\]\n\\[\nP(Y \\leq 2) = \\frac{1}{1 + \\exp(-(-1.50 - 2.55))} = \\frac{1}{1 + \\exp(4.05)} \\approx 0.017\n\\]\n\\[\nP(Y \\leq 3) = \\frac{1}{1 + \\exp(-(-0.50 - 2.55))} = \\frac{1}{1 + \\exp(3.05)} \\approx 0.045\n\\]\n\\[\nP(Y \\leq 4) = \\frac{1}{1 + \\exp(-(1.00 - 2.55))} = \\frac{1}{1 + \\exp(1.55)} \\approx 0.175\n\\]\n\\[\nP(Y = 5) = 1 - P(Y \\leq 4) = 1 - 0.175 = 0.825\n\\]\n\n\n\nTóm lại\n\nXác suất rằng khách hàng rất không hài lòng: 0.6%\nXác suất rằng khách hàng không hài lòng: 1.7% - 0.6%\nXác suất rằng khách hàng bình thường: 4.5% - 1.7%\nXác suất rằng khách hàng hài lòng: 17.5% - 4.5%\nXác suất rằng khách hàng rất hài lòng: 82.5%\n\n\n\nĐánh giá mô hình\n\nCohen’s Kappa:\n\nSử dụng: Với mô hình có biến phụ thuộc là một biến định danh (nominal variable) và muốn đánh giá mức độ chính xác của mô hình dự báo.\nVí dụ: Đánh giá phân loại giữa hai bác sĩ về chẩn đoán bệnh.\n\n⭐Weighted Cohen’s Kappa hoặc Kendall’s Tau:\n\nSử dụng: Với mô hình có biến phụ thuộc là một biến thứ bậc (ordinal variable).\n⭐Weighted Cohen’s Kappa: Điều chỉnh cho trọng số khác nhau của sự không chính xác của kết quả dự báo để phản ánh mức độ nghiêm trọng của các sai lệch dự đoán.\nKendall’s Tau: Được sử dụng để đánh giá mức độ tương quan và độ chính xác của dự báo.\nVí dụ: Đánh giá mức độ hài lòng của khách hàng trên thang điểm từ 1 đến 5.\n\nPearson’s Correlation:\n\nSử dụng: Với mô hình có biến phụ thuộc là một biến định lượng và muốn biết mức độ tương quan giữa kết quả thực tế và dự báo.\nVí dụ: So sánh kết quả của hai phương pháp đo lường huyết áp.\n\nFleiss’ Kappa:\n\nSử dụng: Khi bạn có nhiều hơn hai mẫu phụ thuộc danh nghĩa (nominal dependent samples) và muốn đánh giá mức độ đồng thuận giữa nhiều mô hình dự báo.\nVí dụ: Đánh giá sự đồng thuận giữa nhiều bác sĩ về chẩn đoán bệnh trong một tập hợp các trường hợp.\n\n\n\n\n\nImage Description\n\n\n\n\nQuadratic Weighted Kappa (QWK).\n\n1. Cohen’s Kappa\n\\[ \\kappa = \\frac{p_0 - p_e}{1 - p_e} \\]\n\n\\(p_0\\): Xác suất quan sát đồng thuận (tỷ lệ các giá trị dự đoán trùng với giá trị thực tế).\n\\(p_e\\): Xác suất đồng thuận ngẫu nhiên (tính dựa trên ma trận đồng thuận và xác suất của từng nhãn).\n\n\n\n2. Quadratic Weighted Kappa (QWK)\n\\[ QWK = 1 - \\frac{\\sum_{i,j} W_{ij} O_{ij}}{\\sum_{i,j} W_{ij} E_{ij}} \\]\n\n\\(O_{ij}\\): Ma trận quan sát (confusion matrix), với \\(O_{ij}\\) là số lượng trường hợp mà giá trị thực tế là \\(i\\) và giá trị dự đoán là \\(j\\).\n\\(E_{ij}\\): Ma trận kỳ vọng, được tính dựa trên xác suất của các nhãn.\n\\(W_{ij}\\): Ma trận trọng số, tính dựa trên khoảng cách giữa các nhãn \\(i\\) và \\(j\\), thường được tính như sau:\n\n\\[\nW_{ij} = \\frac{(i - j)^2}{(N - 1)^2}\n\\]\ntrong đó \\(N\\) là số lượng nhãn.\n\n\n3. Khoảng Giá Trị:\n\nQWK có giá trị dao động từ -1 đến 1:\n\n1: Đồng thuận hoàn hảo giữa dự báo và thực tế.\n0: Không có sự đồng thuận nào ngoài mức ngẫu nhiên.\n&lt; 0: Sự đồng thuận kém hơn mức ngẫu nhiên (phản đồng thuận).\n\n\n\n\n4. Ý Nghĩa:\n\n1: Mô hình dự báo hoàn toàn khớp với thực tế.\n0.81 - 1.00: Đồng thuận gần như hoàn hảo giữa dự báo và thực tế.\n0.61 - 0.80: Đồng thuận đáng kể.\n0.41 - 0.60: Đồng thuận khá.\n0.21 - 0.40: Đồng thuận vừa phải.\n0.01 - 0.20: Đồng thuận nhẹ.\n0: Không có sự đồng thuận.\n&lt; 0: Đồng thuận kém hơn ngẫu nhiên, mô hình hoặc đánh giá có vấn đề nghiêm trọng.\n\n\n\n\nVí dụ Minh Họa:\nGiả sử chúng ta có dự báo và giá trị thực tế trên thang điểm từ 1 đến 5 như sau:\n\n\n\nThực tế\nDự báo\n\n\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5\n5\n\n\n\nTrong trường hợp này, QWK sẽ bằng 1, biểu thị sự đồng thuận hoàn hảo giữa dự báo và thực tế.\nTuy nhiên, nếu có một số sự khác biệt như:\n\n\n\nThực tế\nDự báo\n\n\n\n\n1\n2\n\n\n2\n2\n\n\n3\n3\n\n\n4\n5\n\n\n5\n4\n\n\n\nQWK sẽ phản ánh mức độ đồng thuận kém hơn giữa dự báo và thực tế.\nQWK là một chỉ số quan trọng trong việc đánh giá mô hình dự báo trong các bài toán phân loại thứ bậc, đặc biệt khi các hạng mục có thứ tự. Nó giúp chúng ta hiểu rõ hơn về mức độ tương đồng giữa dự báo của mô hình và giá trị thực tế, qua đó cải thiện chất lượng mô hình.\n\n\nVí dụ 2:\n\n\n\nThực tế (Actual)\nDự đoán (Predicted)\n\n\n\n\nRất không hài lòng (1)\nRất không hài lòng (1)\n\n\nKhông hài lòng (2)\nKhông hài lòng (2)\n\n\nTrung lập (3)\nTrung lập (3)\n\n\nHài lòng (4)\nTrung lập (3)\n\n\nRất hài lòng (5)\nRất hài lòng (5)\n\n\nRất không hài lòng (1)\nKhông hài lòng (2)\n\n\nKhông hài lòng (2)\nRất không hài lòng (1)\n\n\nTrung lập (3)\nHài lòng (4)\n\n\nHài lòng (4)\nRất hài lòng (5)\n\n\nRất hài lòng (5)\nHài lòng (4)\n\n\n\n\n\n1. Cohen’s Kappa\nBước 1: Tạo ma trận đồng thuận (confusion matrix).\n\\[\n\\begin{array}{c|ccccc}\n& 1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n1 & 1 & 1 & 0 & 0 & 0 \\\\\n2 & 1 & 1 & 0 & 0 & 0 \\\\\n3 & 0 & 0 & 1 & 1 & 0 \\\\\n4 & 0 & 0 & 1 & 0 & 1 \\\\\n5 & 0 & 0 & 0 & 1 & 1 \\\\\n\\end{array}\n\\]\nBước 2: Tính xác suất quan sát đồng thuận \\(p_0\\) và xác suất đồng thuận ngẫu nhiên \\(p_e\\).\n\n\\(p_0 = \\frac{1 + 1 + 1 + 0 + 1}{10} = \\frac{4}{10} = 0.4\\)\nTính xác suất của từng nhãn:\n\n\\(P(\\text{Actual} = 1) = \\frac{2}{10}, P(\\text{Predicted} = 1) = \\frac{1}{10}\\)\n\\(P(\\text{Actual} = 2) = \\frac{2}{10}, P(\\text{Predicted} = 2) = \\frac{1}{10}\\)\n\\(P(\\text{Actual} = 3) = \\frac{2}{10}, P(\\text{Predicted} = 3) = \\frac{2}{10}\\)\n\\(P(\\text{Actual} = 4) = \\frac{2}{10}, P(\\text{Predicted} = 4) = \\frac{2}{10}\\)\n\\(P(\\text{Actual} = 5) = \\frac{2}{10}, P(\\text{Predicted} = 5) = \\frac{2}{10}\\)\n\n\\(p_e = (0.2 \\times 0.1) + (0.2 \\times 0.1) + (0.2 \\times 0.2) + (0.2 \\times 0.2) + (0.2 \\times 0.2) = 0.02 + 0.02 + 0.04 + 0.04 + 0.04 = 0.16\\)\n\nBước 3: Tính Cohen’s Kappa.\n\\[\n\\kappa = \\frac{p_0 - p_e}{1 - p_e} = \\frac{0.4 - 0.16}{1 - 0.16} = \\frac{0.24}{0.84} \\approx 0.286\n\\]\n\n\n3. Quadratic Weighted Kappa (QWK)\nBước 1: Tạo ma trận đồng thuận (confusion matrix).\n\\[\n\\begin{array}{c|ccccc}\n& 1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n1 & 1 & 1 & 0 & 0 & 0 \\\\\n2 & 1 & 1 & 0 & 0 & 0 \\\\\n3 & 0 & 0 & 1 & 1 & 0 \\\\\n4 & 0 & 0 & 1 & 0 & 1 \\\\\n5 & 0 & 0 & 0 & 1 & 1 \\\\\n\\end{array}\n\\]\nBước 2: Tạo ma trận trọng số dựa trên khoảng cách giữa các nhãn.\n$$\n\\[\\begin{array}{c|ccccc}\n& 1 &\n\n2 & 3 & 4 & 5 \\\\\n\\hline\n1 & 0 & 0.0625 & 0.25 & 0.5625 & 1 \\\\\n2 & 0.0625 & 0 & 0.0625 & 0.25 & 0.5625 \\\\\n3 & 0.25 & 0.0625 & 0 & 0.0625 & 0.25 \\\\\n4 & 0.5625 & 0.25 & 0.0625 & 0 & 0.0625 \\\\\n5 & 1 & 0.5625 & 0.25 & 0.0625 & 0 \\\\\n\\end{array}\\]\n$$\nBước 3: Tạo ma trận kỳ vọng dựa trên xác suất của các nhãn.\n\\[\n\\begin{array}{c|ccccc}\n& 1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\\\\n2 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\\\\n3 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\\\\n4 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\\\\n5 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\\\\n\\end{array}\n\\]\nBước 4: Tính QWK.\n\\[ QWK = 1 - \\frac{\\sum_{i,j} W_{ij} O_{ij}}{\\sum_{i,j} W_{ij} E_{ij}} \\]\n\\[ \\sum_{i,j} W_{ij} O_{ij} = 1 \\times 0 + 1 \\times 0.0625 + 1 \\times 0 + 0 \\times 0.0625 + 1 \\times 0 + 0 \\times 0.0625 + 1 \\times 0.25 + 1 \\times 0.0625 + 0 \\times 0.0625 + 1 \\times 0.0625 = 0.625\n\\]\n\\[\n\\sum_{i,j} W_{ij} E_{ij} = 0.1 \\times 0 + 0.1 \\times 0.0625 + 0.1 \\times 0.25 + 0.1 \\times 0.5625 + 0.1 \\times 1 + 0.1 \\times 0.0625 + 0.1 \\times 0 + 0.1 \\times 0.0625 + 0.1 \\times 0.25 + 0.1 \\times 0.5625 + 0.2 \\times 0.25 + 0.2 \\times 0.0625 + 0.2 \\times 0 + 0.2 \\times 0.0625 + 0.2 \\times 0.25 + 0.2 \\times 0.25 + 0.2 \\times 0.0625 + 0.2 \\times 0 + 0.2 \\times 0.0625 + 0.2 \\times 0.25 + 0.2 \\times 0.5625 + 0.2 \\times 0.25 + 0.2 \\times 0.0625 + 0.2 \\times 0 + 0.2 \\times 0.0625 = 1.5\n\\]\n\\[\nQWK = 1 - \\frac{0.625}{1.5} = 1 - 0.4167 = 0.5833\n\\]\n\n\nSo sánh trọng số Linear, Quadratic của hệ số kappa của Cohen (Cohen’s kappa coefficient)\nTrong đánh giá các mô hình phân loại thứ bậc, hai loại trọng số thường được sử dụng là trọng số tuyến tính (linear weights) và trọng số bậc hai (quadratic weights). Các trọng số này ảnh hưởng đến cách tính toán độ sai lệch giữa các dự đoán và giá trị thực tế.\n\nCông thức Tính Trọng số\n\nLinear Weight: \\[\nW_{ij} = \\frac{|i - j|}{N - 1}\n\\] Trong đó:\n\n\\(i\\) và \\(j\\) là các nhãn.\n\\(N\\) là tổng số nhãn.\n\nQuadratic Weight: \\[\nW_{ij} = \\frac{(i - j)^2}{(N - 1)^2}\n\\] Trong đó:\n\n\\(i\\) và \\(j\\) là các nhãn.\n\\(N\\) là tổng số nhãn.\n\n\n\n\n\nSo sánh Trọng số Linear và Quadratic cho các giá trị phân loại\nGiả sử chúng ta có các giá trị phân loại từ 0 đến 4:\n\n\n\nThực tế\nDự báo\n\n\n\n\n0\n0\n\n\n1\n0\n\n\n2\n0\n\n\n3\n0\n\n\n4\n0\n\n\n\n\nTrọng số Linear\nVới \\(N = 5\\):\n\\[ W_{ij}^{\\text{linear}} = \\frac{|i - j|}{4} \\]\n\n\n\nThực tế (\\(i\\))\nDự báo (\\(j\\))\n\\(\\|i - j\\|\\)\nTrọng số Linear\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n1\n0.25\n\n\n2\n0\n2\n0.5\n\n\n3\n0\n3\n0.75\n\n\n4\n0\n4\n1\n\n\n\n\n\nTrọng số Quadratic\nVới \\(N = 5\\):\n\\[ W_{ij}^{\\text{quadratic}} = \\frac{(i - j)^2}{16} \\]\n\n\n\nThực tế (\\(i\\))\nDự báo (\\(j\\))\n\\((i - j)^2\\)\nTrọng số Quadratic\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n1\n0.0625\n\n\n2\n0\n4\n0.25\n\n\n3\n0\n9\n0.5625\n\n\n4\n0\n16\n1\n\n\n\n\n\n\nNhận xét\n\nLinear Weights: Trọng số tăng tuyến tính với sự khác biệt giữa nhãn thực tế và nhãn dự đoán. Điều này có nghĩa là mỗi đơn vị sai lệch sẽ có một mức độ ảnh hưởng tương đương.\nQuadratic Weights: Trọng số tăng bậc hai với sự khác biệt giữa nhãn thực tế và nhãn dự đoán. Điều này có nghĩa là các sai lệch lớn hơn sẽ bị phạt nặng hơn rất nhiều so với các sai lệch nhỏ hơn.\n\n\n\nKết luận\n\nLinear Weights: Phù hợp hơn trong các trường hợp mà mọi sự khác biệt giữa các nhãn đều quan trọng như nhau.\nQuadratic Weights: Phù hợp hơn trong các trường hợp mà các sai lệch lớn cần được phạt nặng hơn, phản ánh mức độ nghiêm trọng của các dự đoán sai lệch lớn hơn.\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Tải dữ liệu\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\ndata = pd.read_csv(url, sep=';')\n\n# Kiểm tra dữ liệu\nprint(data.head())\nprint(data.info())\n\n# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\nX = data.drop('quality', axis=1)\ny = data['quality'] - 3  # Điều chỉnh các giá trị chất lượng\n\n# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Hiển thị một số thông tin về tập huấn luyện và tập kiểm tra\nprint(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n\n# Chuyển đổi biến mục tiêu thành dạng ordinal\ny_train = y_train.astype('category')\ny_test = y_test.astype('category')\n\n   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.4              0.70         0.00             1.9      0.076   \n1            7.8              0.88         0.00             2.6      0.098   \n2            7.8              0.76         0.04             2.3      0.092   \n3           11.2              0.28         0.56             1.9      0.075   \n4            7.4              0.70         0.00             1.9      0.076   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 11.0                  34.0   0.9978  3.51       0.56   \n1                 25.0                  67.0   0.9968  3.20       0.68   \n2                 15.0                  54.0   0.9970  3.26       0.65   \n3                 17.0                  60.0   0.9980  3.16       0.58   \n4                 11.0                  34.0   0.9978  3.51       0.56   \n\n   alcohol  quality  \n0      9.4        5  \n1      9.8        5  \n2      9.8        5  \n3      9.8        6  \n4      9.4        5  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1599 entries, 0 to 1598\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   fixed acidity         1599 non-null   float64\n 1   volatile acidity      1599 non-null   float64\n 2   citric acid           1599 non-null   float64\n 3   residual sugar        1599 non-null   float64\n 4   chlorides             1599 non-null   float64\n 5   free sulfur dioxide   1599 non-null   float64\n 6   total sulfur dioxide  1599 non-null   float64\n 7   density               1599 non-null   float64\n 8   pH                    1599 non-null   float64\n 9   sulphates             1599 non-null   float64\n 10  alcohol               1599 non-null   float64\n 11  quality               1599 non-null   int64  \ndtypes: float64(11), int64(1)\nmemory usage: 150.0 KB\nNone\nX_train shape: (1279, 11), y_train shape: (1279,)\nX_test shape: (320, 11), y_test shape: (320,)\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel\n\n# Xây dựng mô hình Ordinal Logistic Regression\nmodel = OrderedModel(y_train, X_train, distr='logit')\nresult = model.fit(method='bfgs')\n\n# In kết quả\nprint(result.summary())\n\nc:\\Users\\binhnn2\\anaconda3\\envs\\rdm\\lib\\site-packages\\statsmodels\\miscmodels\\ordinal_model.py:206: Warning: the endog has ordered == False, risk of capturing a wrong order for the categories. ordered == True preferred.\n  warnings.warn(\"the endog has ordered == False, \"\n\n\nOptimization terminated successfully.\n         Current function value: 0.966817\n         Iterations: 90\n         Function evaluations: 94\n         Gradient evaluations: 94\n                             OrderedModel Results                             \n==============================================================================\nDep. Variable:                quality   Log-Likelihood:                -1236.6\nModel:                   OrderedModel   AIC:                             2505.\nMethod:            Maximum Likelihood   BIC:                             2588.\nDate:                Fri, 05 Jul 2024                                         \nTime:                        16:28:50                                         \nNo. Observations:                1279                                         \nDf Residuals:                    1263                                         \nDf Model:                          11                                         \n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nfixed acidity            0.0783      0.091      0.859      0.390      -0.100       0.257\nvolatile acidity        -3.1342      0.449     -6.987      0.000      -4.013      -2.255\ncitric acid             -0.6750      0.520     -1.299      0.194      -1.693       0.343\nresidual sugar           0.0359      0.054      0.664      0.506      -0.070       0.142\nchlorides               -4.8182      1.462     -3.295      0.001      -7.684      -1.952\nfree sulfur dioxide      0.0185      0.008      2.371      0.018       0.003       0.034\ntotal sulfur dioxide    -0.0125      0.003     -4.676      0.000      -0.018      -0.007\ndensity                 -2.5943     75.908     -0.034      0.973    -151.372     146.183\npH                      -0.9766      0.669     -1.459      0.145      -2.288       0.335\nsulphates                2.5650      0.397      6.463      0.000       1.787       3.343\nalcohol                  0.8942      0.095      9.401      0.000       0.708       1.081\n0/1                     -2.4053     74.344     -0.032      0.974    -148.117     143.306\n1/2                      0.5993      0.169      3.548      0.000       0.268       0.930\n2/3                      1.3067      0.044     30.034      0.000       1.221       1.392\n3/4                      1.0376      0.042     24.546      0.000       0.955       1.120\n4/5                      1.1218      0.091     12.268      0.000       0.943       1.301\n========================================================================================\n\n\n\n# Các giá trị \\tau\nnum_of_thresholds = 5\nmodel.transform_threshold_params(result.params[-num_of_thresholds:])\n\narray([       -inf, -2.40532186, -0.58442067,  3.10969434,  5.93215937,\n        9.00252064,         inf])\n\n\n\n# Dự đoán trên tập kiểm tra\npred_proba = result.predict(X_test)\npred_one_hot = np.eye(len(set(y)))[np.round(pred_proba).astype(int)]\n\n# Chuyển đổi one-hot encoding sang mảng một chiều\npred = np.argmax(pred_one_hot, axis=1)[:,1]\n\n# Chuyển đổi lại y_test thành dạng int để phù hợp với các hàm đánh giá\ny_test_int = y_test.astype(int).to_numpy()\n\n\nfrom sklearn.metrics import cohen_kappa_score, confusion_matrix\nfrom scipy.stats import spearmanr\n# Đảm bảo kích thước phù hợp\nif len(pred) == len(y_test_int):\n    # Đánh giá mô hình sử dụng QWK\n    qwk = cohen_kappa_score(y_test_int, pred, weights='quadratic')\n    print(\"Quadratic Weighted Kappa: \", qwk)\n\n    # Đánh giá mô hình sử dụng Spearman's Rank Correlation\n    spearman_corr, _ = spearmanr(y_test_int, pred)\n    print(\"Spearman's Rank Correlation: \", spearman_corr)\n\n    # Đánh giá mô hình sử dụng Cohen's Kappa\n    cohen_kappa = cohen_kappa_score(y_test_int, pred)\n    print(\"Cohen's Kappa: \", cohen_kappa)\nelse:\n    print(\"Error: The size of y_test and pred do not match.\")\n\nQuadratic Weighted Kappa:  0.17843809134148214\nSpearman's Rank Correlation:  0.35484507423547046\nCohen's Kappa:  0.2695153974695631\n\n\n\n\nCác bước thực hiện mô hình có biến phụ thuộc dạng thứ bậc\n\nXác định nhãn\nChia train/test: Stratify\nChọn biến: RFECV (by kappa), Spearman Correlation\nThuật toán sử dụng: Logistic, RandomForest, Xgboost, LightGBM\nHàm loss: tùy chỉnh hàm loss: kappa loss\nĐánh giá mô hình: Cohen’s kappa coefficient\n\n\n\nTham khảo\n\nCohen’s Kappa:\n\nBài viết này trên trang DATAtab cung cấp một hướng dẫn chi tiết về cách tính toán và sử dụng chỉ số Kappa của Cohen. Chỉ số này được sử dụng để đo lường mức độ đồng thuận giữa hai hoặc nhiều người đánh giá trong các nhiệm vụ phân loại. Hướng dẫn bao gồm cả lý thuyết cơ bản về chỉ số Kappa và cách tính toán nó với ví dụ minh họa.\n\nWeighted Cohen’s Kappa:\n\nBài viết này giải thích chi tiết về chỉ số Weighted Cohen’s Kappa, một mở rộng của Cohen’s Kappa để xử lý các dữ liệu phân loại có thứ tự. Nội dung bao gồm các công thức tính toán cho trọng số tuyến tính và trọng số bậc hai, cùng với các ví dụ minh họa cụ thể để giúp hiểu rõ hơn về cách áp dụng chỉ số này trong thực tế.\n\nWine Quality EDA, Prediction, and Deploy:\n\nNotebook này trên Kaggle của Lus Fernando Torres thực hiện phân tích khám phá dữ liệu (EDA), xây dựng mô hình dự đoán chất lượng rượu vang và triển khai mô hình. Nó bao gồm các bước xử lý dữ liệu, khám phá dữ liệu trực quan, xây dựng và đánh giá các mô hình học máy khác nhau, cũng như hướng dẫn triển khai mô hình để sử dụng thực tế.\n\n1st Place Solution - RAPIDS & XGBoost:\n\nĐây là notebook của người chiến thắng giải nhất trong một cuộc thi Kaggle, sử dụng RAPIDS và XGBoost để xây dựng mô hình. Notebook này chi tiết các bước và chiến lược mà tác giả sử dụng để đạt được kết quả tốt nhất trong cuộc thi, bao gồm xử lý dữ liệu, lựa chọn đặc trưng, và tinh chỉnh mô hình.\n\nPS S03E05 Pytorch NN Modeling:\n\nNotebook này trên Kaggle của Ryanirl tập trung vào việc xây dựng mô hình mạng nơ-ron nhân tạo (NN) bằng PyTorch cho tập dữ liệu của một cuộc thi Kaggle (PS S03E05). Nội dung bao gồm các bước từ chuẩn bị dữ liệu, xây dựng và huấn luyện mô hình, đến đánh giá và tinh chỉnh mô hình.\n\nS3E5 XGB, LGB, Cat:\n\nNotebook của Tetsutani trên Kaggle thực hiện so sánh giữa ba mô hình học máy phổ biến: XGBoost, LightGBM, và CatBoost trên tập dữ liệu của cuộc thi S3E5. Nó bao gồm các bước chuẩn bị dữ liệu, huấn luyện từng mô hình, và đánh giá hiệu suất của chúng để tìm ra mô hình tốt nhất.\n\nDrop Features Reduction with t-SNE Model:\n\nNotebook này của David H Guerrero trên Kaggle trình bày cách giảm chiều dữ liệu bằng phương pháp t-SNE và chọn lọc đặc trưng dựa trên tầm quan trọng của chúng. Tác giả sử dụng phương pháp t-SNE để trực quan hóa dữ liệu và quyết định loại bỏ những đặc trưng không cần thiết, nhằm cải thiện hiệu suất mô hình.\n\nOrdinal Logistic Regression Solution:\n\nNotebook của Ronen Nakash trên Kaggle giải thích cách xây dựng mô hình hồi quy logistic thứ bậc để giải quyết các bài toán phân loại thứ bậc. Nội dung bao gồm các bước từ chuẩn bị dữ liệu, xây dựng và huấn luyện mô hình, đến đánh giá và tinh chỉnh mô hình, giúp hiểu rõ hơn về việc áp dụng hồi quy logistic thứ bậc trong thực tế.\n\nOrdinal Regression - Statsmodels:\n\nBài viết trên trang Statsmodels cung cấp hướng dẫn chi tiết về cách sử dụng lớp OrderedModel cho hồi quy thứ bậc với các phân phối khác nhau (probit, logit, custom). Nội dung bao gồm các bước tải dữ liệu, xây dựng mô hình, diễn giải các hệ số, tính toán các ngưỡng, dự đoán và đánh giá hiệu suất mô hình.\n\n\n\n\nMột số code thực hành\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nimport warnings\nimport lightgbm as lgb\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\n# Load the dataset\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\ndata = pd.read_csv(url, sep=';')\n\n# Define features and target\nX = data.drop('quality', axis=1)\ny = data['quality'] - 3\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert the data into LightGBM Dataset\ndtrain = lgb.Dataset(X_train_scaled, label=y_train)\ndtest = lgb.Dataset(X_test_scaled, label=y_test, reference=dtrain)\n\n# Set the parameters for LightGBM\nparams = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y)),  # Number of classes\n    'metric': 'multi_logloss',\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42,\n    'verbose': -1\n}\n\n# Train the LightGBM model\nbst = lgb.train(params, dtrain, num_boost_round=100, valid_sets=[dtest])\n\n# Predict on the test set\ny_pred_prob = bst.predict(X_test_scaled, num_iteration=bst.best_iteration)\ny_pred = np.argmax(y_pred_prob, axis=1)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nkappa = cohen_kappa_score(y_test, y_pred)\nqwk = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n\nprint(f'Test Accuracy: {accuracy:.2f}')\nprint(f'Cohen\\'s Kappa: {kappa:.2f}')\nprint(f'Quadratic Weighted Kappa: {qwk:.2f}')\n\nTest Accuracy: 0.68\nCohen's Kappa: 0.48\nQuadratic Weighted Kappa: 0.66\n\n\n\nSử dụng kappa evaluation function thay cho multi_logloss\n\n# Custom kappa evaluation function\ndef kappa_eval(preds, train_data):\n    y_true = train_data.get_label()\n    y_pred = np.argmax(preds.reshape(len(np.unique(y_true)), -1), axis=0)\n    kappa = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n    return 'kappa', kappa, True\n\n# Convert the data into LightGBM Dataset\ndtrain = lgb.Dataset(X_train_scaled, label=y_train)\ndtest = lgb.Dataset(X_test_scaled, label=y_test, reference=dtrain)\n\n# Set the parameters for LightGBM\nparams = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y)),  # Number of classes\n    'metric': 'multi_logloss',  # Default metric\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42,\n    'verbose': -1\n}\n\n# Train the LightGBM model with custom kappa evaluation\nbst = lgb.train(params, dtrain, num_boost_round=100, valid_sets=[dtest], feval=kappa_eval)\n\n# Predict on the test set\ny_pred_prob = bst.predict(X_test_scaled, num_iteration=bst.best_iteration)\ny_pred = np.argmax(y_pred_prob, axis=1)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nkappa = cohen_kappa_score(y_test, y_pred)\nqwk = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n\nprint(f'Test Accuracy: {accuracy:.2f}')\nprint(f'Cohen\\'s Kappa: {kappa:.2f}')\nprint(f'Quadratic Weighted Kappa: {qwk:.2f}')\n\nTest Accuracy: 0.68\nCohen's Kappa: 0.48\nQuadratic Weighted Kappa: 0.66\n\n\n\n\nChia dữ liệu thành 3 tập sử dụng kappa evaluation function thay cho multi_logloss\n\n# Split the data into train, eval, and test sets (60% train, 20% eval, 20% test)\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_eval, y_train, y_eval = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_eval_scaled = scaler.transform(X_eval)\nX_test_scaled = scaler.transform(X_test)\n\n# Custom kappa evaluation function\ndef kappa_eval(preds, train_data):\n    y_true = train_data.get_label()\n    y_pred = np.argmax(preds.reshape(len(np.unique(y_true)), -1), axis=0)\n    kappa = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n    return 'kappa', kappa, True\n\n# Convert the data into LightGBM Datasets\ndtrain = lgb.Dataset(X_train_scaled, label=y_train)\ndeval = lgb.Dataset(X_eval_scaled, label=y_eval, reference=dtrain)\ndtest = lgb.Dataset(X_test_scaled, label=y_test, reference=dtrain)\n\n# Set the parameters for LightGBM\nparams = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y)),  # Number of classes\n    'metric': 'multi_logloss',  # Default metric\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42,\n    'verbose': -1\n}\n\n# Train the LightGBM model with custom kappa evaluation\nbst = lgb.train(params, dtrain, num_boost_round=100, valid_sets=[deval], feval=kappa_eval)\n\n# Predict on the test set\ny_pred_prob = bst.predict(X_test_scaled, num_iteration=bst.best_iteration)\ny_pred = np.argmax(y_pred_prob, axis=1)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nkappa = cohen_kappa_score(y_test, y_pred)\nqwk = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n\nprint(f'Test Accuracy: {accuracy:.2f}')\nprint(f'Cohen\\'s Kappa: {kappa:.2f}')\nprint(f'Quadratic Weighted Kappa: {qwk:.2f}')\n\nTest Accuracy: 0.65\nCohen's Kappa: 0.44\nQuadratic Weighted Kappa: 0.60\n\n\n\n\nThay metric = ‘custom’\n\n# Set the parameters for LightGBM\nparams = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y)),  # Number of classes\n    'metric': 'custom',  # Default metrics\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42,\n    'verbose': -1\n}\n\n# Train the LightGBM model with custom kappa evaluation\nbst = lgb.train(params, dtrain, num_boost_round=100, valid_sets=[deval], feval=kappa_eval)\n\n# Predict on the test set\ny_pred_prob = bst.predict(X_test_scaled, num_iteration=bst.best_iteration)\ny_pred = np.argmax(y_pred_prob, axis=1)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nkappa = cohen_kappa_score(y_test, y_pred)\nqwk = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n\nprint(f'Test Accuracy: {accuracy:.2f}')\nprint(f'Cohen\\'s Kappa: {kappa:.2f}')\nprint(f'Quadratic Weighted Kappa: {qwk:.2f}')\n\nTest Accuracy: 0.65\nCohen's Kappa: 0.44\nQuadratic Weighted Kappa: 0.60",
    "crumbs": [
      "Statistic",
      "Mô Hình Hồi Quy Logistic Thứ Tự"
    ]
  },
  {
    "objectID": "statistic/Binomial_interval.html",
    "href": "statistic/Binomial_interval.html",
    "title": "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial",
    "section": "",
    "text": "Phân phối nhị thức (binomial distribution) là một dạng phân phối xác suất rời rạc, đặc trưng cho số lần thành công trong một số lần thử nghiệm độc lập và có xác suất thành công cố định trong mỗi lần thử nghiệm. Phân phối nhị thức thường được sử dụng trong các tình huống mà mỗi thử nghiệm chỉ có hai kết quả có thể xảy ra (thành công hoặc thất bại), và xác suất thành công là không đổi trong tất cả các lần thử nghiệm.\nHàm xác suất của phân phối nhị thức được mô tả bởi công thức:\n\\[P(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k}\\]\nỞ đây: - \\(X\\) là biến ngẫu nhiên biểu thị số lần thành công trong \\(n\\) lần thử nghiệm. - \\(k\\) là một giá trị cụ thể của \\(X\\) (số lần thành công). - \\(n\\) là số lần thử nghiệm độc lập. - \\(p\\) là xác suất thành công trong mỗi lần thử nghiệm. - \\(\\binom{n}{k}\\) là số cách chọn \\(k\\) thành công từ \\(n\\) lần thử nghiệm, được gọi là hệ số nhị thức.\nPhân phối nhị thức thường được ký hiệu là \\(B(n, p)\\), trong đó \\(n\\) là số lần thử nghiệm và \\(p\\) là xác suất thành công. Phân phối này có giá trị kỳ vọng \\(np\\) và phương sai \\(np(1-p)\\).\nPhân phối nhị thức được ứng dụng rộng rãi trong thống kê, xã hội học, kinh tế học, và nhiều lĩnh vực khác để mô tả sự biến động trong số lần thành công trong một số thử nghiệm độc lập.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import binom\n\n# Các tham số của phân phối nhị thức\nn_values = [10, 20, 30]  # Số lần thử nghiệm\np = 0.5  # Xác suất thành công\n\n# Tạo mảng giá trị X từ 0 đến n_max cho mỗi n\nn_max = max(n_values)\nx_values = np.arange(0, n_max + 1)\n\n# Vẽ hình minh họa phân phối nhị thức cho các giá trị n khác nhau\nplt.figure(figsize=(10, 6))\n\nfor n in n_values:\n    pmf = binom.pmf(x_values, n, p)\n    plt.plot(x_values, pmf, label=f'n={n}')\n\nplt.title('Phân phối nhị thức với n khác nhau')\nplt.xlabel('Số lần thành công (X)')\nplt.ylabel('Xác suất')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Statistic",
      "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial"
    ]
  },
  {
    "objectID": "statistic/Binomial_interval.html#clopper-pearson-method-1",
    "href": "statistic/Binomial_interval.html#clopper-pearson-method-1",
    "title": "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial",
    "section": "Clopper-Pearson Method:",
    "text": "Clopper-Pearson Method:\n\nƯu điểm:\n\nBảo đảm độ tin cậy: Clopper-Pearson Method đảm bảo rằng khoảng tin cậy xây dựng sẽ bao gồm tỷ lệ thành công thực sự với độ tin cậy đã chọn.\nDễ hiểu và tính toán: Phương pháp này dễ hiểu và tính toán, thích hợp cho các ứng dụng với mẫu nhỏ.\n\n\n\nNhược điểm:\n\nRộng hóa khoảng tin cậy: Cho mẫu nhỏ, khoảng tin cậy Clopper-Pearson có thể rộng hơn so với các phương pháp khác, đặc biệt là khi tỷ lệ thành công gần 0 hoặc 1.",
    "crumbs": [
      "Statistic",
      "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial"
    ]
  },
  {
    "objectID": "statistic/Binomial_interval.html#jeffreys-method-1",
    "href": "statistic/Binomial_interval.html#jeffreys-method-1",
    "title": "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial",
    "section": "Jeffreys Method:",
    "text": "Jeffreys Method:\n\nƯu điểm:\nPhù hợp với mẫu lớn: Thích hợp cho các mẫu lớn, có thể tạo ra khoảng tin cậy ngắn hơn so với Clopper-Pearson.\n\n\nNhược điểm:\nTính phức tạp: Yêu cầu tính toán phức tạp hơn so với Clopper-Pearson, đặc biệt là khi sử dụng các phương pháp số để tính toán các giá trị của phân phối Beta.",
    "crumbs": [
      "Statistic",
      "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial"
    ]
  },
  {
    "objectID": "statistic/Binomial_interval.html#likelihood-ratio-method-1",
    "href": "statistic/Binomial_interval.html#likelihood-ratio-method-1",
    "title": "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial",
    "section": "Likelihood Ratio Method:",
    "text": "Likelihood Ratio Method:\n\nƯu điểm:\n\nPhù hợp với mẫu lớn: Tích hợp tốt với các mẫu lớn, có thể tạo ra khoảng tin cậy ngắn hơn.\nTính chất lý thuyết tỷ số kiểm định: Phương pháp này có liên quan chặt chẽ đến lý thuyết tỷ số kiểm định cơ bản, có ứng dụng rộng trong thống kê.\n\n\n\nNhược điểm:\n\nKhông đảm bảo chính xác độ tin cậy: Không đảm bảo chính xác độ tin cậy với mức ý nghĩa đã chọn, đặc biệt là với mẫu nhỏ.\nYêu cầu giả định: Đòi hỏi giả định về phân phối của ước lượng trong các trường hợp cụ thể.\n\n\nimport numpy as np\nfrom scipy.stats import beta\nimport matplotlib.pyplot as plt\n\ndef calculate_confidence_intervals(X, n, alpha=0.05, method='clopper-pearson'):\n    if method == 'clopper-pearson':\n        lower = beta.ppf(alpha/2, X, n - X + 1)\n        upper = beta.ppf(1 - alpha/2, X + 1, n - X)\n    elif method == 'jeffreys':\n        lower = beta.ppf(alpha/2, X + 0.5, n - X + 0.5)\n        upper = beta.ppf(1 - alpha/2, X + 0.5, n - X + 0.5)\n    elif method == 'likelihood-ratio':\n        p_hat = X/n\n        se_lr = np.sqrt(p_hat * (1 - p_hat) / n)\n        z_lr = 1.96  # Giá trị z tương ứng với mức ý nghĩa 0.025\n        lower = p_hat - z_lr * se_lr\n        upper = p_hat + z_lr * se_lr\n    else:\n        raise ValueError(\"Invalid method. Choose 'clopper-pearson', 'jeffreys', or 'likelihood-ratio'.\")\n    \n    return lower, upper\n\n# Dữ liệu mẫu\nX = 5\nn = 20\n\n# Tính khoảng tin cậy Clopper-Pearson\nlower_cp, upper_cp = calculate_confidence_intervals(X, n, method='clopper-pearson')\n\n# Tính khoảng tin cậy Jeffreys\nlower_j, upper_j = calculate_confidence_intervals(X, n, method='jeffreys')\n\n# Tính khoảng tin cậy Likelihood Ratio\nlower_lr, upper_lr = calculate_confidence_intervals(X, n, method='likelihood-ratio')\n\n# Hiển thị kết quả\nprint(f\"Clopper-Pearson CI: [{lower_cp}, {upper_cp}]\")\nprint(f\"Jeffreys CI: [{lower_j}, {upper_j}]\")\nprint(f\"Likelihood Ratio CI: [{lower_lr}, {upper_lr}]\")\n\n# Dữ liệu mẫu lớn hơn\nX_large = 500\nn_large = 10000\n\n# Tính khoảng tin cậy Clopper-Pearson\nlower_cp_large, upper_cp_large = calculate_confidence_intervals(X_large, n_large, method='clopper-pearson')\n\n# Tính khoảng tin cậy Jeffreys\nlower_j_large, upper_j_large = calculate_confidence_intervals(X_large, n_large, method='jeffreys')\n\n# Tính khoảng tin cậy Likelihood Ratio\nlower_lr_large, upper_lr_large = calculate_confidence_intervals(X_large, n_large, method='likelihood-ratio')\n\n# Hiển thị kết quả\nprint(f\"Clopper-Pearson CI (Large Sample): [{lower_cp_large}, {upper_cp_large}]\")\nprint(f\"Jeffreys CI (Large Sample): [{lower_j_large}, {upper_j_large}]\")\nprint(f\"Likelihood Ratio CI (Large Sample): [{lower_lr_large}, {upper_lr_large}]\")\n\nClopper-Pearson CI: [0.08657146910143461, 0.49104587170795744]\nJeffreys CI: [0.1023984856830451, 0.46419462924086813]\nLikelihood Ratio CI: [0.06022381603583657, 0.4397761839641634]\nClopper-Pearson CI (Large Sample): [0.0458099421987706, 0.05445452702835886]\nJeffreys CI (Large Sample): [0.04585790480962443, 0.054402519732842006]\nLikelihood Ratio CI (Large Sample): [0.045728279035330145, 0.05427172096466986]\n\n\nTrong trường hợp mẫu lớn thì kết quả ước lượng gần bằng nhau, nhưng trong trường hợp mẫu nhỏ thì phương pháp Clopper-Pearson thận trọng hơn\n\nlower_cp_large, upper_cp_large = calculate_confidence_intervals(243, 682, method='clopper-pearson')\nprint(lower_cp_large, upper_cp_large)\n\nlower_cp_large, upper_cp_large = calculate_confidence_intervals(243, 682, method='jeffreys')\nprint(lower_cp_large, upper_cp_large)\n\nlower_cp_large, upper_cp_large = calculate_confidence_intervals(243, 682, method='likelihood-ratio')\nprint(lower_cp_large, upper_cp_large)\n\n0.32031802030853457 0.3935415694585724\n0.32103156127821925 0.3927951336634465\n0.3203619370534926 0.39224803362099425\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\ndef calculate_intervals(X, n, alpha=0.05):\n    # Clopper-Pearson\n    cp_lower = beta.ppf(alpha/2, X, n - X + 1)\n    cp_upper = beta.ppf(1 - alpha/2, X + 1, n - X)\n\n    # Jeffreys\n    jeffreys_lower = beta.ppf(alpha/2, X + 0.5, n - X + 0.5)\n    jeffreys_upper = beta.ppf(1 - alpha/2, X + 0.5, n - X + 0.5)\n\n    # Likelihood Ratio\n    p_hat = X/n\n    se_lr = np.sqrt(p_hat * (1 - p_hat) / n)\n    z_lr = 1.96  # Giá trị z tương ứng với mức ý nghĩa 0.025\n    lr_lower = p_hat - z_lr * se_lr\n    lr_upper = p_hat + z_lr * se_lr\n\n    return cp_lower, cp_upper, jeffreys_lower, jeffreys_upper, lr_lower, lr_upper\n\ndef plot_distribution_and_intervals(X, n):\n    alpha = 0.05\n\n    # Calculate intervals\n    cp_lower, cp_upper, jeffreys_lower, jeffreys_upper, lr_lower, lr_upper = calculate_intervals(X, n, alpha)\n\n    # Vẽ đồ thị\n    x_values = np.linspace(0, 1, 1000)\n    pdf_values = beta.pdf(x_values, X, n - X + 1)\n\n    plt.plot(x_values, pdf_values, label=f'Beta({X}, {n - X + 1})', color='blue')\n    plt.axvline(cp_lower, color='red', linestyle='--', label='Clopper-Pearson (Lower)')\n    plt.axvline(jeffreys_lower, color='green', linestyle='--', label='Jeffreys (Lower)')\n    plt.axvline(cp_upper, color='red', linestyle='--', label='Clopper-Pearson (Upper)')\n    plt.axvline(jeffreys_upper, color='green', linestyle='--', label='Jeffreys (Upper)')\n\n    # Likelihood Ratio Interval\n    plt.axvline(lr_lower, color='purple', linestyle='--', label='Likelihood Ratio (Lower)')\n    plt.axvline(lr_upper, color='purple', linestyle='--', label='Likelihood Ratio (Upper)')\n\n    plt.title('Beta Distribution and Quantiles')\n    plt.xlabel('p')\n    plt.ylabel('Probability Density Function (PDF)')\n    plt.legend()\n    plt.show()\n\n# Example usage with small sample size\nplot_distribution_and_intervals(5, 20)\n\n# Example usage with large sample size\nplot_distribution_and_intervals(500, 2000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import binom\n\n# Example data\nsuccesses = 243\ntrials = 682\nconfidence_level = 0.95\n    \n# Calculate binomial confidence interval using the Clopper-Pearson method\nlower, upper = binom.interval(confidence_level, trials, successes/trials)\n\nprint(f\"Binomial Confidence Interval (Clopper-Pearson): [{lower}, {upper}]\")\n\nBinomial Confidence Interval (Clopper-Pearson): [219.0, 268.0]\n\n\n\n# Clopper–Pearson interval\nfrom scipy.stats import beta\nk = 243\nn = 682\nalpha = 0.05\np_u, p_o = beta.ppf([alpha/2, 1 - alpha/2], [k, k + 1], [n - k + 1, n - k])\nprint(f\"Binomial Confidence Interval (Clopper-Pearson): [{p_u}, {p_o}]\")\n\nBinomial Confidence Interval (Clopper-Pearson): [0.32031802030853457, 0.3935415694585724]",
    "crumbs": [
      "Statistic",
      "So sánh Likelihood Ratio, Jeffreys và Clopper-Pearson trong ước lượng khoảng tin cậy cho phân phối binomial"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html",
    "href": "statistic/PSI_KS_ENTROPY.html",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "",
    "text": "Chỉ số Ổn định tổng thể (PSI - Population Stability Index) là một phép đo thường được sử dụng trong ngành ngân hàng, đặc biệt trong việc xếp hạng tín dụng, để giám sát sự ổn định và hiệu suất của các mô hình xếp hạng theo thời gian. Đây là cách để định lượng sự thay đổi trong phân phối tổng thể, điều này có thể ảnh hưởng đến sức mạnh dự đoán của một mô hình.\nPSI được tính như sau:\n\\[ PSI = \\sum_{i=1}^{N} (Actual_{i} - Expected_{i}) \\log \\left( \\frac{Actual_{i}}{Expected_{i}} \\right) \\]\nTrong đó:\n\n\\(Actual_{i}\\) và \\(Expected_{i}\\) là tỷ lệ quan sát rơi vào bin (i) cho dữ liệu thực tế (mới hoặc kiểm tra) và dữ liệu mong đợi (cũ hoặc đào tạo), tương ứng. - (N) là tổng số bins.\n\nCách giải thích thông thường về các giá trị PSI như sau:\n\nPSI &lt; 0.1: Không có sự thay đổi đáng kể về tổng thể. Mô hình thường được coi là ổn định.\n0.1 ≤ PSI &lt; 0.25: Có một số thay đổi nhỏ trong tổng thể, có thể cần được điều tra thêm.\nPSI ≥ 0.25: Sự thay đổi đáng kể về tổng thể. Mô hình có thể không còn phù hợp và cần được cập nhật hoặc xây dựng lại.\n\nLưu ý rằng những ngưỡng này không phải là cố định và có thể thay đổi tùy thuộc vào đặc điểm cụ thể của tình huống và mức độ rủi ro bạn sẵn sàng chấp nhận.\n\nimport numpy as np\nimport warnings\n\ndef calculate_psi(expected, actual, categorical=False, bins=None):\n    \"\"\"\n    Calculate the Population Stability Index (PSI) between two sets of data.\n\n    Args:\n        expected (numpy.ndarray): The expected values.\n        actual (numpy.ndarray): The actual values to be compared against the expected values.\n        categorical (bool, optional): Set to True if the variables are categorical. Default is False.\n        bins (int, optional): Number of bins for binning numeric data. Default is None (auto).\n\n    Returns:\n        float: The calculated PSI value.\n    \"\"\"\n    # Check if the variables are categorical\n    if categorical:\n        # Get unique categories\n        categories = np.unique(np.concatenate([expected, actual]))\n\n        # Issue a warning if the number of unique categories exceeds 20\n        if len(categories) &gt; 20:\n            warnings.warn(\"Warning: Number of unique categories exceeds 20.\")\n\n        # Calculate the expected and actual proportions for each category\n        expected_probs = np.array([np.sum(expected == cat) for cat in categories]) / len(expected)\n        actual_probs = np.array([np.sum(actual == cat) for cat in categories]) / len(actual)\n    else:\n        expected = expected[~np.isnan(expected)]\n        actual = actual[~np.isnan(actual)]\n        # Apply binning for numeric columns\n        if bins is None:\n            bins = 10  # Default to 10 bins, you can change this value as needed\n\n        # Calculate the bin edges based on percentiles\n        bin_edges = np.percentile(np.hstack((expected, actual)), np.linspace(0, 100, bins + 1))\n\n        # Calculate the expected and actual proportions for each bin\n        expected_probs, _ = np.histogram(expected, bins=bin_edges)\n        actual_probs, _ = np.histogram(actual, bins=bin_edges)\n\n        # Normalize to get proportions\n        expected_probs = expected_probs / len(expected)\n        actual_probs = actual_probs / len(actual)\n        \n    # Initialize PSI\n    psi_value = 0\n\n    # Loop over each bin or category\n    for i in range(len(expected_probs)):\n        # Avoid division by zero and log of zero\n        if expected_probs[i] == 0 or actual_probs[i] == 0:\n            continue\n        # Calculate the PSI for this bin or category\n        psi_value += (expected_probs[i] - actual_probs[i]) * np.log(expected_probs[i] / actual_probs[i])\n\n    return psi_value\n\n# Example usage\nexpected_data = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 6, 6, 7, 8, 9, 10])\nactual_data = np.array([1, 1, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 7, 8, 9, 10])\n\npsi = calculate_psi(expected_data, actual_data, categorical=False, bins=5)\nprint(\"Population Stability Index (PSI):\", psi)\n\nPopulation Stability Index (PSI): 0.028379201320332823",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html#psi",
    "href": "statistic/PSI_KS_ENTROPY.html#psi",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "",
    "text": "Chỉ số Ổn định tổng thể (PSI - Population Stability Index) là một phép đo thường được sử dụng trong ngành ngân hàng, đặc biệt trong việc xếp hạng tín dụng, để giám sát sự ổn định và hiệu suất của các mô hình xếp hạng theo thời gian. Đây là cách để định lượng sự thay đổi trong phân phối tổng thể, điều này có thể ảnh hưởng đến sức mạnh dự đoán của một mô hình.\nPSI được tính như sau:\n\\[ PSI = \\sum_{i=1}^{N} (Actual_{i} - Expected_{i}) \\log \\left( \\frac{Actual_{i}}{Expected_{i}} \\right) \\]\nTrong đó:\n\n\\(Actual_{i}\\) và \\(Expected_{i}\\) là tỷ lệ quan sát rơi vào bin (i) cho dữ liệu thực tế (mới hoặc kiểm tra) và dữ liệu mong đợi (cũ hoặc đào tạo), tương ứng. - (N) là tổng số bins.\n\nCách giải thích thông thường về các giá trị PSI như sau:\n\nPSI &lt; 0.1: Không có sự thay đổi đáng kể về tổng thể. Mô hình thường được coi là ổn định.\n0.1 ≤ PSI &lt; 0.25: Có một số thay đổi nhỏ trong tổng thể, có thể cần được điều tra thêm.\nPSI ≥ 0.25: Sự thay đổi đáng kể về tổng thể. Mô hình có thể không còn phù hợp và cần được cập nhật hoặc xây dựng lại.\n\nLưu ý rằng những ngưỡng này không phải là cố định và có thể thay đổi tùy thuộc vào đặc điểm cụ thể của tình huống và mức độ rủi ro bạn sẵn sàng chấp nhận.\n\nimport numpy as np\nimport warnings\n\ndef calculate_psi(expected, actual, categorical=False, bins=None):\n    \"\"\"\n    Calculate the Population Stability Index (PSI) between two sets of data.\n\n    Args:\n        expected (numpy.ndarray): The expected values.\n        actual (numpy.ndarray): The actual values to be compared against the expected values.\n        categorical (bool, optional): Set to True if the variables are categorical. Default is False.\n        bins (int, optional): Number of bins for binning numeric data. Default is None (auto).\n\n    Returns:\n        float: The calculated PSI value.\n    \"\"\"\n    # Check if the variables are categorical\n    if categorical:\n        # Get unique categories\n        categories = np.unique(np.concatenate([expected, actual]))\n\n        # Issue a warning if the number of unique categories exceeds 20\n        if len(categories) &gt; 20:\n            warnings.warn(\"Warning: Number of unique categories exceeds 20.\")\n\n        # Calculate the expected and actual proportions for each category\n        expected_probs = np.array([np.sum(expected == cat) for cat in categories]) / len(expected)\n        actual_probs = np.array([np.sum(actual == cat) for cat in categories]) / len(actual)\n    else:\n        expected = expected[~np.isnan(expected)]\n        actual = actual[~np.isnan(actual)]\n        # Apply binning for numeric columns\n        if bins is None:\n            bins = 10  # Default to 10 bins, you can change this value as needed\n\n        # Calculate the bin edges based on percentiles\n        bin_edges = np.percentile(np.hstack((expected, actual)), np.linspace(0, 100, bins + 1))\n\n        # Calculate the expected and actual proportions for each bin\n        expected_probs, _ = np.histogram(expected, bins=bin_edges)\n        actual_probs, _ = np.histogram(actual, bins=bin_edges)\n\n        # Normalize to get proportions\n        expected_probs = expected_probs / len(expected)\n        actual_probs = actual_probs / len(actual)\n        \n    # Initialize PSI\n    psi_value = 0\n\n    # Loop over each bin or category\n    for i in range(len(expected_probs)):\n        # Avoid division by zero and log of zero\n        if expected_probs[i] == 0 or actual_probs[i] == 0:\n            continue\n        # Calculate the PSI for this bin or category\n        psi_value += (expected_probs[i] - actual_probs[i]) * np.log(expected_probs[i] / actual_probs[i])\n\n    return psi_value\n\n# Example usage\nexpected_data = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 6, 6, 7, 8, 9, 10])\nactual_data = np.array([1, 1, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 7, 8, 9, 10])\n\npsi = calculate_psi(expected_data, actual_data, categorical=False, bins=5)\nprint(\"Population Stability Index (PSI):\", psi)\n\nPopulation Stability Index (PSI): 0.028379201320332823",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html#ks-kolmogorov-smirnov",
    "href": "statistic/PSI_KS_ENTROPY.html#ks-kolmogorov-smirnov",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "KS (Kolmogorov-Smirnov)",
    "text": "KS (Kolmogorov-Smirnov)\nKiểm định KS (Kolmogorov-Smirnov) là một kiểm định thống kê phi tham số dùng để so sánh hai phân phối tích lũy (CDFs) hoặc một mẫu dữ liệu với một phân phối lý thuyết. Nó có hai ứng dụng chính:\n\nKiểm tra sự phù hợp của mẫu: Được sử dụng để kiểm tra xem một tập dữ liệu có tuân theo một phân phối lý thuyết cụ thể (như phân phối chuẩn, phân phối đều, v.v.) hay không.\nSo sánh hai mẫu dữ liệu: Được sử dụng để kiểm tra xem hai tập dữ liệu có xuất phát từ cùng một phân phối gốc hay không.\n\nCông thức tính toán cho chỉ số KS là:\n\\[ D = \\max |F_1(x) - F_2(x)| \\]\nTrong đó: - ( F_1(x) ) và ( F_2(x) ) là hai hàm phân phối tích lũy cần so sánh. - ( D ) là giá trị lớn nhất của sự khác biệt tuyệt đối giữa hai hàm phân phối tích lũy trên toàn bộ phạm vi x.\nMột đặc điểm quan trọng của kiểm định KS là nó không yêu cầu giả định về dạng của phân phối, làm cho nó trở thành một công cụ mạnh mẽ và linh hoạt khi so sánh phân phối.\n\nfrom scipy.stats import ks_2samp\n\n# Generate two sample datasets\ndata1 = np.random.normal(0, 1, 1000)\ndata2 = np.random.normal(0.5, 1.5, 1000)\n\n# Compute the KS statistic and p-value\nks_statistic, p_value = ks_2samp(data1, data2)\n\nks_statistic, p_value\n\n(0.221, 8.402229010639847e-22)",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html#divergence-test-entropy",
    "href": "statistic/PSI_KS_ENTROPY.html#divergence-test-entropy",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "Divergence Test (entropy)",
    "text": "Divergence Test (entropy)\nKiểm định Divergence, thường được gọi là Divergence Kullback-Leibler (KL), là một chỉ số đo sự khác biệt giữa một phân phối xác suất so với một phân phối xác suất thứ hai mong đợi. Nó được sử dụng để so sánh hai phân phối xác suất cho cùng một sự kiện.\nCho hai phân phối xác suất, \\(P\\) và \\(Q\\), Divergence Kullback-Leibler của \\(Q\\) so với \\(P\\) được định nghĩa như sau:\n\\[ D_{KL}(P||Q) = \\sum_{i} P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)\\]\nTrong đó: - \\(P(i)\\) là xác suất của sự kiện \\(i\\) theo phân phối \\(P\\), - \\(Q(i)\\) là xác suất của sự kiện \\(i\\) theo phân phối \\(Q\\), - Tổng được tính trên tất cả các sự kiện \\(i\\) có thể xảy ra.\nMột số điểm quan trọng về KL Divergence:\n\nKhông đối xứng: \\(D_{KL}(P||Q)\\) không bằng \\(D_{KL}(Q||P)\\). Điều này có nghĩa là Divergence KL của \\(Q\\) so với \\(P\\) không giống như Divergence KL của \\(P\\) so với \\(Q\\).\nKhông âm: Divergence KL luôn không âm, và nó bằng không chỉ khi \\(P\\) và \\(Q\\) là cùng một phân phối.\nĐơn vị: Divergence KL được đo bằng bit nếu logarithm có cơ số 2 (log2), hoặc bằng nats nếu logarithm có cơ số \\(e\\) (logarithm tự nhiên).\n\nTrên thực tế, KL Divergence có thể được sử dụng để đo sự khác biệt giữa phân phối thực tế và dự đoán, hoặc giữa một phân phối quan sát và một phân phối lý thuyết. Nó đặc biệt phổ biến trong các lĩnh vực như lý thuyết thông tin và học máy.\n\nimport numpy as np\n\ndef kl_divergence(p, q):\n    \"\"\"Compute KL divergence of two probability distributions.\"\"\"\n    return np.sum(p * np.log(p / q))\n\n# Example distributions\np = np.array([0.4, 0.5, 0.1])\nq = np.array([0.3, 0.4, 0.3])\n\n# Ensure the distributions are valid (i.e., sum to 1 and non-negative)\nassert np.all(p &gt;= 0) and np.all(q &gt;= 0)\nassert np.isclose(p.sum(), 1) and np.isclose(q.sum(), 1)\n\n# Calculate KL Divergence\ndivergence_value = kl_divergence(p, q)\nprint(f\"KL Divergence between p and q: {divergence_value:.4f}\")\n\nKL Divergence between p and q: 0.1168\n\n\n\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef kl_divergence(p, q):\n    \"\"\"Compute KL divergence of two probability distributions.\"\"\"\n    return entropy(p, q)\n\n# Example distributions\np = np.array([0.4, 0.5, 0.1])\nq = np.array([0.3, 0.4, 0.3])\n\n# Calculate KL Divergence from p to q\ndivergence_value = kl_divergence(p, q)\n\nprint(f\"KL Divergence from p to q: {divergence_value:.4f}\")\n\nKL Divergence from p to q: 0.1168",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html#so-sánh-ks-và-divergence",
    "href": "statistic/PSI_KS_ENTROPY.html#so-sánh-ks-và-divergence",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "So sánh KS và Divergence",
    "text": "So sánh KS và Divergence\n\nMục đích:\n\nKiểm định KS: Đây là một kiểm định phi tham số được sử dụng để xác định xem hai mẫu có xuất phát từ cùng một phân phối hay không. Thống kê KS đo sự khác biệt lớn nhất giữa các hàm phân phối tích lũy (CDFs) của hai mẫu.\nKiểm định Divergence (KL Divergence): Nó đo cách một phân phối xác suất khác biệt so với một phân phối xác suất thứ hai mong đợi. Nó thường được sử dụng trong lý thuyết thông tin để đo “khoảng cách” giữa hai phân phối.\n\nKết quả:\n\nKiểm định KS: Kết quả là một chỉ số (D) đại diện cho sự khác biệt lớn nhất giữa hai CDFs và một giá trị p kiểm tra giả thuyết rằng hai mẫu được rút ra từ cùng một phân phối.\nKiểm định Divergence (KL Divergence): Kết quả là một giá trị không âm, trong đó giá trị 0 chỉ ra rằng hai phân phối là giống nhau. Lưu ý rằng KL Divergence không đối xứng, tức là \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\).\n\nGiả định:\n\nKiểm định KS: Không giả định về phân phối của dữ liệu.\nKiểm định Divergence (KL Divergence): Giả định \\(Q(i) &gt; 0\\) cho bất kỳ \\(i\\) nào sao cho \\(P(i) &gt; 0\\), nếu không sự khác biệt sẽ vô cùng.\n\nỨng dụng:\n\nKiểm định KS: Thường được sử dụng trong kiểm định giả thuyết để xác định xem một mẫu dữ liệu có tuân theo một phân phối cụ thể hay không.\nKiểm định Divergence (KL Divergence): Rộng rãi được sử dụng trong lý thuyết thông tin, học máy và thống kê, đặc biệt khi so sánh một phân phối thực nghiệm với một phân phối lý thuyết.\n\nGiải thích:\n\nKiểm định KS: Một giá trị p nhỏ cho thấy rằng hai mẫu đến từ các phân phối khác nhau.\nKiểm định Divergence (KL Divergence): Một Divergence KL lớn hơn chỉ ra rằng hai phân phối khác biệt hơn so với nhau.\n\n\nTóm lại, mặc dù cả Kiểm định KS và KL Divergence đều được sử dụng để so sánh các phân phối, nhưng chúng có các phương pháp, giải thích và ứng dụng khác nhau. Sự lựa chọn giữa chúng phụ thuộc vào vấn đề cụ thể và bản chất của dữ liệu.",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/PSI_KS_ENTROPY.html#so-sánh-giá-trị-của-các-kiểm-định",
    "href": "statistic/PSI_KS_ENTROPY.html#so-sánh-giá-trị-của-các-kiểm-định",
    "title": "Giám sát độ ổn định của mô hình",
    "section": "So sánh giá trị của các kiểm định",
    "text": "So sánh giá trị của các kiểm định\nGiá trị tối thiểu (min) và tối đa (max) của KS Statistic và KL Divergence có thể xảy ra trong các trường hợp sau:\n\nKS Statistic (Kolmogorov-Smirnov Statistic):\n\nMin: Giá trị KS Statistic sẽ gần bằng 0 khi hai phân phối hoàn toàn giống nhau hoặc rất giống nhau. Điều này có nghĩa rằng các dãy số có thể hoàn toàn trùng nhau.\nMax: Giá trị KS Statistic sẽ đạt tới giá trị lớn nhất khi hai phân phối hoàn toàn khác biệt và không có sự tương đồng nào giữa chúng. Trong trường hợp này, hai phân phối có dãy số hoàn toàn không trùng nhau.\n\nKL Divergence (Kullback-Leibler Divergence):\n\nMin: KL Divergence sẽ gần bằng 0 khi hai phân phối hoàn toàn giống nhau. Điều này có nghĩa rằng không có sự khác biệt nào giữa hai phân phối về thông tin.\nMax: Giá trị KL Divergence sẽ lớn nhất khi hai phân phối hoàn toàn khác biệt và không chia sẻ bất kỳ thông tin nào. Trong trường hợp này, phân phối này không thể ước lượng hoặc dự đoán từ phân phối kia.\n\n\n\nimport numpy as np\nfrom scipy.stats import ks_2samp\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\n\n# Number of examples\nnum_examples = 5\n\n# Create a figure and axis for the plot\nfig, axs = plt.subplots(num_examples, 2, figsize=(12, 16))\nfig.subplots_adjust(hspace=0.5)\n\nfor example in range(num_examples):\n    # Generate two sample datasets with different loc and scale parameters\n    np.random.seed(example)\n    loc1 = np.random.uniform(-2, 2)  # Random loc for sample1 between -1 and 1\n    scale1 = np.random.uniform(0.5, 2)  # Random scale for sample1 between 0.5 and 2\n    sample1 = np.random.normal(loc1, scale1, 1000)\n    \n    loc2 = np.random.uniform(1, 10)  # Random loc for sample2 between -1 and 1\n    scale2 = np.random.uniform(0.5, 2)  # Random scale for sample2 between 0.5 and 2\n    sample2 = np.random.normal(loc2, scale2, 1000)\n    \n    # Compute KS statistic and p-value\n    ks_statistic, ks_p_value = ks_2samp(sample1, sample2)\n    \n    # Compute KL Divergence\n    hist_sample1, _ = np.histogram(sample1, bins=10)\n    hist_sample2, _ = np.histogram(sample2, bins=10)\n    kl_divergence_value = entropy(hist_sample1, hist_sample2)\n    \n    # Plot the histograms of the two samples with mean and std in the legend\n    legend_label = f'Sample 1\\nMean: {np.mean(sample1):.2f}\\nStd: {np.std(sample1):.2f}'\n    legend_label_sample2 = f'Sample 2\\nMean: {np.mean(sample2):.2f}\\nStd: {np.std(sample2):.2f}'\n    axs[example, 0].hist(sample1, bins=30, alpha=0.5, color='blue', label=legend_label)\n    axs[example, 0].hist(sample2, bins=30, alpha=0.5, color='orange', label=legend_label_sample2)\n    axs[example, 0].set_title(f'Example {example + 1}: Distributions')\n    axs[example, 0].legend()\n    \n    # Plot KS Statistic and KL Divergence with labels in the legend\n    axs[example, 1].bar(['KS Statistic', 'KL Divergence'], [ks_statistic, kl_divergence_value], color=['blue', 'green'])\n    legend_label = f'KS Statistic: {ks_statistic:.4f}\\nKL Divergence: {kl_divergence_value:.4f}'\n    axs[example, 1].set_title(f'Example {example + 1}: Comparison')\n    axs[example, 1].legend([legend_label])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nKhi 2 phân phối có khác biệt về giá trị trung bình thì KS sẽ khác nhau\nCòn khi khác biệt về dạng phân phối thì Entropy sẽ khác nhau\n\n\nimport numpy as np\nfrom scipy.stats import ks_2samp\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\n\n# Number of examples\nnum_examples = 5\n\n# Create a figure and axis for the plot\nfig, axs = plt.subplots(num_examples, 2, figsize=(12, 16))\nfig.subplots_adjust(hspace=0.5)\n\nfor example in range(num_examples):\n    # Generate two sample datasets with different loc and scale parameters\n    np.random.seed(example)\n    loc1 = np.random.uniform(-2, 2)  # Random loc for sample1 between -1 and 1\n    scale1 = np.random.uniform(0.5, 2)  # Random scale for sample1 between 0.5 and 2\n    sample1 = np.random.normal(loc1, scale1, 1000)\n    \n    add_value = np.random.uniform(1, 10)  # Random loc for sample2 between -1 and 1    \n    sample2 = sample1 + add_value\n    \n    # Compute KS statistic and p-value\n    ks_statistic, ks_p_value = ks_2samp(sample1, sample2)\n    \n    # Compute KL Divergence\n    hist_sample1, _ = np.histogram(sample1, bins=10)\n    hist_sample2, _ = np.histogram(sample2, bins=10)\n    kl_divergence_value = entropy(hist_sample1, hist_sample2)\n    \n    # Plot the histograms of the two samples with mean and std in the legend\n    legend_label = f'Sample 1\\nMean: {np.mean(sample1):.2f}\\nStd: {np.std(sample1):.2f}'\n    legend_label_sample2 = f'Sample 2\\nMean: {np.mean(sample2):.2f}\\nStd: {np.std(sample2):.2f}'\n    axs[example, 0].hist(sample1, bins=30, alpha=0.5, color='blue', label=legend_label)\n    axs[example, 0].hist(sample2, bins=30, alpha=0.5, color='orange', label=legend_label_sample2)\n    axs[example, 0].set_title(f'Example {example + 1}: Distributions')\n    axs[example, 0].legend()\n    \n    # Plot KS Statistic and KL Divergence with labels in the legend\n    axs[example, 1].bar(['KS Statistic', 'KL Divergence'], [ks_statistic, kl_divergence_value], color=['blue', 'green'])\n    legend_label = f'KS Statistic: {ks_statistic:.4f}\\nKL Divergence: {kl_divergence_value:.4f}'\n    axs[example, 1].set_title(f'Example {example + 1}: Comparison')\n    axs[example, 1].legend([legend_label])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nSử dụng 2 loại phân phối khác nhau, giá trị Entropy có sự khác biệt rõ rệt\n\n\nimport numpy as np\nfrom scipy.stats import ks_2samp, beta, gamma\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\n\n# Function to calculate KL Divergence\ndef kl_divergence(p, q):\n    return entropy(p, q)\n\n# Number of examples\nnum_examples = 5\n\n# Create a figure and axis for the plot\nfig, axs = plt.subplots(num_examples, 2, figsize=(12, 16))\nfig.subplots_adjust(hspace=0.5)\n\nfor example in range(num_examples):\n    # Generate two sample datasets with different loc and scale parameters\n    np.random.seed(example)\n    \n    # Sample1 follows a gamma distribution\n    shape1 = np.random.uniform(1, 5)  # Random shape parameter for gamma distribution\n    scale1 = np.random.uniform(0.5, 2)  # Random scale parameter for gamma distribution\n    sample1 = gamma.rvs(shape1, scale=scale1, size=1000)\n    \n    # Sample2 follows a beta distribution\n    a2 = np.random.uniform(1, 10)  # Random shape parameter for beta distribution\n    b2 = np.random.uniform(1, 10)  # Random scale parameter for beta distribution\n    sample2 = beta.rvs(a2, b2, size=1000)\n    \n    # Compute KS statistic and p-value\n    ks_statistic, ks_p_value = ks_2samp(sample1, sample2)\n    \n    # Compute KL Divergence\n    hist_sample1, _ = np.histogram(sample1, bins=10)\n    hist_sample2, _ = np.histogram(sample2, bins=10)\n    kl_divergence_value = kl_divergence(hist_sample1, hist_sample2)\n    \n    # Plot the histograms of the two samples with mean and std in the legend\n    legend_label_sample1 = f'Sample 1 (Gamma)\\nShape: {shape1:.2f}, Scale: {scale1:.2f}\\nMean: {np.mean(sample1):.2f}\\nStd: {np.std(sample1):.2f}'\n    legend_label_sample2 = f'Sample 2 (Beta)\\na={a2:.2f}, b={b2:.2f}\\nMean: {np.mean(sample2):.2f}\\nStd: {np.std(sample2):.2f}'\n    axs[example, 0].hist(sample1, bins=30, alpha=0.5, color='blue', label=legend_label_sample1)\n    axs[example, 0].hist(sample2, bins=30, alpha=0.5, color='orange', label=legend_label_sample2)\n    axs[example, 0].set_title(f'Example {example + 1}: Distributions')\n    axs[example, 0].legend()\n    \n    # Plot KS Statistic and KL Divergence with labels in the legend\n    axs[example, 1].bar(['KS Statistic', 'KL Divergence'], [ks_statistic, kl_divergence_value], color=['blue', 'green'])\n    legend_label = f'KS Statistic: {ks_statistic:.4f}\\nKL Divergence: {kl_divergence_value:.4f}'\n    axs[example, 1].set_title(f'Example {example + 1}: Comparison')\n    axs[example, 1].legend([legend_label])\n\nplt.show()",
    "crumbs": [
      "Statistic",
      "Giám sát độ ổn định của mô hình"
    ]
  },
  {
    "objectID": "statistic/DiD.html",
    "href": "statistic/DiD.html",
    "title": "Difference-in-Differences (DiD)",
    "section": "",
    "text": "Sự khác biệt trong khác biệt (DiD) là một phương pháp thống kê thường được sử dụng trong đánh giá tác động để ước tính tác động nhân quả của một phương pháp điều trị hoặc sự thay đổi của chính sách nào đó. Phương pháp này rất hữu hiệu khi bạn có dữ liệu quan sát và muốn so sánh kết quả của một nhóm được can thiệp hay bị ảnh hưởng từ chính sách (the “treatment group”) với kết quả của một nhóm không can thiệp (the “control group”) trước và sau khi việc can thiệp được thực hiện. Dưới đây là các bước chính để tiến hành đánh giá tác động bằng DiD:\n\nXác định nhóm bị can thiệp và nhóm đối chứng (Treatment and Control Groups):\n\nXác định nhóm bị can thiệp hoặc thay đổi chính sách.\nLựa chọn nhóm đối chứng tương tự như nhóm can thiệp nhưng chưa bị tác động của chính sách. Nhóm này lý tưởng nhất phải có những đặc điểm và xu hướng tương tự theo thời gian.\n\nThu thập dữ liệu:\n\nThu thập dữ liệu về biến kết quả được quan tâm của cả nhóm can thiệp và nhóm đối chứng trong nhiều khoảng thời gian, trước và sau can thiệp. Bạn cần ít nhất hai thời điểm trước và hai thời điểm sau can thiệp.\n\nGiả định về xu hướng song song:\n\nMột giả định quan trọng trong phân tích DiD là nếu không có biện pháp can thiệp, nhóm can thiệp và đối chứng sẽ có xu hướng song song theo thời gian. Nói cách khác, kết quả của họ sẽ tiến triển tương tự nếu việc can thiệp không xảy ra.\n\nƯớc tính mô hình DiD:\n\nChạy mô hình hồi quy bao gồm các biến chỉ số cho cả nhóm can thiệp và các khoảng thời gian sau can thiệp. Công cụ ước tính DiD là hệ số về sự tương tác giữa các biến chỉ báo này.\n\nMô hình DiD:\n\\[Y_{it} = β_0 + β_1 * Treatment_i + β_2 * Post_t + β_3 * (Treatment_i * Post_t) + ε_{it}\\]\n\n\\(Y_{it}\\) là biến kết quả của cá nhân i tại thời điểm t.\n\\(Treatment_i\\) là chỉ báo nhị phân cho biết cá nhân i thuộc nhóm can thiệp (1) hay nhóm đối chứng (0).\n\\(Post_t\\) là chỉ báo nhị phân cho biết khoảng thời gian là sau can thiệp (1) hay trước (0).\n\\(β_3\\) là ước lượng DiD, biểu thị hiệu quả can thiệp.\n\nGiải thích kết quả:\n\nHệ số \\(β_3\\) thể hiện hiệu quả can thiệp ước tính. Giá trị dương cho biết chính sách có tác động tích cực đến kết quả, giá trị âm cho biết tác động tiêu cực.\n\nKiểm tra ổn định:\n\nTiến hành phân tích độ nhạy để đánh giá độ tin cậy của kết quả. Điều này có thể bao gồm việc thử nghiệm các tiêu chí đánh giá khác nhau của nhóm kiểm soát, khoảng thời gian hoặc hệ số của mô hình.\n\nGiải quyết các vấn đề tiềm ẩn:\n\nGiải quyết mọi vấn đề tiềm ẩn như sai lệch lựa chọn, lỗi đo lường hoặc tính nội sinh có thể ảnh hưởng đến tính hợp lệ của kết quả của bạn.\n\n\n\nTừ công thức hồi quy DiD:\n\\[Y_{it} = β_0 + β_1 * Treatment_i + β_2 * Post_t + β_3 * (Treatment_i * Post_t) + ε_{it}\\]\nTa có thể tách riêng công thức cho nhóm treatment và nhóm control như sau:\n\nCho nhóm treatment (Treatment = 1):\n\n\\[Y_{it}^{Treatment} = (β_0 + β_1) + (β_2 + β_3) * Post_t + ε_{it}^{Treatment}\\]\n\nCho nhóm control (Treatment = 0):\n\n\\[Y_{it}^{Control} = β_0 + β_2 * Post_t + ε_{it}^{Control}\\]\nTrong đó:\n\n\\(Y_{it}^{Treatment}\\) là biến phản ứng (ví dụ: chi tiêu trung bình hàng tháng) cho nhóm treatment tại thời điểm \\(t\\).\n\\(Y_{it}^{Control}\\) là biến phản ứng cho nhóm control tại thời điểm \\(t\\).\n\\(β_0\\) là hệ số chặn (intercept) chung cho cả hai nhóm.\n\\(β_1\\) là hệ số ước lượng cho biến Treatment (nhóm treatment vs. nhóm control).\n\\(β_2\\) là hệ số ước lượng cho biến Post (thời gian sau can thiệp vs. trước can thiệp).\n\\(β_3\\) là hệ số ước lượng cho biến tương tác (Treatment và Post).\n\\(ε_{it}^{Treatment}\\) và \\(ε_{it}^{Control}\\) là thành phần sai số (error term) tương ứng cho từng nhóm.\n\nCông thức này cho phép bạn tính ước lượng cho từng nhóm riêng biệt và thấy cách can thiệp (Treatment) ảnh hưởng đến biến phản ứng (Y) trong từng nhóm.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulated data\nnp.random.seed(0)\n\n# Create a DataFrame with pre- and post-reform data for treatment and control groups\ndata = pd.DataFrame({\n    'Group': ['A', 'A', 'B', 'B'],\n    'Period': ['Before', 'After', 'Before', 'After'],\n    'Credit_Limit': [5000, 5300, 5200, 5100],  # Simulated credit limit data\n    'Spending': [3500, 4100, 4200, 4300],  # Simulated credit card spending\n})\n\n# Calculate the DiD estimate using the parallel trends assumption\ncontrol_group_before_mean = data[(data['Group'] == 'B') & (data['Period'] == 'Before')]['Spending'].mean()\ncontrol_group_after_mean = data[(data['Group'] == 'B') & (data['Period'] == 'After')]['Spending'].mean()\n\ntreatment_group_before_mean = data[(data['Group'] == 'A') & (data['Period'] == 'Before')]['Spending'].mean()\n\ncounterfactual_treatment_group_after = treatment_group_before_mean + (control_group_after_mean - control_group_before_mean)\ncounterfactual_control_group_after = control_group_before_mean  # Counterfactual for control group\n\n# Visualize the results with line graphs\nplt.figure(figsize=(10, 6))\nplt.title('Difference-in-Differences (DiD) Estimate: Impact on Credit Card Spending')\nplt.xlabel('Time Period')\nplt.ylabel('Average Monthly Credit Card Spending')\nplt.grid(True)\n\n# Plot spending for the treatment group before and after the intervention\ntreatment_group = data[data['Group'] == 'A']\nplt.plot(treatment_group['Period'], treatment_group['Spending'], marker='o', label='Treatment Group (A)')\n\n# Plot spending for the control group before and after the intervention\ncontrol_group = data[data['Group'] == 'B']\nplt.plot(control_group['Period'], control_group['Spending'], marker='x', label='Control Group (B)')\n\n# Plot spending for the treatment group assuming no intervention (counterfactual for treatment group)\nplt.plot(treatment_group['Period'], [treatment_group_before_mean, counterfactual_treatment_group_after],\n         linestyle='--', label='Treatment Group (No Intervention)')\n\n# Plot spending for the control group assuming no intervention (counterfactual for control group)\nplt.plot(control_group['Period'], [control_group_before_mean, counterfactual_control_group_after],\n         linestyle='--', label='Control Group (No Intervention)')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# Simulated data\nnp.random.seed(0)\n\n# Create a DataFrame with pre- and post-intervention data for treatment and control groups\nyears = list(range(2010, 2021))\ntreatment_status = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]  # 0 for control, 1 for treatment\ncredit_limit = [5000, 5000, 5000, 5000, 5000, 10000, 10000, 10000, 10000, 10000, 10000]\nspending = [3500, 4000, 4200, 4800, 5500, 8000, 8500, 9000, 9200, 9500, 9800]\n\ndata = pd.DataFrame({\n    'Year': years,\n    'Treatment': treatment_status,\n    'Credit_Limit': credit_limit,\n    'Spending': spending,\n})\n\n# Create an indicator variable for the post-intervention period\ndata['Post'] = (data['Year'] &gt;= 2015).astype(int)\n\n# Create the interaction term 'Treatment*Post'\ndata['Treatment*Post'] = data['Treatment'] * data['Post']\n\n# Run a DiD regression\nX = data[['Treatment', 'Post', 'Treatment*Post', 'Credit_Limit']]\nX = sm.add_constant(X)  # Add a constant term\ny = data['Spending']\n\n\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Spending   R-squared:                       0.927\nModel:                            OLS   Adj. R-squared:                  0.919\nMethod:                 Least Squares   F-statistic:                     113.9\nDate:                Thu, 07 Sep 2023   Prob (F-statistic):           2.08e-06\nTime:                        10:09:13   Log-Likelihood:                -86.750\nNo. Observations:                  11   AIC:                             177.5\nDf Residuals:                       9   BIC:                             178.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst            -49.9999    174.960     -0.286      0.782    -445.788     345.788\nTreatment         50.0000    174.960      0.286      0.782    -345.788     445.788\nPost              50.0000    174.960      0.286      0.782    -345.788     445.788\nTreatment*Post    50.0000    174.960      0.286      0.782    -345.788     445.788\nCredit_Limit       0.8900      0.035     25.434      0.000       0.811       0.969\n==============================================================================\nOmnibus:                        0.470   Durbin-Watson:                   1.375\nProb(Omnibus):                  0.791   Jarque-Bera (JB):                0.496\nSkew:                           0.039   Prob(JB):                        0.780\nKurtosis:                       1.962   Cond. No.                     1.63e+37\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.71e-66. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\nc:\\Users\\binhnn2\\anaconda3\\envs\\rdm\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1736: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=11\n  warnings.warn(\"kurtosistest only valid for n&gt;=20 ... continuing \"\n\n\nTác động của các hệ số:\n\n\\(β_1\\) đo lường tác động của can thiệp (Treatment) lên biến phản ứng (\\(Y_{it}\\)) cho nhóm treatment so với nhóm control trước can thiệp. Nếu \\(β_1\\) có ý nghĩa thống kê, điều này ngụ ý rằng nếu không có bất kỳ can thiệp gì thì nhóm treatment đã khác nhóm control một lượng là \\(β_1\\).\n\\(β_2\\) đo lường tác động của thời gian (Post) lên biến phản ứng (\\(Y_{it}\\)) cho nhóm control. Nếu \\(β_2\\) dương và có ý nghĩa thống kê, nếu không có bất kỳ hành động can thiệp nào thì khoảng thời gian sau năm 2015 vẫn chi tiêu cao hơn trước.\n\\(β_3\\) đo lường hiệu ứng can thiệp (DiD) trên biến phản ứng (\\(Y_{it}\\)). Nếu \\(β_3\\) dương và có ý nghĩa thống kê, điều này ngụ ý rằng can thiệp đã tạo ra sự thay đổi khác biệt giữa nhóm treatment và nhóm control sau can thiệp. Điều này là ước lượng chính của bạn trong mô hình DiD.\n\n\n# Visualize the results\nplt.figure(figsize=(12, 6))\nplt.title('Difference-in-Differences Analysis: Impact on Credit Card Spending')\nplt.axvline(x=2015, color='red', linestyle='--', label='Intervention Year')\n\n# Plot the control group\ncontrol_group = data[data['Treatment'] == 0]\nplt.plot(control_group['Year'], control_group['Spending'], marker='o', label='Control Group')\n\n# Plot the treatment group\ntreatment_group = data[data['Treatment'] == 1]\nplt.plot(treatment_group['Year'], treatment_group['Spending'], marker='x', label='Treatment Group')\n\nplt.xlabel('Year')\nplt.ylabel('Average Monthly Credit Card Spending')\nplt.legend()\nplt.grid(True)\n\nplt.show()",
    "crumbs": [
      "Statistic",
      "Difference-in-Differences (DiD)"
    ]
  },
  {
    "objectID": "learning-more.html",
    "href": "learning-more.html",
    "title": "Learning more",
    "section": "",
    "text": "An excellent overview: Reproducible authoring with Quarto - Mine Çetinkaya-Rundel, Feb 2022 - slides, youtube\nA Quarto tip a day in June 2022, from Mine Çetinkaya-Rundel.\n\n\n\nOpenscapes Champions Lessons Series\nOpenscapes Approach Guide\n\nNASA Earthdata Cloud Cookbook\n\nSee many more examples at the quarto gallery!\n\n\n\nAre you making onboarding documentation? Check out The Fay Lab Manual (now in Quarto!) for inspiration on structure - you could also start there and make it your own.",
    "crumbs": [
      "Learning more"
    ]
  },
  {
    "objectID": "learning-more.html#learn-more",
    "href": "learning-more.html#learn-more",
    "title": "Learning more",
    "section": "",
    "text": "An excellent overview: Reproducible authoring with Quarto - Mine Çetinkaya-Rundel, Feb 2022 - slides, youtube\nA Quarto tip a day in June 2022, from Mine Çetinkaya-Rundel.\n\n\n\nOpenscapes Champions Lessons Series\nOpenscapes Approach Guide\n\nNASA Earthdata Cloud Cookbook\n\nSee many more examples at the quarto gallery!\n\n\n\nAre you making onboarding documentation? Check out The Fay Lab Manual (now in Quarto!) for inspiration on structure - you could also start there and make it your own.",
    "crumbs": [
      "Learning more"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html",
    "href": "quarto-workflows/jupyter.html",
    "title": "From Jupyter",
    "section": "",
    "text": "You can interact with Quarto through JupyterLab or JupyterHub. Your Jupyter setup will involve .ipynb notebooks and the command line. Quarto’s JupyterLab tutorials has great instructions on getting started with JupyterLab, including computations and authoring.\nHere we will demonstrate how to work with this Quarto tutorial site in JupyterHub and add a Jupyter Notebook (.ipynb file). This example uses the NASA-Openscapes JupyterHub that already has all python environments as well as Quarto installed.",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#setup",
    "href": "quarto-workflows/jupyter.html#setup",
    "title": "From Jupyter",
    "section": "Setup",
    "text": "Setup\n\nJupyterHub\nOur JupyterHub is already setup with python environments as well as Quarto (through nasa-openscapes/corn), so there is no further installation required.\n\n\nClone your repo\nYou’ll start by cloning your repository into JupyterHub. Do this by opening a terminal (File &gt; New &gt; Terminal). In the Terminal, git clone your repository and cd into it:\ngit clone https://github.com/openscapes/quarto-website-tutorial\ncd quarto-website-tutorial\n\n\nInstall Quarto\nNot needed - Quarto is already installed on the NASA-Openscapes JupyterHub! But to install elsewhere you would do so from https://quarto.org/docs/get-started/.\nQuarto is a Command Line Interface (CLI), like git. Once download is complete, follow the installation prompts on your computer like you do for other software. You won’t see an application to click on when it is installed.\nNote for Mac users: If you do not have administrative privileges, please select “Install for me only” during the Destination Selection installation step (you will first click on “Change Install Location” at the Installation Type step).\nYou can check to confirm that Quarto is installed properly from the command line:\nquarto check install\n\n\n\n\n\n\nAdditional checks\n\n\n\n\n\nYou can also run:\n\nquarto check knitr to locate R, verify we have the rmarkdown package, and do a basic render\nquarto check jupyter to locate Python, verify we have Jupyter, and do a basic render\nquarto check to run all of these checks together\n\n\n\n\n\n\n\n\n\n\nHistorical aside: Install Quarto in a docker container\n\n\n\n\n\nIn Summer 2021 some NASA Mentors trying to install quarto locally was not an option, but they were able to install it inside a container using the following Dockerfile:\n#| fold: true\n#| summary: \"Show the Dockerfile\"\n\n##############################\n# This Dockerfile installs quarto and then runs quarto serve against the\n# internal /home/quarto/to_serve.\n#\n# BUILD\n# -----\n# To build this container, run\n#\n#     docker build -t quarto_serve .\n#\n# Add the --no-cache option to force docker to build fresh and get the most\n# recent version of quarto.\n#\n#\n# RUN\n# ---\n# 1. Find the directory you want quarto to serve. Let's call this /PATH/TO/earthdata-cloud-cookbook.\n# 2. Run docker:\n#\n#     docker run --rm -it -p 4848:4848 -v /PATH/TO/earthdata-cloud-cookbook:/home/quarto/to_serve quarto_serve\n#\n# 3. Open your browser and go to http://127.0.0.1:4848/\n#\n##############################\n\nFROM ubuntu:hirsute\n\n######\n# Install some command line tools we'll need\n######\nRUN apt-get update\nRUN apt-get -y install wget\nRUN apt-get -y install gdebi-core\nRUN apt-get -y install git\n\n\n######\n# Install quarto (https://quarto.org/)\n######\n\n# This is a quick and dirty way of getting the newest version number from\n# https://github.com/quarto-dev/quarto-cli/releases/latest. What's happening is\n# we're pulling the version number out of the redirect URL. This will end up\n# with QVER set to something like 0.2.11.\nRUN QVER=`wget --max-redirect 0 https://github.com/quarto-dev/quarto-cli/releases/latest 2&gt;&1 | grep \"Location\" | sed 's/L.*tag\\/v//' | sed 's/ .*//'` \\\n    && wget -O quarto.deb \"https://github.com/quarto-dev/quarto-cli/releases/download/v$QVER/quarto-$QVER-amd64.deb\"\nRUN gdebi -n quarto.deb\n\n# Run this to make sure quarto installed correctly\nRUN quarto check install\n\n\n######\n# Create a non-root user called quarto\n######\nRUN useradd -ms /bin/bash quarto\nUSER quarto\nRUN mkdir /home/quarto/to_serve\nWORKDIR /home/quarto/to_serve\n\n\n######\n# Start quarto serve\n######\n\nCMD quarto serve --no-browse --host 0.0.0.0 --port 4848",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#quarto-preview",
    "href": "quarto-workflows/jupyter.html#quarto-preview",
    "title": "From Jupyter",
    "section": "Quarto preview",
    "text": "Quarto preview\nLet’s start off by previewing our quarto site locally. In Terminal, type quarto preview, which will provide a URL with a preview of our site!\nquarto preview\n# Preparing to preview\n# Watching files for changes\n# Browse at https://openscapes.2i2c.cloud/user/jules32/proxy/4593/\nCopy this URL into another browser window; and arrange them so you can see them both. I make a bit more space in Jupyter by collapsing the left file menu by clicking on the file icon at the top of the left sidebar.\n\n\n\n\n\n\nMake a small change and preview it\nNow we’ll be able to see live changes in the preview as we edit in our .md files. Let’s try it: Change the date in index.md by opening it from the file directory. Change to today’s date, and save. Your preview window will refresh automatically! If it does not, you can also refresh the page manually. The refreshed previewed site will now display your changes!",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#create-a-new-.ipynb-page",
    "href": "quarto-workflows/jupyter.html#create-a-new-.ipynb-page",
    "title": "From Jupyter",
    "section": "Create a new .ipynb page",
    "text": "Create a new .ipynb page\nLet’s add a new page to our site. Instead of an .md file like the others, let’s add a .ipynb file.\nFile &gt; New &gt; Notebook. Accept the default kernel by clicking Select.\n\nFirst chunk: raw yaml\nBy default, this Notebook will give us a first chunk that is code. Let’s change it to raw so that we can write our yaml at the top.\n\n\n\n\n\nIn our Raw code chunk, let’s write the title of this document. We need three dashes --- on separate lines preceding and following the title:, which you can name as you’d like.\n---\ntitle: Python Example\n---\n\n\nSecond chunk: Markdown\nLet’s add a new chunk that is Markdown so we can write some description of what this page will be.\nClick the + symbol at the top of the document, and this will add a new chunk, which by default again is a Code chunk. Change it to a Markdown Chunk following the steps we did above when switching to Raw.\nHere, write a little bit of text in Markdown. Since your title is effectively a level-1 header, avoid using level-1 headers in the rest of your document. Here is some example text I wrote:\n## Introduction\n\nThis example has some Python code that will be a part of our Quarto site.\n\n\nThird chunk: Code\nNow let’s create a new chunk with the default Code setting.\nPaste the following code (or write some of your own to test):\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\nNow, go ahead and execute this code chunk like you normally would, by clicking the cursor in a code block and clicking the sideways “play” triangle to run the selected cells (and advance to the next cell). This code produces a plot.\nNote that the code runs as it normally would; the code options in the comments are just comments.\n\n\nSave your file\nSave your document - I’ll call mine python-example.ipynb in the main repository.",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#update-_quarto.yml",
    "href": "quarto-workflows/jupyter.html#update-_quarto.yml",
    "title": "From Jupyter",
    "section": "Update _quarto.yml",
    "text": "Update _quarto.yml\nNow we’ll add python-example.ipynb to our _quarto.yml file; this is where we register of all files to include in our site. Let’s add it after the section called “Basic Workflows”.\nOpen _quarto.yml by clicking on it from the file directory.\nScroll down to review the current contents in the sidebar: section. It’s there we see all the file arrangement that we see in the previewed site.\nAdd - python-example.ipynb to line 46, making sure that your indentation aligns with the other pages.\n\n\n\n\n\nYou’ll see that our new page shows up in our Preview, and the code is executed since we did that in the Jupyter Notebook itself. By default, Quarto will not execute code chunks since your computations will likely become more complex and you will want to control when they are executed (or “run”).\nSince Quarto is still previewing our website and the python-example.ipynb, the plot also displays in the notebook after the code is run and the file is saved, as shown below.\n\n\n\n\n\nSo, your normal workflow for creating and running code blocks in your Jupyter Notebook is the same one you’ll use as Quarto displays the preview.",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#quarto-render",
    "href": "quarto-workflows/jupyter.html#quarto-render",
    "title": "From Jupyter",
    "section": "Quarto render",
    "text": "Quarto render\nSo far we have used Quarto preview to view our website as we develop it. Quarto render will build the html elements of the website that we can see when we preview. Rendering will format the markdown text and code nicely as a website (or however is indicated in the _quarto.yml).\nBy default, Quarto render does not execute code in a Jupyter notebook. It will never run .ipynb files unless you tell it to.\n\nRender whole notebook\nIf you would like it to specifically execute code in a Jupyter notebook, you can do so in Terminal.\nOur Terminal is still busy previewing our website, so let’s open a new Terminal.\nFile &gt; New &gt; Terminal. Then type:\ncd quarto-website-tutorial\nquarto render python-example.ipynb --execute",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#authoring-tips",
    "href": "quarto-workflows/jupyter.html#authoring-tips",
    "title": "From Jupyter",
    "section": "Authoring tips",
    "text": "Authoring tips\nQuarto.org has details about authoring, including specific instructions about authoring in Jupyter: quarto.org/docs/reference/cells/cells-jupyter.",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#commit-and-push",
    "href": "quarto-workflows/jupyter.html#commit-and-push",
    "title": "From Jupyter",
    "section": "Commit and push!",
    "text": "Commit and push!\nCommitting and pushing will make the changes you see locally live on your website (using the GitHub Action we set up earlier).",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/jupyter.html#troubleshooting",
    "href": "quarto-workflows/jupyter.html#troubleshooting",
    "title": "From Jupyter",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nMy changes don’t show up in preview\nMake sure you’ve saved your file! There might be a slight delay depending on your JupyterHub/Lab setup.\n\n\nQuarto render hangs / does not complete\nCheck the specific notebook, are there any `—` throughout to denote line breaks rather than yaml? They might be causing the issue; consider deleting those.\nAlso check how long the first raw cell is. Are there level-1 headers (#)? Try removing them.",
    "crumbs": [
      "Quarto workflows",
      "From Jupyter"
    ]
  },
  {
    "objectID": "quarto-workflows/index.html",
    "href": "quarto-workflows/index.html",
    "title": "Quarto workflows",
    "section": "",
    "text": "How do you work in Quarto? You can use whichever tool you’re comfortable with (RStudio, Jupyter, GitHub, VS Code, etc). Developing your quarto site will have the same basic workflow, no matter which tool you use. It is very iterative, and each is explored more below.\n\nAuthoring: write text, code, images, etc in a file. Supported files include .md, .Rmd, .qmd, .ipynb…\nUpdate _quarto.yml as needed (for example, if you’ve created a new file you’d like included in your site)\nRender individual files and/or the whole website\nRepeat, repeat, repeat\nCommit and push your website to GitHub, your updates will publish automatically!\nRepeat all of the above to make the website as you’d like!\n\nNote: if editing from your internet browser we won’t render in Step 3. That step will not be separate, but combined with Step 5, which will only require a commit, not a push.",
    "crumbs": [
      "Quarto workflows"
    ]
  },
  {
    "objectID": "quarto-workflows/index.html#basic-workflow",
    "href": "quarto-workflows/index.html#basic-workflow",
    "title": "Quarto workflows",
    "section": "",
    "text": "How do you work in Quarto? You can use whichever tool you’re comfortable with (RStudio, Jupyter, GitHub, VS Code, etc). Developing your quarto site will have the same basic workflow, no matter which tool you use. It is very iterative, and each is explored more below.\n\nAuthoring: write text, code, images, etc in a file. Supported files include .md, .Rmd, .qmd, .ipynb…\nUpdate _quarto.yml as needed (for example, if you’ve created a new file you’d like included in your site)\nRender individual files and/or the whole website\nRepeat, repeat, repeat\nCommit and push your website to GitHub, your updates will publish automatically!\nRepeat all of the above to make the website as you’d like!\n\nNote: if editing from your internet browser we won’t render in Step 3. That step will not be separate, but combined with Step 5, which will only require a commit, not a push.",
    "crumbs": [
      "Quarto workflows"
    ]
  },
  {
    "objectID": "quarto-workflows/index.html#authoring",
    "href": "quarto-workflows/index.html#authoring",
    "title": "Quarto workflows",
    "section": "Authoring",
    "text": "Authoring\nAs an author, you have a lot of options of how your text will be formatted, arranged, and interlinked. You will be writing in Markdown, which is a lightweight text formatting language. The Quarto documentation about authoring introduces markdown-basics that will get you started. Also see Mine Çetinkaya-Rundel’s A Quarto tip a day.\nEach page of our site has a similar first few lines - this YAML, like we saw in our _quarto.yml and it is indicated by two sets of 3 dashes --- :\n---\ntitle: My title\n---\nYou’re able to add more features to individual pages by including it in the YAML, which for the most part here only includes a title. See Quarto excecution options for more information of what you can include in the YAML.",
    "crumbs": [
      "Quarto workflows"
    ]
  },
  {
    "objectID": "quarto-workflows/index.html#update-_quarto.yml",
    "href": "quarto-workflows/index.html#update-_quarto.yml",
    "title": "Quarto workflows",
    "section": "Update _quarto.yml",
    "text": "Update _quarto.yml\nLet’s have a closer look at the _quarto.yml file.\nThis type of file (.yml or .yaml) is written in YAML (“Yet Another Markup Language”). You’ll be able to shift the arrangement of webpages by reordering/adding/deleting them in the _quarto.yml file following the patterns you see in this example.\n\n\n\n_quarto.yml and website side-by-side\n\n\nNotice that there are multiple ways in the _quarto.yml for you to include a file in your website. For example, in the above image, the “First Observations” we see in the left sidebar of the published website (right image) is represented in _quarto.yml (left image) over two lines, with line 36 indicating the file reference and line 37 indicating the text to show up in the left sidebar. However, “From RStudio” is only represented in one line of _quarto.yml, on line 43. This represents two strategies for including a file in your website. By default, the title of a specified file will show up in the website’s sidebar, which is what is happening with the “From RStudio” example. If you would like more control over what is written in the sidebar vs the title of your files, then the approach we took with “First Observations” is what you’ll want to do: you’ll see that only “First Observations” shows up in the sidebar as we specified in _quarto.yml, but the page’s title says “First Observations & Setup” (which in our preference was too long for the sidebar).\n\n\n\n\n\n\nNote\n\n\n\nAs you modify _quarto.yml, the most important thing to know is that spacing matters. Pay attention to whether text is indented by one, two, four, or other spaces, and make sure you follow it; if your site is not looking as expected it is likely a silent error in your YAML. Some text editors like RStudio provide debugging support for YAML and are highly recommended to save you time and heartache.",
    "crumbs": [
      "Quarto workflows"
    ]
  },
  {
    "objectID": "quarto-workflows/index.html#install-quarto",
    "href": "quarto-workflows/index.html#install-quarto",
    "title": "Quarto workflows",
    "section": "Install Quarto",
    "text": "Install Quarto\nhttps://quarto.org/docs/get-started/ describes how to install Quarto, which will depend on your operating system. We’ll walk through installation for each tool in the next chapters.",
    "crumbs": [
      "Quarto workflows"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html",
    "href": "irb/ModelGuideline.html",
    "title": "Model guideline",
    "section": "",
    "text": "Nhìn tổng thể chân dung khách hàng",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#triết-lý-xây-dựng-mô-hình",
    "href": "irb/ModelGuideline.html#triết-lý-xây-dựng-mô-hình",
    "title": "Model guideline",
    "section": "",
    "text": "Nhìn tổng thể chân dung khách hàng",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#phạm-vi-mô-hình",
    "href": "irb/ModelGuideline.html#phạm-vi-mô-hình",
    "title": "Model guideline",
    "section": "Phạm vi mô hình",
    "text": "Phạm vi mô hình\n\nCần xác định rõ phạm vi của mô hình trước khi bắt đầu xây dựng & phát triển mô hình",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#single-factor-analysis",
    "href": "irb/ModelGuideline.html#single-factor-analysis",
    "title": "Model guideline",
    "section": "Single factor analysis",
    "text": "Single factor analysis\n\nWoE binning\n\nSố lượng nhóm phân chia dựa trên tỷ lệ bad rate tương đối, nếu 5 nhóm thì nhóm thứ 3 tỷ lệ này xấp xỉ 1\nSố lượng nhóm khoảng 5-7 nhóm (đẹp nhất là 5 nhóm)\nNên chọn số lượng nhóm số lẻ\nMissing tùy thuộc từng trường hợp sẽ nhóm vào nhóm có tỷ lệ bad rate cao nhất hoặc nhóm có tỷ lệ bad rate xấp xỉ\nTùy từng biến có trend sẽ coasre trend theo biến\n\n\n\nChuẩn hóa biến\n\nNormalising Score: Dùng để dễ so sánh coef của mô hình đa biến sau khi đã hồi quy. Normalised Log Odds thông thường có trung bình = 0, std = 50",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#multiple-factors-analysis",
    "href": "irb/ModelGuideline.html#multiple-factors-analysis",
    "title": "Model guideline",
    "section": "Multiple Factors Analysis",
    "text": "Multiple Factors Analysis\n\nTheo triết lý xây dựng mô hình, mỗi nhóm thông tin khách hàng có ít nhất 1 biến:\nVD: Nhóm thông tin chung có 1 biến, nhóm thông tin hành vi tiêu dùng 3 biến, … các nhóm thông tin còn lại có ít nhất 1 biến",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#chọn-mô-hình-cuối-cùng",
    "href": "irb/ModelGuideline.html#chọn-mô-hình-cuối-cùng",
    "title": "Model guideline",
    "section": "Chọn mô hình cuối cùng",
    "text": "Chọn mô hình cuối cùng\n\n30% &gt; std-coef &gt; 5%\nNếu có hệ số dưới 5%, điều chỉnh lại trọng số = 5%, rồi nhân ngược lại ra adjusted-coef\nCân nhắc trọng số của các nhóm thông tin: Ví dụ: Nhóm thông tin A có 2 biến tổng trọng số 10%, nhóm thông tin B có 3 biến tổng trọng số 50% (nhóm thông tin B này quá mạnh so với nhóm thông tin A)\nNên tăng hoặc giảm hệ số các biến trong cùng 1 nhóm thông tin",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#kiểm-định-mô-hình",
    "href": "irb/ModelGuideline.html#kiểm-định-mô-hình",
    "title": "Model guideline",
    "section": "Kiểm định mô hình",
    "text": "Kiểm định mô hình",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#giám-sát-mô-hình",
    "href": "irb/ModelGuideline.html#giám-sát-mô-hình",
    "title": "Model guideline",
    "section": "Giám sát mô hình",
    "text": "Giám sát mô hình\n\nGini đơn biến suy giảm quá 15%, hoặc 10% cho các biến nhân khẩu học điều chỉnh lại mô hình\nSau khi mô hình vi phạm các ngưỡng thì xem xét lại mô hình\nBáo cáo lại những yếu tố ảnh hưởng tới mô hình. VD: Tình hình cho vay, …\nFront-end monitoring: Xem sự dịch chuyển các nhóm hạng, danh mục\nBack-end monitoring: performance (test trên cả overall, sub-segment)của mô hình vẫn tốt, các factors vẫn phản ánh được kết quả của mô hình. Tùy thuộc dữ liệu có thể ấy dữ liệu từ 2 năm đến 5 năm dữ liệu\nThông thường các factors dễ thay đổi, có thể chạy mô hình trên dữ liệu mới\nPopulation thay đổi (PSI):\n\nFront-end\n\n\n\n\n\n\n\n\n\n\n\nDev\nOOT\nRemark\n\n\n\n\nPSI\nNA\n3.1%\nChỉ số ổn định tổng thể (PSI)\n- PSI &lt; 15%: chỉ số phân phối điểm ổn định.\n- PSI &gt;= 15% và &lt; 25%: chỉ số dịch chuyển vừa phải.\n- PSI &gt;=25%: thể hiện sự thay đổi đáng kể.\nNếu có sự thay đổi đáng kể, thẻ điểm có thể không áp dụng cho mẫu OOT.\n\n\n\nAHI\n5.1%\n4.0%\nChỉ số Herfindah được điều chỉnh (AHI) để đo độ tập trung trong dải điểm nhất định.\n- AHI &lt; 20%: không tập trung\n- AHI &gt;= 20%: có sự tập trung trong dải điểm nhất định. Cần điều tra để đảm bảo không có vấn đề gì với khả năng phân biệt rủi ro.\n\n\n\nBack-end\n\n\n\n\n\n\n\n\nOverall\nDev\nOOT\n\n\n\n\nGini\n\n\n\n\nNo of default\n\n\n\n\nOutcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSub-segment\nRisk Ranking\n\nDefault\n\nOutcome\n\n\n\n\n\n\nDev\nOOT\nDev\nOOT\nBaseline\n2020\n\n\nProduct type\n\n\n\n\n\n\n\n\nCredit card only\n\n\n\n\n\n\n\n\nPersonal Loan only\n\n\n\n\n\n\n\n\nĐơn vị kinh doanh\n\n\n\n\n\n\n\n\nHO\n\n\n\n\n\n\n\n\nCông ty tài chính\n\n\n\n\n\n\n\n\n\nFactor mới thêm vào có thể dùng phương pháp điều chỉnh weight",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#chuyển-đổi-điểm",
    "href": "irb/ModelGuideline.html#chuyển-đổi-điểm",
    "title": "Model guideline",
    "section": "Chuyển đổi điểm",
    "text": "Chuyển đổi điểm",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#apply-irb",
    "href": "irb/ModelGuideline.html#apply-irb",
    "title": "Model guideline",
    "section": "Apply IRB",
    "text": "Apply IRB\n\nQuy trình tín dụng\nQuy trình đánh giá khách hàng",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#pd-model-calibration",
    "href": "irb/ModelGuideline.html#pd-model-calibration",
    "title": "Model guideline",
    "section": "PD Model Calibration",
    "text": "PD Model Calibration\n\nHousing loan: với 6 tháng hành vi trả nợ chưa đủ mạnh, chưa có nhiều behavious, sử dụng MoB 12 để xác định khách hàng mới hoặc cũ\nThẻ tín dụng: chỉ cần 6 tháng đã có được hành vi tiêu dùng, trả nợ\nChú ý tính đồng nhất của danh mục\nNếu thay đổi chính sách thì mô hình phải phản ánh chính sách forward-looking\nNếu không có thông tin liên quan đến thị trường, thì phải có điều chỉnh downgrade PD\nA-score: Cyclicality ~ 40 -50%, B-score ~ 60-70%\nƯớc tính cyclicality: do 1 model thường không dùng qua 1 chu kỳ kinh tế, nếu back score thì có thẻ dữ liệu cũ không đủ. Những biến như age, gender thì không có cyclicality\nRetail có thể có master scale riêng với từng danh mục, nhưng có thể distribution không đẹp",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#xác-định-chu-kỳ-kinh-tế",
    "href": "irb/ModelGuideline.html#xác-định-chu-kỳ-kinh-tế",
    "title": "Model guideline",
    "section": "Xác định chu kỳ kinh tế",
    "text": "Xác định chu kỳ kinh tế\n\nThu thập dữ liệu càng dài càng tốt, tối thiểu 5 năm\nThị trường châu á, lấy mốc khủng hoảng kinh tế là 1998\nDùng portfolio có tính chất tương tự, có thể dùng portfolio tương tự của Central Bank NPL, chứng minh correlation sau đó extrapolate\nForward looking, chọn số trung bình giữa baseline và worst-case\nTrong giai đoạn covid, nếu có nhà nước có nhiều chính sách làm cho odr giảm, thì có thể bỏ những data point này ra khỏi mẫu trong thời kỳ crisis",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#chia-pool",
    "href": "irb/ModelGuideline.html#chia-pool",
    "title": "Model guideline",
    "section": "Chia pool",
    "text": "Chia pool\n\nDựa trên product types, pd, mob, bucket …",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#đánh-giá-lại-pd",
    "href": "irb/ModelGuideline.html#đánh-giá-lại-pd",
    "title": "Model guideline",
    "section": "Đánh giá lại PD",
    "text": "Đánh giá lại PD\n\ndựa vào CT, ODR, đánh giá implied pd còn phù hợp hay không",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#moc",
    "href": "irb/ModelGuideline.html#moc",
    "title": "Model guideline",
    "section": "MoC",
    "text": "MoC",
    "crumbs": [
      "Model guideline"
    ]
  },
  {
    "objectID": "irb/ModelGuideline.html#thấu-chi",
    "href": "irb/ModelGuideline.html#thấu-chi",
    "title": "Model guideline",
    "section": "Thấu chi",
    "text": "Thấu chi\n\nCó tài sản đảm bảo\n\nNếu là KH housing loan vay thêm thấu chi, có thể dùng housing score card\n\n\n\nNếu tài sản là tiền gửi nhỏ, thì áp dụng standardise\n\nKhông có tài sản\n\nXếp chung vào nhóm credit card",
    "crumbs": [
      "Model guideline"
    ]
  }
]