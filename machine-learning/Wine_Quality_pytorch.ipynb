{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on the Wine Quality dataset using ordinal loss and the Kappa metric in PyTorch\n",
    "\n",
    "Date: 2024-12-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "data = pd.read_csv(url, sep=';')\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality'] - 3\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Set the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "The model definition remains the same as in the previous solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class OrdinalNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrdinalNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 6)  # Number of classes (3-8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = OrdinalNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalFocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.25, num_classes=6):\n",
    "        super(OrdinalFocalLoss, self).__init__()\n",
    "        self.gamma = gamma  # Focusing parameter\n",
    "        self.alpha = alpha  # Weighting factor\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Compute the Ordinal Focal Loss\n",
    "\n",
    "        :param outputs: Predicted logits from the model (batch_size, num_classes)\n",
    "        :param targets: Ground truth labels (batch_size)\n",
    "        :return: Loss value\n",
    "        \"\"\"\n",
    "        # Convert targets to one-hot encoding\n",
    "        targets_one_hot = torch.zeros(targets.size(0), self.num_classes).to(targets.device)\n",
    "        targets_one_hot.scatter_(1, targets.unsqueeze(1), 1)\n",
    "\n",
    "        # Apply softmax to outputs to get class probabilities\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "        # Calculate the probability of the true class\n",
    "        p_t = torch.sum(probs * targets_one_hot, dim=1)  # This is p_t for each instance\n",
    "\n",
    "        # Compute the focal loss\n",
    "        loss = -self.alpha * (1 - p_t) ** self.gamma * torch.log(p_t + 1e-8)  # Add epsilon to avoid log(0)\n",
    "\n",
    "        # Return the average loss\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.5258033365011214, Kappa: 0.027160395525093195\n",
      "Epoch 2/100, Loss: 1.14848440438509, Kappa: 0.1941581189042062\n",
      "Epoch 3/100, Loss: 1.0254976019263267, Kappa: 0.29289946055146165\n",
      "Epoch 4/100, Loss: 0.988839827477932, Kappa: 0.3383639594251141\n",
      "Epoch 5/100, Loss: 0.965928153693676, Kappa: 0.3631989771546358\n",
      "Epoch 6/100, Loss: 0.9539458021521569, Kappa: 0.3677904087658541\n",
      "Epoch 7/100, Loss: 0.9409917533397675, Kappa: 0.3671246515932114\n",
      "Epoch 8/100, Loss: 0.9327839985489845, Kappa: 0.3751479492595736\n",
      "Epoch 9/100, Loss: 0.9210372045636177, Kappa: 0.3685858232131014\n",
      "Epoch 10/100, Loss: 0.922743383049965, Kappa: 0.3722848662886773\n",
      "Epoch 11/100, Loss: 0.9097453355789185, Kappa: 0.3813580749062848\n",
      "Epoch 12/100, Loss: 0.9032061487436295, Kappa: 0.37044592857844605\n",
      "Epoch 13/100, Loss: 0.8965445622801781, Kappa: 0.3797972038405859\n",
      "Epoch 14/100, Loss: 0.8881877571344375, Kappa: 0.4032125634822731\n",
      "Epoch 15/100, Loss: 0.8835199296474456, Kappa: 0.40018818898663533\n",
      "Epoch 16/100, Loss: 0.8790309250354766, Kappa: 0.3903325462495525\n",
      "Epoch 17/100, Loss: 0.8702574223279953, Kappa: 0.40721159778533744\n",
      "Epoch 18/100, Loss: 0.8697167977690696, Kappa: 0.4072256904559757\n",
      "Epoch 19/100, Loss: 0.8619306489825249, Kappa: 0.4140637008713334\n",
      "Epoch 20/100, Loss: 0.8568678289651871, Kappa: 0.4211031088992041\n",
      "Epoch 21/100, Loss: 0.8524924352765083, Kappa: 0.41451338248257064\n",
      "Epoch 22/100, Loss: 0.8480689927935601, Kappa: 0.41419673103023\n",
      "Epoch 23/100, Loss: 0.8440612345933914, Kappa: 0.44178941625750134\n",
      "Epoch 24/100, Loss: 0.8406461104750633, Kappa: 0.4034622168334888\n",
      "Epoch 25/100, Loss: 0.8361509755253792, Kappa: 0.43327084440256236\n",
      "Epoch 26/100, Loss: 0.8344028279185295, Kappa: 0.41844363720856625\n",
      "Epoch 27/100, Loss: 0.8277643755078316, Kappa: 0.4222757717701464\n",
      "Epoch 28/100, Loss: 0.8244240581989288, Kappa: 0.43849674868294786\n",
      "Epoch 29/100, Loss: 0.8242936119437217, Kappa: 0.42040352491703603\n",
      "Epoch 30/100, Loss: 0.8143198490142822, Kappa: 0.43104302757442947\n",
      "Epoch 31/100, Loss: 0.8092397406697274, Kappa: 0.44060655010864125\n",
      "Epoch 32/100, Loss: 0.8103402808308602, Kappa: 0.445418000079017\n",
      "Epoch 33/100, Loss: 0.8025514364242554, Kappa: 0.4359494216495535\n",
      "Epoch 34/100, Loss: 0.801366850733757, Kappa: 0.45975261807226997\n",
      "Epoch 35/100, Loss: 0.7979567974805832, Kappa: 0.4482904623702938\n",
      "Epoch 36/100, Loss: 0.7909636944532394, Kappa: 0.4499257267039999\n",
      "Epoch 37/100, Loss: 0.7835334852337837, Kappa: 0.46419209788669\n",
      "Epoch 38/100, Loss: 0.7814476490020752, Kappa: 0.4648827375611594\n",
      "Epoch 39/100, Loss: 0.7808900579810143, Kappa: 0.46110282448596707\n",
      "Epoch 40/100, Loss: 0.7702956169843673, Kappa: 0.4892929788113062\n",
      "Epoch 41/100, Loss: 0.7734991297125816, Kappa: 0.47925739353272945\n",
      "Epoch 42/100, Loss: 0.7667867332696915, Kappa: 0.48040533912521344\n",
      "Epoch 43/100, Loss: 0.7656202584505081, Kappa: 0.475668626326781\n",
      "Epoch 44/100, Loss: 0.7598542019724845, Kappa: 0.4858225595558915\n",
      "Epoch 45/100, Loss: 0.7581931084394455, Kappa: 0.49598754033499703\n",
      "Epoch 46/100, Loss: 0.7500043898820877, Kappa: 0.4938932823137663\n",
      "Epoch 47/100, Loss: 0.7498278692364693, Kappa: 0.4952857622244943\n",
      "Epoch 48/100, Loss: 0.743773840367794, Kappa: 0.5079226029464861\n",
      "Epoch 49/100, Loss: 0.7405090779066086, Kappa: 0.5198281901732583\n",
      "Epoch 50/100, Loss: 0.7364834100008011, Kappa: 0.5023553770687714\n",
      "Epoch 51/100, Loss: 0.7336597308516503, Kappa: 0.5167070843799515\n",
      "Epoch 52/100, Loss: 0.7293936885893345, Kappa: 0.508031801733295\n",
      "Epoch 53/100, Loss: 0.7258813440799713, Kappa: 0.5246337519963721\n",
      "Epoch 54/100, Loss: 0.7226672306656837, Kappa: 0.5265172456118816\n",
      "Epoch 55/100, Loss: 0.7217974692583085, Kappa: 0.5230864181656874\n",
      "Epoch 56/100, Loss: 0.712223195284605, Kappa: 0.5266455229603828\n",
      "Epoch 57/100, Loss: 0.7112390361726284, Kappa: 0.5342339017248592\n",
      "Epoch 58/100, Loss: 0.7029153138399125, Kappa: 0.5468456178231427\n",
      "Epoch 59/100, Loss: 0.7027535423636436, Kappa: 0.5393219619781618\n",
      "Epoch 60/100, Loss: 0.7004692874848842, Kappa: 0.553876299131837\n",
      "Epoch 61/100, Loss: 0.6959677867591381, Kappa: 0.5544725929676446\n",
      "Epoch 62/100, Loss: 0.6945664048194885, Kappa: 0.5516791604790041\n",
      "Epoch 63/100, Loss: 0.6882325552403927, Kappa: 0.5410228096173484\n",
      "Epoch 64/100, Loss: 0.6827452167868614, Kappa: 0.552530873054403\n",
      "Epoch 65/100, Loss: 0.6810622230172158, Kappa: 0.5720976915356306\n",
      "Epoch 66/100, Loss: 0.6765776731073856, Kappa: 0.5615048120704035\n",
      "Epoch 67/100, Loss: 0.6707129381597042, Kappa: 0.5688434451478712\n",
      "Epoch 68/100, Loss: 0.6698078818619251, Kappa: 0.5779857384414895\n",
      "Epoch 69/100, Loss: 0.6700948402285576, Kappa: 0.5699745672150069\n",
      "Epoch 70/100, Loss: 0.6645593464374542, Kappa: 0.5621607359002477\n",
      "Epoch 71/100, Loss: 0.6604863002896308, Kappa: 0.5721880442236442\n",
      "Epoch 72/100, Loss: 0.6597750805318355, Kappa: 0.5939745996643764\n",
      "Epoch 73/100, Loss: 0.6539112649857998, Kappa: 0.5778682427539623\n",
      "Epoch 74/100, Loss: 0.6505351833999157, Kappa: 0.5958212350004243\n",
      "Epoch 75/100, Loss: 0.6485376708209515, Kappa: 0.5947310051377226\n",
      "Epoch 76/100, Loss: 0.6440189868211746, Kappa: 0.590627979197287\n",
      "Epoch 77/100, Loss: 0.6387162208557129, Kappa: 0.5899935591403664\n",
      "Epoch 78/100, Loss: 0.6353652991354466, Kappa: 0.5983798594678247\n",
      "Epoch 79/100, Loss: 0.6351467996835709, Kappa: 0.6006119625320245\n",
      "Epoch 80/100, Loss: 0.631549759209156, Kappa: 0.6093738987430988\n",
      "Epoch 81/100, Loss: 0.6277068927884102, Kappa: 0.61762519112683\n",
      "Epoch 82/100, Loss: 0.6236165843904018, Kappa: 0.6089009813451609\n",
      "Epoch 83/100, Loss: 0.6264286696910858, Kappa: 0.6153901337182963\n",
      "Epoch 84/100, Loss: 0.6178285926580429, Kappa: 0.6276718056686829\n",
      "Epoch 85/100, Loss: 0.6154053710401058, Kappa: 0.6062257547132006\n",
      "Epoch 86/100, Loss: 0.6118617177009582, Kappa: 0.5975909896858916\n",
      "Epoch 87/100, Loss: 0.6087865322828293, Kappa: 0.6062408326862287\n",
      "Epoch 88/100, Loss: 0.6076791845262051, Kappa: 0.6200215390448939\n",
      "Epoch 89/100, Loss: 0.6064654208719731, Kappa: 0.6229728790088249\n",
      "Epoch 90/100, Loss: 0.600296714156866, Kappa: 0.6301554272539882\n",
      "Epoch 91/100, Loss: 0.6021047808229923, Kappa: 0.6221714061388228\n",
      "Epoch 92/100, Loss: 0.5982880525290966, Kappa: 0.6244946251032697\n",
      "Epoch 93/100, Loss: 0.590181715041399, Kappa: 0.6320299679124919\n",
      "Epoch 94/100, Loss: 0.5918105013668538, Kappa: 0.6247179516270536\n",
      "Epoch 95/100, Loss: 0.5913812078535556, Kappa: 0.6348833139597976\n",
      "Epoch 96/100, Loss: 0.5805985651910305, Kappa: 0.6416545681465067\n",
      "Epoch 97/100, Loss: 0.5784861400723458, Kappa: 0.6488824583376482\n",
      "Epoch 98/100, Loss: 0.5749207615852356, Kappa: 0.6498347050063702\n",
      "Epoch 99/100, Loss: 0.5697298489511013, Kappa: 0.652995478504598\n",
      "Epoch 100/100, Loss: 0.5659848034381867, Kappa: 0.6566265295483256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "# Initialize the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to compute the Kappa score\n",
    "def compute_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true.cpu(), y_pred.cpu())\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Collect true and predicted labels for Kappa score\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "    \n",
    "    # Calculate Kappa score\n",
    "    kappa_score = compute_kappa(torch.tensor(y_true), torch.tensor(y_pred))\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Kappa: {kappa_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Kappa Score: 0.38975208678525664\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Collect true and predicted labels for Kappa score\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "# Calculate Kappa score on the test set\n",
    "test_kappa = compute_kappa(torch.tensor(y_true), torch.tensor(y_pred))\n",
    "print(f\"Test Kappa Score: {test_kappa}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So sánh Cross Entropy Loss và Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with CrossEntropyLoss:\n",
      "Test Accuracy: 0.6125, Test Kappa: 0.3739\n",
      "\n",
      "Training with Focal Loss:\n",
      "Test Accuracy: 0.5969, Test Kappa: 0.3695\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming OrdinalFocalLoss and model are defined as before\n",
    "# CrossEntropy Loss\n",
    "cross_entropy_criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Focal Loss\n",
    "focal_loss_criterion = OrdinalFocalLoss().to(device)\n",
    "\n",
    "# Initialize your model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OrdinalNN().to(device)\n",
    "\n",
    "# Helper function for Kappa Score and Accuracy\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true.cpu(), y_pred.cpu())\n",
    "    kappa = cohen_kappa_score(y_true.cpu(), y_pred.cpu())\n",
    "    return accuracy, kappa\n",
    "\n",
    "# Training function for comparison\n",
    "def train_model(criterion, model, num_epochs=100, train_loader=None, test_loader=None):\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Collect true and predicted labels for Kappa score\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "        \n",
    "        # Calculate Kappa score\n",
    "        kappa_score = compute_kappa(torch.tensor(y_true), torch.tensor(y_pred))\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Kappa: {kappa_score}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    accuracy, kappa = compute_metrics(torch.tensor(y_true), torch.tensor(y_pred))\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, Test Kappa: {kappa:.4f}\")\n",
    "\n",
    "# Run training and evaluation for both loss functions\n",
    "print(\"Training with CrossEntropyLoss:\")\n",
    "train_model(cross_entropy_criterion, model=model, num_epochs=100, train_loader=train_loader, test_loader=test_loader)\n",
    "\n",
    "print(\"\\nTraining with Focal Loss:\")\n",
    "train_model(focal_loss_criterion, model=model, num_epochs=100, train_loader=train_loader, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalModelWithDropout(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dropout_rate=0.5):\n",
    "        super(OrdinalModelWithDropout, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        \n",
    "        # Adding dropout layers between fully connected layers\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout after the first layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # Apply dropout after the second layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with CrossEntropyLoss:\n",
      "Test Accuracy: 0.6188, Test Kappa: 0.3709\n",
      "\n",
      "Training with Focal Loss:\n",
      "Test Accuracy: 0.6438, Test Kappa: 0.4167\n"
     ]
    }
   ],
   "source": [
    "# Initialize your model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OrdinalModelWithDropout(input_size=X.shape[1], num_classes=6).to(device)\n",
    "\n",
    "# Run training and evaluation for both loss functions\n",
    "print(\"Training with CrossEntropyLoss:\")\n",
    "train_model(cross_entropy_criterion, model=model, num_epochs=100, train_loader=train_loader, test_loader=test_loader)\n",
    "\n",
    "print(\"\\nTraining with Focal Loss:\")\n",
    "train_model(focal_loss_criterion, model=model, num_epochs=100, train_loader=train_loader, test_loader=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "937f2672c7c02c8da1475acbe4f8068c6920a68256c60c8f3dd6a7265bf7f580"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
