{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Reinforcement Learning\n",
    "author: \"Nguyễn Ngọc Bình\"\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning là gì\n",
    "\n",
    "Reinforcement Learning (RL) là một lĩnh vực của trí tuệ nhân tạo mà trong đó một \"agent\" học cách tương tác với một \"environment\" để đạt được mục tiêu thông qua việc thực hiện \"actions\". Agent học từ các phản hồi gọi là \"rewards\" từ environment và dần dần điều chỉnh hành vi để tối đa hóa tổng số rewards.\n",
    "\n",
    "## Các thuật ngữ thông dụng\n",
    "\n",
    "Trong reinforcement learning có rất nhiều các thuật ngữ khác nhau. Sau đây chúng ta cùng liệt kê các thuật ngữ thông dụng và tìm hiểu ý nghĩa của từng thuật ngữ\n",
    "\n",
    "1. **Agent**: Là thực thể thực hiện các hành động trong môi trường để đạt được mục tiêu.\n",
    "\n",
    "2. **Environment**: Là môi trường mà agent tương tác và học từ đó. Nó bao gồm tất cả các yếu tố ảnh hưởng đến agent và có thể thay đổi dựa trên hành động của agent.\n",
    "\n",
    "3. **Action**: Là các hành động mà agent thực hiện trong môi trường để thay đổi trạng thái hiện tại của nó.\n",
    "\n",
    "4. **Observation**: Là thông tin mà agent thu thập từ môi trường sau khi thực hiện một hành động.\n",
    "\n",
    "5. **State**: Là biểu diễn trạng thái hiện tại của môi trường. Nó chứa thông tin cần thiết để quyết định tương lai của agent.\n",
    "\n",
    "6. **Policy**: Là chiến lược hoặc kế hoạch mà agent sử dụng để chọn hành động dựa trên trạng thái hiện tại.\n",
    "\n",
    "7. **Reward**: Là phản hồi từ môi trường sau mỗi hành động. Reward định rõ giá trị của hành động và giúp agent học cách tối đa hóa tổng số reward theo thời gian.\n",
    "\n",
    "8. **Khai thác và khám phá (exploit or explore)**: Là quá trình cân bằng giữa việc sử dụng kiến thức hiện có để đạt được reward ngay lập tức (khai thác) và việc thử nghiệm các hành động mới để tìm hiểu thêm về môi trường (khám phá).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/08/Reinforcement-Learning-in-ML-TV.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So sánh Reinforcement Learning (RL), Unsupervised Learning, and Supervised Learning\n",
    "\n",
    "Học tăng cường (RL), Học không giám sát và Học có giám sát là ba mô hình cơ bản trong học máy, mỗi mô hình phục vụ các mục đích khác nhau và giải quyết các loại vấn đề riêng biệt. Dưới đây là so sánh ba cách tiếp cận này:\n",
    "\n",
    "### Học tăng cường (RL):\n",
    "- **Mục tiêu**: Học cách đưa ra quyết định để tối đa hóa phần thưởng tích lũy trong một môi trường.\n",
    "- **Bản chất**: Liên quan đến việc tác nhân tương tác với môi trường bằng cách thực hiện hành động và nhận phần thưởng dựa trên hành động của tác nhân đó.\n",
    "- **Dữ liệu đào tạo**: RL học hỏi từ các tương tác và phản hồi thay vì dữ liệu được gắn nhãn.\n",
    "- **Ví dụ**: Chơi trò chơi, điều khiển robot, hệ thống đề xuất.\n",
    "- **Thách thức**: Đánh đổi giữa thăm dò và khai thác, phần thưởng bị trì hoãn, cân bằng giữa thăm dò và khai thác.\n",
    "\n",
    "### Học không giám sát:\n",
    "- **Mục tiêu**: Tìm các mẫu hoặc cấu trúc trong dữ liệu chưa được gắn nhãn.\n",
    "- **Bản chất**: Không có mục tiêu hoặc kết quả cụ thể để dự đoán. Mục tiêu là khám phá các mối quan hệ hoặc cụm ẩn trong dữ liệu.\n",
    "- **Dữ liệu huấn luyện**: Học từ dữ liệu đầu vào mà không có nhãn đầu ra rõ ràng.\n",
    "- **Ví dụ**: Phân cụm, giảm kích thước, tạo mô hình tổng quát (ví dụ: GAN).\n",
    "- **Thách thức**: Xác định số lượng cụm thích hợp, xử lý dữ liệu nhiều chiều.\n",
    "\n",
    "### Học tập có giám sát:\n",
    "- **Mục tiêu**: Học cách ánh xạ từ đầu vào đến đầu ra dựa trên dữ liệu huấn luyện được gắn nhãn.\n",
    "- **Bản chất**: Yêu cầu tập dữ liệu được gắn nhãn trong đó mô hình học cách dự đoán kết quả đầu ra chính xác từ các đầu vào nhất định.\n",
    "- **Dữ liệu huấn luyện**: Sử dụng các cặp đầu vào-đầu ra để huấn luyện.\n",
    "- **Ví dụ**: Phân loại, hồi quy.\n",
    "- **Thách thức**: Quá phù hợp, sai lệch, khái quát hóa đối với dữ liệu không nhìn thấy được.\n",
    "\n",
    "**Sự khác biệt chính**:\n",
    "- RL tập trung vào việc học các hành động tối ưu để tối đa hóa phần thưởng, trong khi việc học có giám sát và không giám sát tập trung vào các mô hình học tập hoặc các mối quan hệ trong dữ liệu.\n",
    "- Học không giám sát thiếu dữ liệu được dán nhãn, trong khi học có giám sát yêu cầu dữ liệu được dán nhãn.\n",
    "- Học có giám sát phù hợp với các nhiệm vụ đã biết đầu ra mong muốn, trong khi RL phù hợp với các nhiệm vụ liên quan đến việc ra quyết định tuần tự.\n",
    "- RL thường liên quan đến việc khám phá và đánh đổi giữa phần thưởng trước mắt và lâu dài, trong khi học tập có giám sát và không giám sát là tìm kiếm các mô hình hoặc đưa ra dự đoán.\n",
    "\n",
    "![](https://www.mathworks.com/discovery/reinforcement-learning/_jcr_content/mainParsys3/discoverysubsection/mainParsys/image.adapt.full.medium.png/1690832474215.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tham khảo\n",
    "\n",
    "- [codelearn](https://codelearn.io/sharing/reinforcement-learning-la-gi)\n",
    "- [techvidvan](https://techvidvan.com/tutorials/reinforcement-learning/)\n",
    "- [mathworks](https://www.mathworks.com/discovery/reinforcement-learning.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
